[
  {
    "objectID": "lectures/part2/5_bayes_parameter_estimation.html",
    "href": "lectures/part2/5_bayes_parameter_estimation.html",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "6.1 Introduction to the Bayesian approach\nAs discussed in the course introduction, Bayesian inference takes a different perspective from the frequentist approach:\nThe randomness here does not mean that the parameter is physically fluctuating — instead, it reflects our uncertainty about its value.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAS2901: Statistical Inference",
    "section": "",
    "text": "Preface\nWelcome to the online notes for MAS2901 — Statistical Inference.\nThis 10-credit module at Newcastle University introduces the two main approaches to statistical inference:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-statistical-inference",
    "href": "index.html#what-is-statistical-inference",
    "title": "MAS2901: Statistical Inference",
    "section": "What is Statistical Inference?",
    "text": "What is Statistical Inference?\nStatistical inference is the process of using data from a sample to learn about a population. Sometimes the population is concrete (e.g. “all people in the country today”). In other situations we want to generalise beyond those we sampled to a broader target population (sometimes called a superpopulation), such as “future patients with similar characteristics”. Clear definitions and sensible assumptions are essential for valid conclusions.\nThere are two main types of statistical inference:\n\nFrequentist inference. Parameters are treated as fixed but unknown. Probability statements are about data we might observe under repeated sampling from the same model.\nBayesian inference. Parameters are treated as random with a prior distribution. Probability statements are about the parameter given the data via the posterior distribution.\n\nThe examples below are only to build intuition about the core difference between frequentist and Bayesian inference. Don’t worry about the details—we’ll cover them carefully later in the course.\n\n\n\n\n\n\n\nExample 1 (A comparison of Frequentist vs Bayesian inference) A survey asks \\(n=20\\) students if they prefer study space \\(A\\) or \\(B\\).\nSuppose \\(x=12\\) prefer \\(A\\). Compare a frequentist and a Bayesian estimate of the long-run proportion \\(p\\) who would choose \\(A\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFrequentist. The natural estimator is the sample proportion\n\\[\n\\hat p=\\frac{x}{n}=\\frac{12}{20}=0.60.\n\\]\nBayesian (with a uniform prior). Take a \\(\\mathrm{Beta}(1,1)\\) prior for \\(p\\). The posterior is\n\\[\np \\mid x \\sim \\mathrm{Beta}(1+x,\\,1+n-x)=\\mathrm{Beta}(13,9),\n\\]\nwhose mean is\n\\[\n\\mathbb{E}[p\\mid x]=\\frac{13}{13+9}=\\frac{13}{22}\\approx 0.591.\n\\]\nInterpretation. The frequentist estimate \\(\\hat p\\) is a function of the data and treats \\(p\\) as fixed.\nThe Bayesian estimate is an expectation of \\(p\\) itself under the posterior distribution.\n\n\n\n\n\n\n\n\n\n\n\nExample 2 (Interpreting probability statements) Decide whether each statement is frequentist or Bayesian in spirit.\n\n“If we repeated the survey many times, the method we use would produce intervals that capture \\(p\\) about 95% of the time.”\n\n“Given the data from this survey and our prior, there is a 95% probability that \\(p\\) lies between 0.45 and 0.75.”\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nFrequentist. This describes long-run coverage of a procedure over repeated samples with \\(p\\) fixed.\n\nBayesian. This is a probability statement about \\(p\\) given the observed data (a credible interval).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-course",
    "href": "index.html#structure-of-the-course",
    "title": "MAS2901: Statistical Inference",
    "section": "Structure of the Course",
    "text": "Structure of the Course\nThe notes are structured as follows:\n\nPart 1: Preliminaries\n\nProbability Theory\n\nWhat is Statistical Inference? (introduces statistical models)\n\nTypes of Inference\n\n\nPart 2: Point Estimation\n\nEstimate vs. Estimator\n\nCentral Limit Theorem\n\nBias, Mean-Squared Error and Consistency\n\nLikelihood Function\n\nBayesian Inference\n\n\nPart 3: Interval Estimation\n\nPart 4: Hypothesis Testing\n\nPart 5: Prediction\nExtras:\n\nDistribution Zoo (covers all the distributions used in the course)\n\nRevision Guide\n\nQuestion Bank",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "MAS2901: Statistical Inference",
    "section": "Further Reading",
    "text": "Further Reading\n\nasdasdasds",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html",
    "href": "lectures/part2/4_likelihood.html",
    "title": "5  Likelihood",
    "section": "",
    "text": "5.1 From Joint Density to Likelihood\nThe likelihood function is the joint density (or probability mass) function of the data:\n\\[\nf(\\underline{x} \\mid \\theta), \\quad \\underline{x} = (x_1, \\dots, x_n).\n\\]\nSuppose we observe \\(n\\) independent and identically distributed (i.i.d.) samples,\n\\[\nX_1, \\dots, X_n \\overset{\\text{iid}}{\\sim} f_X(x \\mid \\theta).\n\\]\nThen the joint density is given by multiplying the individual densities:\n\\[\nf(\\underline{x} \\mid \\theta)\n= \\prod_{i=1}^n f_X(x_i \\mid \\theta).\n\\]",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#from-joint-density-to-likelihood",
    "href": "lectures/part2/4_likelihood.html#from-joint-density-to-likelihood",
    "title": "5  Likelihood",
    "section": "",
    "text": "TipRemark (Why products?)\n\n\n\n\n\n\nRemark 5.1 (Why products?). \n\nIndependence means that joint probabilities factorise into products:\n\\[\nP(A \\cap B) = P(A)\\,P(B).\n\\]\nBy the same rule, the joint density of independent random variables is the product of their individual densities.\nThis is often the first place where students forget the independence assumption! Without it, you cannot just multiply.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#likelihood-function",
    "href": "lectures/part2/4_likelihood.html#likelihood-function",
    "title": "5  Likelihood",
    "section": "5.2 Likelihood Function",
    "text": "5.2 Likelihood Function\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 5.1 Likelihood Function. For fixed observed data \\(\\underline{x}\\), the likelihood function is defined as\n\\[\nL(\\theta ; \\underline{x}) \\coloneqq f(\\,\\underline{x} \\mid \\theta).\n\\]\nHere we treat \\(\\theta\\) as the variable, and the data \\(\\underline{x}\\) as fixed.\n\n\n\n\n\n\n\n\n\nTipLikelihood vs. Probability:\n\n\n\n\nPDF/PMF: function of \\(\\underline{x}\\) (data random, parameter fixed).\nLikelihood: function of \\(\\theta\\) (data fixed, parameter variable).\n\n\n\n\n5.2.1 Visualising the Likelihood Function\n\nhtml`&lt;style&gt;\n.plot--large-axis .x-axis .label,\n.plot--large-axis .y-axis .label {\nfont-size: 24px; font-weight: 600;\n}\n.plot--large-axis .x-axis text,\n.plot--large-axis .y-axis text {\nfont-size: 12px; /* tick labels (optional) */\n}\n&lt;/style&gt;`\n\n\n\n\n\n\n\nX = {\n  // Trigger on: resim button, n, trueMu, or trueSigma changes\n  resim; n; trueMu; trueSigma;\n  \n  const m = trueMu, s = trueSigma;\n  const out = new Array(n);\n  for (let i = 0; i &lt; n; i += 2) {\n    const u = Math.random(), v = Math.random();\n    const R = Math.sqrt(-2 * Math.log(u)), T = 2 * Math.PI * v;\n    const z0 = R * Math.cos(T), z1 = R * Math.sin(T);\n    out[i] = m + s * z0;\n    if (i + 1 &lt; n) out[i + 1] = m + s * z1;\n  }\n  return out;\n};\n\nxbar = d3.mean(X);\nS = d3.sum(X, x =&gt; (x - xbar) ** 2);\n\nlogLik = (mu, sigma2) =&gt; {\n  const ss = d3.sum(X, x =&gt; (x - mu) ** 2);\n  return -0.5 * n * Math.log(2 * Math.PI * sigma2) - 0.5 * ss / sigma2;\n};\n\nmuhat = xbar;\nsigma2hat = S / n;\n\n// --- Grid for contours (axes ticks) ---\n// These only need to update when display parameters change\nmuGrid = d3.ticks(muhat - muHalfWidth, muhat + muHalfWidth, gridN)\nsig2Grid = d3.ticks(Math.max(0.05, sigma2hat / 20), sig2Max, gridN).filter(s =&gt; s &gt; 0)\n\n// --- Build a row-major grid for Plot.raster / Plot.contour ---\nnx = muGrid.length\nny = sig2Grid.length\n\n// Memoize the expensive log-likelihood computation\nllValues = {\n  const z = new Float64Array(nx * ny);\n  for (let j = 0; j &lt; ny; ++j) {\n    for (let i = 0; i &lt; nx; ++i) {\n      const s2 = Math.max(1e-6, sig2Grid[j]);\n      z[j * nx + i] = logLik(muGrid[i], s2);\n    }\n  }\n  return z;\n}\n\n\n// ---- Geometry / domains (responsive) ----\ngap = 20\ncontainerW = width\nplotW = Math.round(Math.max(240, Math.min(600, (containerW - gap) / 2 - 15)))\nplotH = Math.round(plotW * 0.8)\n\nmarginTop = 20\nmarginRight = 35\nmarginBottom = 20\nmarginLeft = 35\ninnerW = plotW - marginLeft - marginRight\ninnerH = plotH - marginTop - marginBottom\n\nmuDomain = [muGrid[0], muGrid[muGrid.length - 1]]\nsig2Domain = [sig2Grid[0], sig2Grid[sig2Grid.length - 1]]\n\n// ---- Colour scaling (robust & reactive) ----\nllMax = d3.max(llValues)\nllRel = Float64Array.from(llValues, v =&gt; v - llMax)\n\n// Use more efficient quantile computation\nllRelFinite = Array.from(llRel).filter(Number.isFinite)\nllRelSorted = llRelFinite.slice().sort(d3.ascending)\nq05 = d3.quantileSorted(llRelSorted, 0.05) ?? d3.min(llRelFinite)\n\ncolorDomain = [q05, 0]\nllRelClipped = Float64Array.from(llRel, v =&gt; Math.max(q05, v))\ncontourThresholds = d3.ticks(q05, 0, 10)\n\n\n// ---- Left panel: raster (clipped) + contour (same grid) ----\nviewof leftPanel = {\n  const wrap = html`&lt;div style=\"\n    position: relative;\n    width: ${plotW}px;\n    height: ${plotH}px;\n    flex: 0 0 ${plotW}px;\n    line-height: 0;\n  \"&gt;&lt;/div&gt;`;\n\n  const base = Plot.plot({\n    width: plotW, height: plotH,\n    marginLeft, marginRight, marginTop, marginBottom,\n    style: {\n      background: \"var(--brand-bg)\",\n      color: \"var(--brand-fg)\"\n    },\n    x: { label: \"μ\", domain: muDomain },\n    y: { label: \"σ²\", domain: sig2Domain },\n    color: {\n      type: \"symlog\",\n      domain: colorDomain,\n      clamp: true,\n      scheme: \"turbo\",\n      legend: false\n    },\n    marks: [\n      Plot.raster(llRelClipped, {\n        width: nx, height: ny,\n        x1: muDomain[0], x2: muDomain[1],\n        y1: sig2Domain[0], y2: sig2Domain[1],\n        interpolate: \"nearest\"\n      }),\n      Plot.dot([{ mu: muhat, sig2: sigma2hat }], {\n        x: \"mu\", y: \"sig2\", r: 5, stroke: \"currentColor\", fill: \"var(--brand-bg)\"\n      })\n    ]\n  });\n\n  base.classList.add(\"plot--large-axis\");\n  base.style.display = \"block\";\n  wrap.append(base);\n\n\n  // Crosshair overlay fills wrapper\n  const svg = d3.create(\"svg\")\n    .attr(\"width\", plotW).attr(\"height\", plotH)\n    .style(\"position\", \"absolute\").style(\"left\", \"0\").style(\"top\", \"0\")\n    .style(\"width\", \"100%\").style(\"height\", \"100%\")\n    .style(\"pointer-events\", \"none\");\n\n\n\n  const xScale = d3.scaleLinear().domain(muDomain).range([marginLeft, marginLeft + innerW]);\n  const yScale = d3.scaleLinear().domain(sig2Domain).range([marginTop + innerH, marginTop]);\n\n  const vline = svg.append(\"line\")\n  .attr(\"x1\", xScale(muhat)).attr(\"x2\", xScale(muhat))\n  .attr(\"y1\", marginTop).attr(\"y2\", marginTop + innerH)\n  .attr(\"stroke\", \"currentColor\");\n\n  const hline = svg.append(\"line\")\n  .attr(\"x1\", marginLeft).attr(\"x2\", marginLeft + innerW)\n  .attr(\"y1\", yScale(sigma2hat)).attr(\"y2\", yScale(sigma2hat))\n  .attr(\"stroke\", \"currentColor\");\n\n  wrap.append(svg.node());\n\n  // Hitbox fills wrapper; subtract margins in code\n  const hit = html`&lt;div style=\"\n    position:absolute; left:0; top:0; width:100%; height:100%;\n    cursor:crosshair; background:transparent; touch-action:none;\n  \"&gt;&lt;/div&gt;`;\n  wrap.append(hit);\n\n  const state = { x: muhat, y: sigma2hat, dragging: false };\n  wrap.value = { ...state };            // &lt;-- expose full state via the view's value\n\n  function emit() {\n    wrap.value = { ...state };          // &lt;-- assign a fresh object so equality changes\n    wrap.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  }\n\n  function setFromEvent(evt) {\n    const r = hit.getBoundingClientRect();\n    let px = evt.clientX - r.left, py = evt.clientY - r.top;\n    px = Math.min(Math.max(0, px - marginLeft), innerW);\n    py = Math.min(Math.max(0, py - marginTop), innerH);\n    state.x = muDomain[0] + (px / innerW) * (muDomain[1] - muDomain[0]);\n    state.y = sig2Domain[1] - (py / innerH) * (sig2Domain[1] - sig2Domain[0]);\n    vline.attr(\"x1\", xScale(state.x)).attr(\"x2\", xScale(state.x));\n    hline.attr(\"y1\", yScale(state.y)).attr(\"y2\", yScale(state.y));\n    emit();                              // &lt;-- notify dependents\n  }\n\n  // RAF throttle for move\n  let raf = 0;\n  function scheduleUpdate(e) {\n    if (raf) return;\n    raf = requestAnimationFrame(() =&gt; { raf = 0; setFromEvent(e); });\n  }\n\n  function setDragging(on) {\n    if (state.dragging === on) return;\n    state.dragging = on;\n    emit();                              // &lt;-- notify on drag state changes too\n  }\n\n  hit.addEventListener(\"pointerdown\", (e) =&gt; {\n    setDragging(true);\n    setFromEvent(e);\n    hit.setPointerCapture(e.pointerId);\n  });\n\n  hit.addEventListener(\"pointermove\", (e) =&gt; {\n    if (state.dragging) scheduleUpdate(e);\n  });\n\n  const endDrag = () =&gt; setDragging(false);\n  hit.addEventListener(\"pointerup\", endDrag);\n  hit.addEventListener(\"pointercancel\", endDrag);\n  hit.addEventListener(\"pointerleave\", endDrag);\n\n  // initial notify\n  queueMicrotask(emit);\n\n  return wrap;                           // &lt;-- the view element\n}\n\n\n// isDragging = leftPanel.dragging ?? false\n// console.log(isDragging)\n// numPts = isDragging ? 100 : 200   // fewer x-samples while dragging\n// pdfStroke = isDragging ? 1 : 2   // thinner line while dragging\n\n\n// // Read probe values reactively (no extra visible block)\n// probeRaw = Generators.input(leftPanel.hit);\n// probeMu = (Array.isArray(probeRaw) ? probeRaw[0] : probeRaw?.x) ?? muhat;\n// probeSig2 = (Array.isArray(probeRaw) ? probeRaw[1] : probeRaw?.y) ?? sigma2hat;\n// probeLL = logLik(probeMu, probeSig2);\n\n\n// leftPanel is now { x, y, dragging }\nisDragging = leftPanel.dragging\nnumPts     = isDragging ? 100 : 200\npdfStroke  = isDragging ? 1   : 2\n\n// if you want the probe to follow the crosshair:\nprobeMu   = leftPanel.x\nprobeSig2 = leftPanel.y\nprobeLL = logLik(probeMu, probeSig2);\n\n\n// --- Right panel ---\nnormalPdf = (x, mu, sig2) =&gt; (1 / Math.sqrt(2 * Math.PI * sig2)) * Math.exp(-((x - mu) ** 2) / (2 * sig2))\n\nxExtent = d3.extent(X)\nxPad = 3 * Math.sqrt(probeSig2)\nxMin = Math.min(xExtent[0], probeMu - xPad)\nxMax = Math.max(xExtent[1], probeMu + xPad)\n\n// PDF curve\nxs = d3.range(numPts).map(i =&gt; xMin + (i / (numPts - 1)) * (xMax - xMin))\npdfYs = xs.map(x =&gt; normalPdf(x, probeMu, probeSig2))\n\n// Density reducer\ndensityReducer = (values, extent) =&gt; values.length / n / (extent.x2 - extent.x1)\n\n// Compute yMax\nbins_ = d3.bin().domain([xMin, xMax]).thresholds(bins)(X);\ndensities = bins_.map(b =&gt;\n  (b.length / Math.max(1, n)) / Math.max(1e-9, (b.x1 - b.x0))\n);\nyMax = d3.max(densities);\n\nrightPanel = Plot.plot({\n  width: plotW, height: plotH,\n  marginLeft, marginBottom,\n  x: { label: \"Data\", domain: [xMin, xMax] },\n  y: { label: \"Density\" },\n  marks: [\n    Plot.rectY(\n      X,\n      Plot.binX(\n        { y: densityReducer },\n        {\n          x: d =&gt; d,\n          thresholds: bins,\n          inset: 0,\n          fill: \"var(--brand-red)\",\n          fillOpacity: 0.2\n        }\n      )\n    ),\n    Plot.line(xs.map((x, i) =&gt; ({ x, y: pdfYs[i] })), {\n      x: \"x\", y: \"y\",\n      stroke: \"var(--brand-red)\",\n      strokeWidth: pdfStroke\n    }),\n    Plot.ruleX(X, {\n      y1: 0,\n      y2: yMax * 0.05,\n      stroke: \"var(--brand-red)\",\n      strokeWidth: 2,\n      strokeOpacity: 0.8\n    }),\n    Plot.ruleY([0]),\n    Plot.ruleX([probeMu], { stroke: \"currentColor\", strokeDasharray: \"4,3\" })\n  ]\n})\n\nprobeInfo = md`**Probe:** ${tex`\\mu=${probeMu.toFixed(2)},\\ \\sigma^2=${probeSig2.toFixed(3)}\\ \\Rightarrow\\ \\log f(\\underline{x} \\mid \\mu, \\sigma^2)=${probeLL.toFixed(2)}`}`\n\nhtml`&lt;div style=\"max-width:100%;\"&gt;\n&lt;div style=\"display:flex; gap:${gap}px; align-items:flex-start; flex-wrap:nowrap; overflow-x:auto;\"&gt;\n&lt;div style=\"flex:0 0 ${plotW}px;\"&gt;${viewof leftPanel}&lt;/div&gt;\n&lt;div style=\"flex:0 0 ${plotW}px;\"&gt;${rightPanel}&lt;/div&gt;\n&lt;/div&gt;\n&lt;div style=\"margin-top:10px; margin-bottom:-10px\"&gt;${probeInfo}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;style&gt;\n/* ---------- Collapsible container ---------- */\ndetails.controls-root &gt; summary {\n  list-style: none; cursor: pointer; user-select: none;\n  background: var(--surface, #f1f5f9);\n  border: 1px solid var(--border, #e5e7eb);\n  border-radius: 10px;\n  padding: 8px 10px;\n  font-weight: 600; font-size: 13px;\n  color: var(--fg-strong, #334155);\n  display: flex; align-items: center; gap: 8px;\n}\ndetails.controls-root &gt; summary::-webkit-details-marker { display: none; }\ndetails.controls-root[open] &gt; summary { border-bottom-left-radius: 0; border-bottom-right-radius: 0; }\n\n/* ---------- Two-column grid ---------- */\n.controls-grid {\n  display: grid;\n  grid-template-columns: minmax(260px, 1fr) minmax(260px, 1fr);\n  gap: 12px;\n  border: 1px solid var(--border, #e5e7eb); border-top: none;\n  border-bottom-left-radius: 10px; border-bottom-right-radius: 10px;\n  padding: 10px;\n  background: var(--surface-weak, #fafafa);\n  overflow-x: auto; /* prefer scroll over wrapping on narrow screens */\n}\n\n.controls-col { min-width: 260px; }\n.controls-col h4 {\n  margin: 0 0 6px 0;\n  font-size: 12px; font-weight: 700;\n  color: var(--fg-muted, #475569);\n}\n\n/* ---------- Compact Inputs (fixed slider alignment) ---------- */\n.controls-col label { font-size: 12px !important; }\n\n.controls-col input[type=\"range\"] {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 100%;\n  height: 18px;                     /* overall control box height */\n  background: transparent;\n  vertical-align: middle;\n  accent-color: var(--brand-teal, #55C3CB);\n}\n\n/* WebKit track */\n.controls-col input[type=\"range\"]::-webkit-slider-runnable-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* WebKit thumb */\n.controls-col input[type=\"range\"]::-webkit-slider-thumb {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n  /* Center the 12px thumb over the 4px track: (4 - 12)/2 = -4px */\n  margin-top: -4px;\n}\n\n/* Firefox track */\n.controls-col input[type=\"range\"]::-moz-range-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* Firefox thumb (auto-centred; no negative margin needed) */\n.controls-col input[type=\"range\"]::-moz-range-thumb {\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n}\n\n/* Optional: hide the default filled \"progress\" colour in Firefox */\n.controls-col input[type=\"range\"]::-moz-range-progress {\n  height: 4px; border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n\n/* ---------- Button ---------- */\n.btn-resim button {\n  background: var(--brand-teal, #55C3CB);\n  color: #fff;\n  border: none;\n  border-radius: 6px;\n  padding: 6px 12px;\n  font-weight: 600;\n  font-size: 12px;\n  cursor: pointer;\n  transition: filter 120ms ease, background-color 120ms ease;\n}\n\n/* hover effect */\n.btn-resim button:hover {\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 88%, black);\n}\n\n/* chevron */\ndetails.controls-root &gt; summary::before {\n  content: \"\"; display: inline-block; margin-right: 8px; width: 0; height: 0;\n  border-style: solid; border-width: 3px 0 3px 6px;\n  border-color: transparent transparent transparent var(--fg-strong);\n  transform: rotate(0deg); transform-origin: 3px 50%; transition: transform 50ms ease;\n}\ndetails.controls-root[open] &gt; summary::before { transform: rotate(90deg); }\n&lt;/style&gt;`\n\n\n\n\n\n\n\nviewof controls = {\n  const resimBtn = Inputs.button(\"Resimulate data\");\n  resimBtn.classList.add(\"btn-resim\");\n\n  const leftForm = Inputs.form({\n    n: Inputs.range([1, 400], { value: 10, step: 1, label: md`${tex`n`}` }),\n    trueMu: Inputs.range([-3, 3], { value: 0.5, step: 0.05, label: md`${tex`\\mu_{\\text{true}}`}` }),\n    trueSigma: Inputs.range([0.3, 3], { value: 1.2, step: 0.05, label: md`${tex`\\sigma_{\\text{true}}`}` })\n  }, { submit: false });\n\n  const rightForm = Inputs.form({\n    muHalfWidth: Inputs.range([1, 5], { value: 2.5, step: 0.05, label: md`${tex`\\mu\\text{-range}`}` }),\n    sig2Max: Inputs.range([0.2, 6], { value: 3, step: 0.05, label: md`${tex`\\sigma^2\\text{ max}`}` }),\n    gridN: Inputs.range([25, 121], { value: 81, step: 4, label: \"Grid size\" }),\n    bins: Inputs.range([10, 60], { value: 24, step: 1, label: \"Histogram bins\" })\n  }, { submit: false });\n\n  const root = html`&lt;details class=\"controls-root\" open&gt;\n    &lt;summary&gt;Simulation controls&lt;/summary&gt;\n    &lt;div class=\"controls-grid\"&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Data & truth&lt;/h4&gt;&lt;/div&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Grid & display&lt;/h4&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/details&gt;`;\n\n  const grid = root.querySelector(\".controls-grid\");\n  const leftCol = grid.children[0];\n  const rightCol = grid.children[1];\n\n  leftCol.append(leftForm);\n\n  rightCol.append(rightForm);\n  const btnWrap = html`&lt;div style=\"display:flex; justify-content:flex-start; margin-top:6px;\"&gt;&lt;/div&gt;`;\n  btnWrap.append(resimBtn);\n  rightCol.append(btnWrap);\n\n  // --- Merge values\n  const getValue = () =&gt; ({ ...leftForm.value, ...rightForm.value, resim: resimBtn.value });\n  const update = () =&gt; {\n    root.value = getValue();\n    root.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  };\n\n  // ONE listener for all children (sliders + button)\n  grid.addEventListener(\"input\", update);\n\n  queueMicrotask(update);\n  return root;\n}\n\n// --- Expose reactive vars\nn = controls.n\ntrueMu = controls.trueMu\ntrueSigma = controls.trueSigma\nmuHalfWidth = controls.muHalfWidth\nsig2Max = controls.sig2Max\ngridN = controls.gridN\nbins = controls.bins\nresim = controls.resim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Computing Likelihood Functions\nThis is a very important part of the course. ?sec-binom-text\n\n\n\n\n\n\n\nExample 5.1 (Bernoulli Likelihood) Suppose we have \\(n\\) i.i.d. data distributed according to the Bernoulli distribution 1.1:\n\\[\n  Y_i \\mid p \\sim \\textrm{Bernoulli}(p).\n\\] Derive the likelihood function.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2 (Bernoulli Likelihood) Suppose \\(n=5\\) trials give data \\(\\underline{x} = (1,0,1,1,0)\\). Then \\(\\sum\\_i x\\_i = 3\\). \\[\nL(p ; \\underline{x}) = p^3 (1-p)^2.\n\\]\n\nIf \\(p=0.5\\), \\(L(0.5) = 0.5^3 \\cdot 0.5^2 = 0.03125\\).\nIf \\(p=0.7\\), \\(L(0.7) = 0.7^3 \\cdot 0.3^2 \\approx 0.0309\\).\nIf \\(p=0.8\\), \\(L(0.8) = 0.8^3 \\cdot 0.2^2 = 0.0205\\).\n\nSo the data are slightly more likely under \\(p=0.5\\) than \\(p=0.8\\).",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#maximum-likelihood-estimation",
    "href": "lectures/part2/4_likelihood.html#maximum-likelihood-estimation",
    "title": "5  Likelihood",
    "section": "5.3 Maximum Likelihood Estimation",
    "text": "5.3 Maximum Likelihood Estimation\n\n\n\n\n\n\nNoteDefinition (Maximum Likelihood Estimator (MLE))\n\n\n\n\nDefinition 5.2 (Maximum Likelihood Estimator (MLE)) The MLE is the parameter value \\(\\hat{\\theta}\\) that maximises the likelihood function: \\[\n  \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_{\\theta \\in \\Theta} L(\\theta \\mid \\underline{x}).\n  \\]\n\n\n\nBecause logs are monotone, it is usually easier to maximise the log-likelihood:\n\\[\n\\ell(\\theta ; \\underline{x}) = \\log L(\\theta ; \\underline{x}).\n\\]\n\n5.3.1 Computing MLEs\n\n\n\n\n\n\n\nExample 5.3 (Bernoulli MLE) From Example 5.1, we have\n\\[\nL(\\theta ; x)\n\\]\nCompute the MLE of \\(p\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.1. From the provided likelihood, we first compute the log-likelihood:\n\\[\n\\ell(p \\mid \\underline{x})\n= \\Big(\\sum_i x_i \\Big)\\log p + \\Big(n-\\sum_i x_i\\Big)\\log(1-p).\n\\] Differentiate and solve: \\[\n\\frac{\\partial}{\\partial p}\\ell(p \\mid \\underline{x})\n= \\frac{\\sum_i x_i}{p} - \\frac{n - \\sum_i x_i}{1-p} = 0.\n\\] This gives \\[\n\\hat{p}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nthe sample proportion of successes.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.4 (Binomial MLE) Suppose we have a single Binomial 1.2 observation \\[\nX \\mid \\theta \\sim \\textrm{Bin}(n, \\theta).\n\\] Derive its likelihood function.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.2. Since we only have one observation, the likelihood function is just the distribution of the observation: \\[\nL(\\theta ; x) = f(x \\mid \\theta) = \\binom{n}{x} \\theta^x(1-\\theta)^{n-x}.\n\\] The log-likelihood is thus \\[\n\\ell(\\theta; x) = \\log f(x \\mid \\theta) = \\log\\left(\\binom{n}{x}\\right) + x\\log\\theta + (n-x)\\log(1-\\theta).\n\\] Differentiating1, we obtain \\[\n\\frac{\\partial}{\\partial \\theta}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 5.5 (Binomial MLE) Suppose we have \\(n\\) independent Binomial data: \\[\nX_i \\mid \\theta \\sim \\textrm{Bin}(n_i, \\theta).\n\\] Note here that each Binomial data has a different number of Bernoulli trials \\(n_i\\), which are known.\nDerive its likelihood function.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.3. The likelihood function is the joint distribution of the \\(n\\) observations \\[\nf(\\underline{x} \\mid \\theta) = \\prod_{i=1}^n \\binom{n_i}{x_i} \\theta^{x_i}(1-\\theta)^{n_i - x_i}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nImportant✨ Big picture:\n\n\n\n\nLikelihood tells us how plausible each parameter value is, given the data.\nThe MLE chooses the parameter value that makes the observed data most likely.\nLater, in Bayesian inference, we will combine the likelihood with a prior distribution on \\(\\theta\\) to obtain a posterior distribution.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#big-picture",
    "href": "lectures/part2/4_likelihood.html#big-picture",
    "title": "5  Likelihood",
    "section": "5.5 Big Picture",
    "text": "5.5 Big Picture\n\nLikelihood tells us how plausible each parameter value is, given the data.\nThe MLE chooses the parameter value that makes the observed data most likely.\nLater, in Bayesian inference, we will combine the likelihood with a prior distribution on $$ to obtain a posterior distribution.\n\n\n\nhtml`&lt;style&gt;\n.plot--large-axis .x-axis .label,\n.plot--large-axis .y-axis .label {\nfont-size: 24px; font-weight: 600;\n}\n.plot--large-axis .x-axis text,\n.plot--large-axis .y-axis text {\nfont-size: 12px; /* tick labels (optional) */\n}\n&lt;/style&gt;`\n\n\n\n\n\n\n\nX = {\n  // Trigger on: resim button, n, trueMu, or trueSigma changes\n  resim; n; trueMu; trueSigma;\n  \n  const m = trueMu, s = trueSigma;\n  const out = new Array(n);\n  for (let i = 0; i &lt; n; i += 2) {\n    const u = Math.random(), v = Math.random();\n    const R = Math.sqrt(-2 * Math.log(u)), T = 2 * Math.PI * v;\n    const z0 = R * Math.cos(T), z1 = R * Math.sin(T);\n    out[i] = m + s * z0;\n    if (i + 1 &lt; n) out[i + 1] = m + s * z1;\n  }\n  return out;\n};\n\nxbar = d3.mean(X);\nS = d3.sum(X, x =&gt; (x - xbar) ** 2);\n\nlogLik = (mu, sigma2) =&gt; {\n  const ss = d3.sum(X, x =&gt; (x - mu) ** 2);\n  return -0.5 * n * Math.log(2 * Math.PI * sigma2) - 0.5 * ss / sigma2;\n};\n\nmuhat = xbar;\nsigma2hat = S / n;\n\n// --- Grid for contours (axes ticks) ---\n// These only need to update when display parameters change\nmuGrid = d3.ticks(muhat - muHalfWidth, muhat + muHalfWidth, gridN)\nsig2Grid = d3.ticks(Math.max(0.05, sigma2hat / 20), sig2Max, gridN).filter(s =&gt; s &gt; 0)\n\n// --- Build a row-major grid for Plot.raster / Plot.contour ---\nnx = muGrid.length\nny = sig2Grid.length\n\n// Memoize the expensive log-likelihood computation\nllValues = {\n  const z = new Float64Array(nx * ny);\n  for (let j = 0; j &lt; ny; ++j) {\n    for (let i = 0; i &lt; nx; ++i) {\n      const s2 = Math.max(1e-6, sig2Grid[j]);\n      z[j * nx + i] = logLik(muGrid[i], s2);\n    }\n  }\n  return z;\n}\n\n\n// ---- Geometry / domains (responsive) ----\ngap = 20\ncontainerW = width\nplotW = Math.round(Math.max(240, Math.min(600, (containerW - gap) / 2 - 15)))\nplotH = Math.round(plotW * 0.8)\n\nmarginTop = 20\nmarginRight = 35\nmarginBottom = 20\nmarginLeft = 35\ninnerW = plotW - marginLeft - marginRight\ninnerH = plotH - marginTop - marginBottom\n\nmuDomain = [muGrid[0], muGrid[muGrid.length - 1]]\nsig2Domain = [sig2Grid[0], sig2Grid[sig2Grid.length - 1]]\n\n// ---- Colour scaling (robust & reactive) ----\nllMax = d3.max(llValues)\nllRel = Float64Array.from(llValues, v =&gt; v - llMax)\n\n// Use more efficient quantile computation\nllRelFinite = Array.from(llRel).filter(Number.isFinite)\nllRelSorted = llRelFinite.slice().sort(d3.ascending)\nq05 = d3.quantileSorted(llRelSorted, 0.05) ?? d3.min(llRelFinite)\n\ncolorDomain = [q05, 0]\nllRelClipped = Float64Array.from(llRel, v =&gt; Math.max(q05, v))\ncontourThresholds = d3.ticks(q05, 0, 10)\n\n\n// ---- Left panel: raster (clipped) + contour (same grid) ----\nviewof leftPanel = {\n  const wrap = html`&lt;div style=\"\n    position: relative;\n    width: ${plotW}px;\n    height: ${plotH}px;\n    flex: 0 0 ${plotW}px;\n    line-height: 0;\n  \"&gt;&lt;/div&gt;`;\n\n  const base = Plot.plot({\n    width: plotW, height: plotH,\n    marginLeft, marginRight, marginTop, marginBottom,\n    style: {\n      background: \"var(--brand-bg)\",\n      color: \"var(--brand-fg)\"\n    },\n    x: { label: \"μ\", domain: muDomain },\n    y: { label: \"σ²\", domain: sig2Domain },\n    color: {\n      type: \"symlog\",\n      domain: colorDomain,\n      clamp: true,\n      scheme: \"turbo\",\n      legend: false\n    },\n    marks: [\n      Plot.raster(llRelClipped, {\n        width: nx, height: ny,\n        x1: muDomain[0], x2: muDomain[1],\n        y1: sig2Domain[0], y2: sig2Domain[1],\n        interpolate: \"nearest\"\n      }),\n      Plot.dot([{ mu: muhat, sig2: sigma2hat }], {\n        x: \"mu\", y: \"sig2\", r: 5, stroke: \"currentColor\", fill: \"var(--brand-bg)\"\n      })\n    ]\n  });\n\n  base.classList.add(\"plot--large-axis\");\n  base.style.display = \"block\";\n  wrap.append(base);\n\n\n  // Crosshair overlay fills wrapper\n  const svg = d3.create(\"svg\")\n    .attr(\"width\", plotW).attr(\"height\", plotH)\n    .style(\"position\", \"absolute\").style(\"left\", \"0\").style(\"top\", \"0\")\n    .style(\"width\", \"100%\").style(\"height\", \"100%\")\n    .style(\"pointer-events\", \"none\");\n\n\n\n  const xScale = d3.scaleLinear().domain(muDomain).range([marginLeft, marginLeft + innerW]);\n  const yScale = d3.scaleLinear().domain(sig2Domain).range([marginTop + innerH, marginTop]);\n\n  const vline = svg.append(\"line\")\n  .attr(\"x1\", xScale(muhat)).attr(\"x2\", xScale(muhat))\n  .attr(\"y1\", marginTop).attr(\"y2\", marginTop + innerH)\n  .attr(\"stroke\", \"currentColor\");\n\n  const hline = svg.append(\"line\")\n  .attr(\"x1\", marginLeft).attr(\"x2\", marginLeft + innerW)\n  .attr(\"y1\", yScale(sigma2hat)).attr(\"y2\", yScale(sigma2hat))\n  .attr(\"stroke\", \"currentColor\");\n\n  wrap.append(svg.node());\n\n  // Hitbox fills wrapper; subtract margins in code\n  const hit = html`&lt;div style=\"\n    position:absolute; left:0; top:0; width:100%; height:100%;\n    cursor:crosshair; background:transparent; touch-action:none;\n  \"&gt;&lt;/div&gt;`;\n  wrap.append(hit);\n\n  const state = { x: muhat, y: sigma2hat, dragging: false };\n  wrap.value = { ...state };            // &lt;-- expose full state via the view's value\n\n  function emit() {\n    wrap.value = { ...state };          // &lt;-- assign a fresh object so equality changes\n    wrap.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  }\n\n  function setFromEvent(evt) {\n    const r = hit.getBoundingClientRect();\n    let px = evt.clientX - r.left, py = evt.clientY - r.top;\n    px = Math.min(Math.max(0, px - marginLeft), innerW);\n    py = Math.min(Math.max(0, py - marginTop), innerH);\n    state.x = muDomain[0] + (px / innerW) * (muDomain[1] - muDomain[0]);\n    state.y = sig2Domain[1] - (py / innerH) * (sig2Domain[1] - sig2Domain[0]);\n    vline.attr(\"x1\", xScale(state.x)).attr(\"x2\", xScale(state.x));\n    hline.attr(\"y1\", yScale(state.y)).attr(\"y2\", yScale(state.y));\n    emit();                              // &lt;-- notify dependents\n  }\n\n  // RAF throttle for move\n  let raf = 0;\n  function scheduleUpdate(e) {\n    if (raf) return;\n    raf = requestAnimationFrame(() =&gt; { raf = 0; setFromEvent(e); });\n  }\n\n  function setDragging(on) {\n    if (state.dragging === on) return;\n    state.dragging = on;\n    emit();                              // &lt;-- notify on drag state changes too\n  }\n\n  hit.addEventListener(\"pointerdown\", (e) =&gt; {\n    setDragging(true);\n    setFromEvent(e);\n    hit.setPointerCapture(e.pointerId);\n  });\n\n  hit.addEventListener(\"pointermove\", (e) =&gt; {\n    if (state.dragging) scheduleUpdate(e);\n  });\n\n  const endDrag = () =&gt; setDragging(false);\n  hit.addEventListener(\"pointerup\", endDrag);\n  hit.addEventListener(\"pointercancel\", endDrag);\n  hit.addEventListener(\"pointerleave\", endDrag);\n\n  // initial notify\n  queueMicrotask(emit);\n\n  return wrap;                           // &lt;-- the view element\n}\n\n\n// isDragging = leftPanel.dragging ?? false\n// console.log(isDragging)\n// numPts = isDragging ? 100 : 200   // fewer x-samples while dragging\n// pdfStroke = isDragging ? 1 : 2   // thinner line while dragging\n\n\n// // Read probe values reactively (no extra visible block)\n// probeRaw = Generators.input(leftPanel.hit);\n// probeMu = (Array.isArray(probeRaw) ? probeRaw[0] : probeRaw?.x) ?? muhat;\n// probeSig2 = (Array.isArray(probeRaw) ? probeRaw[1] : probeRaw?.y) ?? sigma2hat;\n// probeLL = logLik(probeMu, probeSig2);\n\n\n// leftPanel is now { x, y, dragging }\nisDragging = leftPanel.dragging\nnumPts     = isDragging ? 100 : 200\npdfStroke  = isDragging ? 1   : 2\n\n// if you want the probe to follow the crosshair:\nprobeMu   = leftPanel.x\nprobeSig2 = leftPanel.y\nprobeLL = logLik(probeMu, probeSig2);\n\n\n// --- Right panel ---\nnormalPdf = (x, mu, sig2) =&gt; (1 / Math.sqrt(2 * Math.PI * sig2)) * Math.exp(-((x - mu) ** 2) / (2 * sig2))\n\nxExtent = d3.extent(X)\nxPad = 3 * Math.sqrt(probeSig2)\nxMin = Math.min(xExtent[0], probeMu - xPad)\nxMax = Math.max(xExtent[1], probeMu + xPad)\n\n// PDF curve\nxs = d3.range(numPts).map(i =&gt; xMin + (i / (numPts - 1)) * (xMax - xMin))\npdfYs = xs.map(x =&gt; normalPdf(x, probeMu, probeSig2))\n\n// Density reducer\ndensityReducer = (values, extent) =&gt; values.length / n / (extent.x2 - extent.x1)\n\n// Compute yMax\nbins_ = d3.bin().domain([xMin, xMax]).thresholds(bins)(X);\ndensities = bins_.map(b =&gt;\n  (b.length / Math.max(1, n)) / Math.max(1e-9, (b.x1 - b.x0))\n);\nyMax = d3.max(densities);\n\nrightPanel = Plot.plot({\n  width: plotW, height: plotH,\n  marginLeft, marginBottom,\n  x: { label: \"Data\", domain: [xMin, xMax] },\n  y: { label: \"Density\" },\n  marks: [\n    Plot.rectY(\n      X,\n      Plot.binX(\n        { y: densityReducer },\n        {\n          x: d =&gt; d,\n          thresholds: bins,\n          inset: 0,\n          fill: \"var(--brand-red)\",\n          fillOpacity: 0.2\n        }\n      )\n    ),\n    Plot.line(xs.map((x, i) =&gt; ({ x, y: pdfYs[i] })), {\n      x: \"x\", y: \"y\",\n      stroke: \"var(--brand-red)\",\n      strokeWidth: pdfStroke\n    }),\n    Plot.ruleX(X, {\n      y1: 0,\n      y2: yMax * 0.05,\n      stroke: \"var(--brand-red)\",\n      strokeWidth: 2,\n      strokeOpacity: 0.8\n    }),\n    Plot.ruleY([0]),\n    Plot.ruleX([probeMu], { stroke: \"currentColor\", strokeDasharray: \"4,3\" })\n  ]\n})\n\nprobeInfo = md`**Probe:** ${tex`\\mu=${probeMu.toFixed(2)},\\ \\sigma^2=${probeSig2.toFixed(3)}\\ \\Rightarrow\\ \\log f(\\underline{x} \\mid \\mu, \\sigma^2)=${probeLL.toFixed(2)}`}`\n\nhtml`&lt;div style=\"max-width:100%;\"&gt;\n&lt;div style=\"display:flex; gap:${gap}px; align-items:flex-start; flex-wrap:nowrap; overflow-x:auto;\"&gt;\n&lt;div style=\"flex:0 0 ${plotW}px;\"&gt;${viewof leftPanel}&lt;/div&gt;\n&lt;div style=\"flex:0 0 ${plotW}px;\"&gt;${rightPanel}&lt;/div&gt;\n&lt;/div&gt;\n&lt;div style=\"margin-top:10px; margin-bottom:-10px\"&gt;${probeInfo}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;style&gt;\n/* ---------- Collapsible container ---------- */\ndetails.controls-root &gt; summary {\n  list-style: none; cursor: pointer; user-select: none;\n  background: var(--surface, #f1f5f9);\n  border: 1px solid var(--border, #e5e7eb);\n  border-radius: 10px;\n  padding: 8px 10px;\n  font-weight: 600; font-size: 13px;\n  color: var(--fg-strong, #334155);\n  display: flex; align-items: center; gap: 8px;\n}\ndetails.controls-root &gt; summary::-webkit-details-marker { display: none; }\ndetails.controls-root[open] &gt; summary { border-bottom-left-radius: 0; border-bottom-right-radius: 0; }\n\n/* ---------- Two-column grid ---------- */\n.controls-grid {\n  display: grid;\n  grid-template-columns: minmax(260px, 1fr) minmax(260px, 1fr);\n  gap: 12px;\n  border: 1px solid var(--border, #e5e7eb); border-top: none;\n  border-bottom-left-radius: 10px; border-bottom-right-radius: 10px;\n  padding: 10px;\n  background: var(--surface-weak, #fafafa);\n  overflow-x: auto; /* prefer scroll over wrapping on narrow screens */\n}\n\n.controls-col { min-width: 260px; }\n.controls-col h4 {\n  margin: 0 0 6px 0;\n  font-size: 12px; font-weight: 700;\n  color: var(--fg-muted, #475569);\n}\n\n/* ---------- Compact Inputs (fixed slider alignment) ---------- */\n.controls-col label { font-size: 12px !important; }\n\n.controls-col input[type=\"range\"] {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 100%;\n  height: 18px;                     /* overall control box height */\n  background: transparent;\n  vertical-align: middle;\n  accent-color: var(--brand-teal, #55C3CB);\n}\n\n/* WebKit track */\n.controls-col input[type=\"range\"]::-webkit-slider-runnable-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* WebKit thumb */\n.controls-col input[type=\"range\"]::-webkit-slider-thumb {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n  /* Center the 12px thumb over the 4px track: (4 - 12)/2 = -4px */\n  margin-top: -4px;\n}\n\n/* Firefox track */\n.controls-col input[type=\"range\"]::-moz-range-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* Firefox thumb (auto-centred; no negative margin needed) */\n.controls-col input[type=\"range\"]::-moz-range-thumb {\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n}\n\n/* Optional: hide the default filled \"progress\" colour in Firefox */\n.controls-col input[type=\"range\"]::-moz-range-progress {\n  height: 4px; border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n\n/* ---------- Button ---------- */\n.btn-resim button {\n  background: var(--brand-teal, #55C3CB);\n  color: #fff;\n  border: none;\n  border-radius: 6px;\n  padding: 6px 12px;\n  font-weight: 600;\n  font-size: 12px;\n  cursor: pointer;\n  transition: filter 120ms ease, background-color 120ms ease;\n}\n\n/* hover effect */\n.btn-resim button:hover {\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 88%, black);\n}\n\n/* chevron */\ndetails.controls-root &gt; summary::before {\n  content: \"\"; display: inline-block; margin-right: 8px; width: 0; height: 0;\n  border-style: solid; border-width: 3px 0 3px 6px;\n  border-color: transparent transparent transparent var(--fg-strong);\n  transform: rotate(0deg); transform-origin: 3px 50%; transition: transform 50ms ease;\n}\ndetails.controls-root[open] &gt; summary::before { transform: rotate(90deg); }\n&lt;/style&gt;`\n\n\n\n\n\n\n\nviewof controls = {\n  const resimBtn = Inputs.button(\"Resimulate data\");\n  resimBtn.classList.add(\"btn-resim\");\n\n  const leftForm = Inputs.form({\n    n: Inputs.range([1, 400], { value: 10, step: 1, label: md`${tex`n`}` }),\n    trueMu: Inputs.range([-3, 3], { value: 0.5, step: 0.05, label: md`${tex`\\mu_{\\text{true}}`}` }),\n    trueSigma: Inputs.range([0.3, 3], { value: 1.2, step: 0.05, label: md`${tex`\\sigma_{\\text{true}}`}` })\n  }, { submit: false });\n\n  const rightForm = Inputs.form({\n    muHalfWidth: Inputs.range([1, 5], { value: 2.5, step: 0.05, label: md`${tex`\\mu\\text{-range}`}` }),\n    sig2Max: Inputs.range([0.2, 6], { value: 3, step: 0.05, label: md`${tex`\\sigma^2\\text{ max}`}` }),\n    gridN: Inputs.range([25, 121], { value: 81, step: 4, label: \"Grid size\" }),\n    bins: Inputs.range([10, 60], { value: 24, step: 1, label: \"Histogram bins\" })\n  }, { submit: false });\n\n  const root = html`&lt;details class=\"controls-root\" open&gt;\n    &lt;summary&gt;Simulation controls&lt;/summary&gt;\n    &lt;div class=\"controls-grid\"&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Data & truth&lt;/h4&gt;&lt;/div&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Grid & display&lt;/h4&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/details&gt;`;\n\n  const grid = root.querySelector(\".controls-grid\");\n  const leftCol = grid.children[0];\n  const rightCol = grid.children[1];\n\n  leftCol.append(leftForm);\n\n  rightCol.append(rightForm);\n  const btnWrap = html`&lt;div style=\"display:flex; justify-content:flex-start; margin-top:6px;\"&gt;&lt;/div&gt;`;\n  btnWrap.append(resimBtn);\n  rightCol.append(btnWrap);\n\n  // --- Merge values\n  const getValue = () =&gt; ({ ...leftForm.value, ...rightForm.value, resim: resimBtn.value });\n  const update = () =&gt; {\n    root.value = getValue();\n    root.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  };\n\n  // ONE listener for all children (sliders + button)\n  grid.addEventListener(\"input\", update);\n\n  queueMicrotask(update);\n  return root;\n}\n\n// --- Expose reactive vars\nn = controls.n\ntrueMu = controls.trueMu\ntrueSigma = controls.trueSigma\nmuHalfWidth = controls.muHalfWidth\nsig2Max = controls.sig2Max\ngridN = controls.gridN\nbins = controls.bins\nresim = controls.resim",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "appendices/distribution-explorer.html",
    "href": "appendices/distribution-explorer.html",
    "title": "Distribution Explorer",
    "section": "",
    "text": "ACCENT  = \"var(--brand-teal)\"\nACCENT2 = \"var(--brand-red)\"\nGRID    = \"var(--border)\"\n\n// Numerical helpers\nlinspace = (a, b, n=401) =&gt; Array.from({length:n}, (_,i)=&gt; a + (b-a)*(i/(n-1)))\n\n// Log-Gamma (Lanczos) and friends (for stable PMFs/PDFs)\nfunction logGamma(z){\n  const g = 7;\n  const p = [\n    0.99999999999980993, 676.5203681218851, -1259.1392167224028,\n    771.32342877765313, -176.61502916214059, 12.507343278686905,\n    -0.13857109526572012, 9.9843695780195716e-6, 1.5056327351493116e-7\n  ];\n  if(z &lt; 0.5){\n    return Math.log(Math.PI) - Math.log(Math.sin(Math.PI*z)) - logGamma(1 - z);\n  }\n  z -= 1;\n  let x = p[0];\n  for(let i=1; i&lt;p.length; i++) x += p[i] / (z + i);\n  const t = z + g + 0.5;\n  return 0.5*Math.log(2*Math.PI) + (z+0.5)*Math.log(t) - t + Math.log(x);\n}\n\nlogChoose = (n,k) =&gt; logGamma(n+1) - logGamma(k+1) - logGamma(n-k+1)\n\n// Discrete PMFs (stable in log-domain)\nbinomPMF = (n,p,k) =&gt; Math.exp(logChoose(n,k) + k*Math.log(p) + (n-k)*Math.log(1-p))\npoisPMF  = (lambda,k) =&gt; Math.exp(k*Math.log(lambda) - lambda - logGamma(k+1))\ngeomPMF1 = (p,k) =&gt; p * Math.pow(1-p, k-1) // support {1,2,...}\nnegbinPMF = (r,p,x) =&gt; {\n  if(x &lt; r) return 0;\n  return Math.exp(logChoose(x-1, r-1) + r*Math.log(p) + (x-r)*Math.log(1-p));\n}\n\n// Continuous PDFs\nnormPDF = (x,mu,sig) =&gt; Math.exp(-0.5*((x-mu)/sig)**2)/(sig*Math.sqrt(2*Math.PI))\nlognormPDF = (x,mu,sig) =&gt; x&lt;=0 ? 0 : Math.exp(-((Math.log(x)-mu)**2)/(2*sig*sig)) / (x*sig*Math.sqrt(2*Math.PI))\nunifPDF = (x,a,b) =&gt; (x&gt;a && x&lt;b) ? 1/(b-a) : 0\nexpPDF = (x,lambda) =&gt; x&gt;0 ? lambda*Math.exp(-lambda*x) : 0\ngammaPDF = (x,a,b) =&gt; { // shape a, rate b\n  if(x&lt;=0) return 0;\n  return Math.exp(a*Math.log(b) - logGamma(a) + (a-1)*Math.log(x) - b*x)\n}\nbetaPDF = (x,a,b) =&gt; {\n  if(x&lt;=0 || x&gt;=1) return 0;\n  const logB = logGamma(a)+logGamma(b)-logGamma(a+b);\n  return Math.exp((a-1)*Math.log(x) + (b-1)*Math.log(1-x) - logB)\n}\nchisqPDF = (x,nu) =&gt; gammaPDF(x, nu/2, 1/2)\ntPDF = (x,nu) =&gt; {\n  const logC = logGamma((nu+1)/2) - (0.5*Math.log(nu*Math.PI) + logGamma(nu/2));\n  return Math.exp(logC - ((nu+1)/2)*Math.log(1 + (x*x)/nu));\n}\nfPDF = (x,d1,d2) =&gt; {\n  if(x&lt;=0) return 0;\n  const logC = logGamma((d1+d2)/2) - (logGamma(d1/2)+logGamma(d2/2)) + (d1/2)*Math.log(d1/d2);\n  return Math.exp(logC + (d1/2 - 1)*Math.log(x) - ((d1+d2)/2)*Math.log(1 + (d1/d2)*x))\n}\n\n// Convenient domain heuristics\ndomainNormal = (mu,sig) =&gt; [mu - 4*sig, mu + 4*sig]\ndomainGamma  = (a,b) =&gt; [0, Math.max(5, a/b + 6*Math.sqrt(a)/b)]\ndomainExp    = (lambda) =&gt; [0, Math.max(5, 6/lambda)]\ndomainBeta   = [0,1]\ndomainPos    = [0, 10]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulliBinomialPoissonGeometricNegative BinomialUniformExponentialGammaNormalLognormalChi-squaredStudent t\n\n\n\n{\n  const p = bern_p;\n  const data = [0, 1].map(x =&gt; ({ x, y: x ? p : 1 - p }));\n\n  const stats = md`**Mean** ${tex`= p`} = ${p.toFixed(2)}; \n  **Var** ${tex`= p(1-p)`} = ${(p * (1 - p)).toFixed(3)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n\n    y: { label: \"PMF\", domain: [0, 1] },\n    x: { label: \"x\", type: \"band\", domain: [0, 1] },  // &lt;-- band scale\n    marks: [\n      Plot.barY(data, { x: \"x\", y: \"y\", fill: ACCENT }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof bern_p = Inputs.range([0.01, 0.99], {\n  value: 0.6, step: 0.01, label: md`${tex`p`}`\n})\n\n\n\n\n\n\n\n\n\n{\n  const n = binom_n, p = binom_p;\n  const data = Array.from({ length: n + 1 }, (_, k) =&gt; ({ x: k, y: binomPMF(n, p, k) }));\n  const xDomain = Array.from({ length: n + 1 }, (_, k) =&gt; k);  // &lt;-- 0..n categories\n\n  const stats = md`**Mean** ${tex`= np`} = ${(n * p).toFixed(2)}; \n  **Var** ${tex`= np(1-p)`} = ${(n * p * (1 - p)).toFixed(2)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n    y: { label: \"PMF\" },\n    x: { label: \"x\", type: \"band\", domain: xDomain, padding: 0.05 },  // &lt;-- band scale\n    marks: [\n      Plot.barY(data, { x: \"x\", y: \"y\", fill: ACCENT }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof binom_n = Inputs.range([1, 200], { value: 20, step: 1,  label: md`${tex`n`}` })\nviewof binom_p = Inputs.range([0.01, 0.99], { value: 0.3, step: 0.01, label: md`${tex`p`}` })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  // compute locally (no exported names)\n  const L     = pois_lambda;\n  const kmax  = Math.max(15, Math.ceil(L + 6*Math.sqrt(L)));\n  const pois_data = Array.from({length: kmax + 1}, (_, k) =&gt; ({ x: k, y: poisPMF(L, k) }));\n\n  // build the UI bits\n  const stats = md`**Mean** ${tex`= \\lambda`} = ${L.toFixed(2)}; \n  **Var** ${tex`= \\lambda`} = ${L.toFixed(2)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n\n    y: { label: \"PMF\" },\n    x: { label: \"x\" },\n    marks: [\n      Plot.barY(pois_data, { x: \"x\", y: \"y\", fill: \"var(--brand-teal)\" }),\n      Plot.ruleY([0], { stroke: \"var(--border)\", strokeOpacity: 0.6 })\n    ]\n  });\n\n  // return both in a single wrapper\n  return html`&lt;div class=\"dist-panel\"&gt;\n    &lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;\n    ${plot}\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof pois_lambda = Inputs.range([0.2, 30], {\n  value: 6, step: 0.2, label: md`${tex`\\lambda`}`\n})\n\n\n\n\n\n\n\n\n\n{\n  // compute locally\n  const p = geom_p;\n  const raw   = Math.log(1e-4) / Math.log(1 - p);            // tail cutoff\n  const kmax  = Math.max(5, Math.min(200, Math.ceil(raw)));  // clamp for stability\n  const geom_data = Array.from({ length: kmax }, (_, i) =&gt; {\n    const k = i + 1; \n    return { x: k, y: geomPMF1(p, k) }; // PMF = p(1-p)^{k-1}\n  });\n\n  // stats readout\n  const stats = md`**Mean** ${tex`= 1/p`} = ${(1/p).toFixed(2)}; \n  **Var** ${tex`= (1-p)/p^2`} = ${(((1-p)/(p*p))).toFixed(2)}.`;\n\n  // plot\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PMF\" },\n    x: { label: \"x\" },\n    marks: [\n      Plot.barY(geom_data, { x: \"x\", y: \"y\", fill: ACCENT }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  // return one wrapper node\n  return html`&lt;div class=\"dist-panel\"&gt;\n    &lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;\n    ${plot}\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof geom_p = Inputs.range([0.02, 0.98], {\n  value: 0.3, step: 0.02, label: md`${tex`p`}`\n})\n\n\n\n\n\n\n\n\n\n{\n  const r = Math.round(negbin_r), p = negbin_p;\n  const mean = r / p, sd = Math.sqrt(r * (1 - p)) / p;\n  const xmax = Math.min(400, Math.ceil(mean + 6 * sd));\n\n  const data = Array.from({ length: Math.max(0, xmax - r + 1) }, (_, i) =&gt; {\n    const x = r + i;\n    return { x, y: negbinPMF(r, p, x) };\n  });\n\n  const stats = md`**Mean** ${tex`= r/p`} = ${(r/p).toFixed(2)}; \n  **Var** ${tex`= r(1-p)/p^2`} = ${((r*(1-p))/(p**2)).toFixed(2)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PMF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.barY(data, { x: \"x\", y: \"y\", fill: ACCENT }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof negbin_r = Inputs.range([1, 30], {value: 5, step: 1, label: md`${tex`r`}`})\nviewof negbin_p = Inputs.range([0.02, 0.98], {value: 0.4, step: 0.02, label: md`${tex`p`}`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const a = unif_a, b = unif_b;\n  const data = linspace(a - 1, b + 1, 401).map(x =&gt; ({ x, y: unifPDF(x, a, b) }));\n\n  const stats = md`**Mean** ${tex`= (a+b)/2`} = ${(((a+b)/2)).toFixed(2)}; \n  **Var** ${tex`= (b-a)^2/12`} = ${(((b-a)**2)/12).toFixed(3)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PDF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof unif_a = Inputs.range([-5, 5], {value: -2, step: 0.1, label: md`${tex`a`}`})\nviewof unif_b = Inputs.range([unif_a + 0.2, unif_a + 10], {value: 3, step: 0.1, label: md`${tex`b`}`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const λ = exp_lambda;\n  const [lo, hi] = domainExp(λ);\n  const data = linspace(lo, hi).map(x =&gt; ({ x, y: expPDF(x, λ) }));\n\n  const stats = md`**Mean** ${tex`= 1/\\lambda`} = ${(1/λ).toFixed(2)}; \n  **Var** ${tex`= 1/\\lambda^2`} = ${((1/(λ**2))).toFixed(2)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PDF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof exp_lambda = Inputs.range([0.2, 5], {value: 1, step: 0.1, label: md`${tex`\\lambda`}`})\n\n\n\n\n\n\n\n\n\n{\n  const a = gamma_a, b = gamma_b;\n  const [lo, hi] = domainGamma(a, b);\n  const data = linspace(lo, hi).map(x =&gt; ({ x, y: gammaPDF(x, a, b) }));\n\n  const stats = md`**Mean** ${tex`= a/b`} = ${(a/b).toFixed(2)}; \n  **Var** ${tex`= a/b^2`} = ${(a/(b**2)).toFixed(2)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PDF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof gamma_a = Inputs.range([0.5, 10], {value: 3, step: 0.1, label: md`${tex`a\\ \\text{(shape)}`}`})\nviewof gamma_b = Inputs.range([0.2, 5],  {value: 1, step: 0.1, label: md`${tex`b\\ \\text{(rate)}`}`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const μ = norm_mu, σ = norm_sig;\n  const [lo, hi] = domainNormal(μ, σ);\n  const data = linspace(lo, hi).map(x =&gt; ({ x, y: normPDF(x, μ, σ) }));\n\n  const stats = md`**Mean** ${tex`= \\mu`} = ${μ.toFixed(2)}; \n  **Var** ${tex`= \\sigma^2`} = ${(σ**2).toFixed(2)}.`;\n\n  const plot = Plot.plot({\n    width, height: 260,\n    style: { background: \"var(--plot-panel-bg)\", color: \"var(--brand-fg)\" },\n    x: { label: \"x\" }, y: { label: \"PDF\" },\n    marks: [\n      // shaded area under the PDF:\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n\n      // PDF outline:\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof norm_mu  = Inputs.range([-3, 3], {value: 0, step: 0.05, label: md`${tex`\\mu`}`})\nviewof norm_sig = Inputs.range([0.05, 3], {value: 1, step: 0.05, label: md`${tex`\\sigma`}`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const μ = lnorm_mu, σ = lnorm_sig;\n  const hi = Math.exp(μ + 5 * σ);\n  const data = linspace(0, hi).map(x =&gt; ({ x, y: lognormPDF(x, μ, σ) }));\n\n  const stats = md`**Mean** ${tex`= e^{\\mu+\\sigma^2/2}`} = ${(Math.exp(μ + (σ**2)/2)).toFixed(2)}; \n  **Var** ${tex`= (e^{\\sigma^2}-1)e^{2\\mu+\\sigma^2}`} = ${(((Math.exp(σ**2)-1)*Math.exp(2*μ + σ**2))).toFixed(2)}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PDF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n  \n}\n\n\n\n\n\n\n\nviewof lnorm_mu  = Inputs.range([-1.5, 2],  {value: 0,   step: 0.1, label: md`${tex`\\mu (\\log X)`}`})\nviewof lnorm_sig = Inputs.range([0.01, 1.5], {value: 0.6, step: 0.01, label: md`${tex`\\sigma (\\log X)`}`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const ν = Math.round(chi_nu);\n  const hi = Math.max(5, ν + 6 * Math.sqrt(2 * ν));\n  const data = linspace(0, hi).map(x =&gt; ({ x, y: chisqPDF(x, ν) }));\n\n  const stats = md`**Mean** ${tex`= \\nu`} = ${ν}; \n  **Var** ${tex`= 2\\nu`} = ${2*ν}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PDF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof chi_nu = Inputs.range([1, 40], {value: 8, step: 1, label: md`${tex`\\nu`}`})\n\n\n\n\n\n\n\n\n\n{\n  const ν = Math.round(t_nu);\n  const data = linspace(-6, 6).map(x =&gt; ({ x, y: tPDF(x, ν) }));\n\n  const stats = md`${tex`\\mathrm{E}[X]=0 (\\nu&gt;1)`}, \n  ${tex`\\operatorname{Var}(X)=\\nu/(\\nu-2)\\ (\\nu&gt;2)`}.  \n  Current: ${ν}.`;\n\n  const plot = Plot.plot({\n        width, height: 260,\n    style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n    },\n y: { label: \"PDF\" }, x: { label: \"x\" },\n    marks: [\n      Plot.lineY(data, { x: \"x\", y: \"y\", stroke: ACCENT, strokeWidth: 2 }),\n      Plot.areaY(data, { x: \"x\", y: \"y\", fill: ACCENT, fillOpacity: 0.18 }),\n      Plot.ruleY([0], { stroke: GRID, strokeOpacity: 0.6 })\n    ]\n  });\n\n  return html`&lt;div class=\"dist-panel\"&gt;&lt;div class=\"stats\"&gt;${stats}&lt;/div&gt;${plot}&lt;/div&gt;`;\n}\n\n\n\n\n\n\n\nviewof t_nu = Inputs.range([1, 40], {value: 5, step: 1, label: md`${tex`\\nu`}`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nUse the tabs to switch distributions. Adjust the sliders to see how the PMF/PDF changes.",
    "crumbs": [
      "Additional Material",
      "Distribution Explorer"
    ]
  },
  {
    "objectID": "lectures/part2/5_bayes_parameter_estimation.html#bayes-theorem",
    "href": "lectures/part2/5_bayes_parameter_estimation.html#bayes-theorem",
    "title": "6  Bayesian Inference",
    "section": "6.2 Bayes’ Theorem",
    "text": "6.2 Bayes’ Theorem\nBayesian inference takes its name from Bayes’ theorem, which tells us how to update our beliefs about \\(\\theta\\) after seeing data.\nRecall from probability that for events \\(A\\) and \\(B\\):\n\\[\n\\mathcal{P}(A \\mid B) = \\frac{\\mathrm{Pr}(B \\mid A)\\,\\mathrm{Pr}(A)}{\\mathrm{Pr}(B)}.\n\\]\nTo generalise this idea to random variables and densities, we use conditional densities:\n\\[\nf_{X \\mid Y}(x \\mid y) = \\frac{f_{XY}(x,y)}{f_Y(y)}.\n\\]\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark 6.1. Notation. We will usually write \\(f(x \\mid y)\\) rather than \\(f_{X \\mid Y}(x \\mid y)\\) to keep things simple. Both mean the same thing.\n\n\n\n\nUsing this, we obtain Bayes’ theorem for densities.\n\n\n\n\n\n\n\nTheorem 6.1 (Bayes’ Theorem for Densities.) For random variables \\(X\\) and \\(Y\\):\n\\[\nf(x \\mid y) = \\frac{f(y \\mid x)\\,f(x)}{f(y)}.\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof 6.1. By the definition of conditional density:\n\\[\nf(x \\mid y) = \\frac{f(x,y)}{f(y)},\n\\qquad\nf(y \\mid x) = \\frac{f(x,y)}{f(x)}.\n\\]\nRearranging the second expression gives \\(f(x,y) = f(y \\mid x) f(x)\\). Substituting this into the first gives\n\\[\nf(x \\mid y) = \\frac{f(y \\mid x) f(x)}{f(y)}.\n\\]",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5_bayes_parameter_estimation.html#bayes-theorem-in-statistics",
    "href": "lectures/part2/5_bayes_parameter_estimation.html#bayes-theorem-in-statistics",
    "title": "6  Bayesian Inference",
    "section": "6.3 Bayes’ Theorem in Statistics",
    "text": "6.3 Bayes’ Theorem in Statistics\nIn Bayesian inference, we apply Bayes’ theorem with:\n\n\\(x = \\theta\\) (the parameter, treated as random),\n\\(y = \\underline{x}\\) (the observed data).\n\nThis gives:\n\\[\n\\pi(\\theta \\mid \\underline{x})\n= \\frac{f(\\underline{x} \\mid \\theta)\\,\\pi(\\theta)}{f(\\underline{x})}.\n\\]\n\n\n\n\n\n\n\nDefinition 6.2 (Posterior Distribution.) \\[\n\\pi(\\theta \\mid \\underline{x})\n= \\frac{f(\\underline{x} \\mid \\theta)\\,\\pi(\\theta)}{f(\\underline{x})}.\n\\]\nThe posterior combines two ingredients:\n\nThe likelihood \\(f(\\underline{x} \\mid \\theta)\\) (information from the data).\nThe prior \\(\\pi(\\theta)\\) (information before seeing the data).\n\nThe denominator \\(f(\\underline{x})\\) is a normalising constant ensuring the posterior integrates to 1.\n\n\n\n\n\n\n\n\n\n\nImportant✨ Big picture:\n\n\n\nBayesian inference updates prior beliefs about parameters using the likelihood from the data, producing the posterior distribution which represents updated uncertainty.\n\n\nLet’s start with a simple example to see how this works:\n\n\n\n\n\n\n\nExample 6.2 Consider an experiment with a possibly biased coin. Let \\(\\theta=\\text{Pr}(\\textrm{Head})\\). Suppose that, before conducting the experiment, we believe that all values of \\(\\theta\\) are equally likely: this gives a prior distribution \\(\\theta\\sim U(0,1)\\), and so \\[\n\\pi(\\theta)=1,\\quad\\quad0&lt;\\theta&lt;1.\n\\] Note that with this prior distribution \\(\\mathrm{E}[\\theta]=0.5\\). We now toss the coin \\(5\\) times and observe \\(1\\) head. Determine the posterior distribution for \\(\\theta\\) given this data.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 6.1. Let \\(X\\) be the number of heads thrown out of \\(5\\). Assuming that each coin throw is independent and identically distributed, the number of heads out of \\(5\\) throws is Binomial2: \\[\n    X \\mid \\theta \\sim \\textrm{Bin}(5,\\theta).\n\\] Note that there is only a single observation here: one experiment is “tossing the coin \\(5\\) times and seeing how many come up heads”. The probability of observing \\(X=1\\) is \\[\n    f(x = 1 \\mid \\theta) = 5\\theta(1-\\theta)^4\n\\] If we plot this:\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ndef binom_lik(theta):\n    return 5 * theta * (1 - theta) ** 4\n\n\ntheta_grid = np.linspace(0, 1, 200)\nbinom_lik_vals = binom_lik(theta_grid)\n\nplt.figure()\nplt.plot(theta_grid, binom_lik_vals)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"Likelihood\")\nplt.title(\"Binomial Likelihood (with $n=5$ and $X=1$)\")\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n\n\n\n\nwe see that this favours \\(\\theta\\) around \\(0.2\\). In fact, in Example 5.4, we saw that the MLE is \\(\\hat{\\theta} = 0.2\\).\nOur prior for \\(\\theta\\) was \\(\\pi(\\theta) = 1\\), for \\(0 &lt; \\theta &lt; 1\\). To update our beliefs, by conditioning on the single observation \\(x=2\\), we use Bayes Theorem 6.2: \\[\n    \\pi(\\theta|x=1)&=\\frac{\\pi(\\theta)f(x=1|\\theta)}{f(x=1)} =\\frac{5\\theta(1-\\theta)^4}{1/6}\\\\\n    &=30\\,\\theta(1-\\theta)^4 =\\frac{\\theta(1-\\theta)^4}{\\mathrm{B}(2,5)},\\quad\\quad 0&lt;\\theta&lt;1.\n\\]",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5_bayes_parameter_estimation.html#why-is-this-useful",
    "href": "lectures/part2/5_bayes_parameter_estimation.html#why-is-this-useful",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "Example 6.1 Example. Suppose \\(\\theta\\) is the average exam score of students in a module. From past experience, the average score tends to be around 60, with most cohorts lying within about 10 marks of this.\nWe could encode this information as a Normal prior:\n\\[\n\\theta \\sim \\mathcal{N}(\\mu=60,\\;\\sigma^2=10^2).\n\\]\nThis prior expresses our belief before seeing the new data.\n\n\n\n\n\nPythonR\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ndef normal_pdf(theta, mean, sd):\n    const = 1 / (sd * (2 * math.pi) ** 0.5)\n    exp_term = np.exp(-((theta - mean) / sd) ** 2 / 2)\n    return const * exp_term\n\nmean = 100.0\nsd   = 10.0\n\ntheta_grid = np.linspace(mean - 3*sd, mean + 3*sd, 200)\npdf_values = normal_pdf(theta_grid, mean=mean, sd=sd)\n\nplt.figure()\nplt.plot(theta_grid, pdf_values)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"Density\")\nplt.title(\"Normal PDF: mean = 100, sd = 10\")\nplt.tight_layout()\nplt.show()  # &lt;- ensures only the figure is rendered\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\nmean &lt;- 100\nsd   &lt;- 10\n\ntheta_grid &lt;- seq(mean - 3*sd, mean + 3*sd, length.out = 200)\npdf_values &lt;- dnorm(theta_grid, mean = mean, sd = sd)\n\nplot(theta_grid, pdf_values, type = \"l\",\n     xlab = expression(theta),\n     ylab = \"Density\",\n     main = \"Normal PDF: mean = 100, sd = 10\")",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html",
    "href": "appendices/formula_sheet.html",
    "title": "7  Formula Sheet",
    "section": "",
    "text": "7.1 Discrete distributions",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html#discrete-distributions",
    "href": "appendices/formula_sheet.html#discrete-distributions",
    "title": "7  Formula Sheet",
    "section": "",
    "text": "7.1.1 Bernoulli\nModel. \\(X \\sim \\operatorname{Bernoulli}(p)\\) with \\(0&lt;p&lt;1\\).\nSupport. \\(x \\in \\{0,1\\}\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = p^{\\,x}(1-p)^{1-x}, \\quad x\\in\\{0,1\\}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=p,\\quad \\operatorname{Var}(X)=p(1-p).\\)\n\n\n7.1.2 Binomial\nModel. \\(X \\sim \\operatorname{Bin}(n,p)\\), \\(n\\in\\mathbb{N}\\), \\(0&lt;p&lt;1\\).\nSupport. \\(x=0,1,\\ldots,n\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=np,\\quad \\operatorname{Var}(X)=np(1-p).\\)\n\n\n7.1.3 Poisson\nModel. \\(X \\sim \\operatorname{Po}(\\lambda)\\), \\(\\lambda&gt;0\\).\nSupport. \\(x=0,1,2,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x)=\\frac{\\lambda^x e^{-\\lambda}}{x!}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\lambda,\\quad \\operatorname{Var}(X)=\\lambda.\\)",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html#geom",
    "href": "appendices/formula_sheet.html#geom",
    "title": "7  Formula Sheet",
    "section": "7.2 Geometric",
    "text": "7.2 Geometric\nModel. \\(X \\sim \\operatorname{Geom}(p)\\) (trials until first success, counting the success), \\(0&lt;p&lt;1\\).\nSupport. \\(x=1,2,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x)=p(1-p)^{x-1}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{1}{p},\\quad \\operatorname{Var}(X)=\\frac{1-p}{p^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nAlternative convention. Some texts start at \\(x=0\\) (failures before the first success). Adjust mean/variance accordingly.\n\n\n\n7.2.1 Negative Binomial\nModel. \\(X \\sim \\operatorname{NegBin}(r,p)\\): number of trials to achieve the \\(r\\)-th success (counts the successes), \\(r&gt;0\\) (often integer), \\(0&lt;p&lt;1\\).\nSupport. \\(x=r,r+1,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = \\binom{x-1}{r-1} p^{\\,r} (1-p)^{x-r}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{r}{p},\\quad \\operatorname{Var}(X)=\\frac{r(1-p)}{p^2}.\\)\n\n\n\n\n\n\nWarning\n\n\n\nParameterisation alert. Another common version counts failures before the \\(r\\)-th success (\\(x=0,1,\\dots\\)) with different mean/variance. Always check which one is used.",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html#continuous-distributions",
    "href": "appendices/formula_sheet.html#continuous-distributions",
    "title": "7  Formula Sheet",
    "section": "7.3 Continuous distributions",
    "text": "7.3 Continuous distributions\n\n7.3.1 Uniform (continuous)\nModel. \\(X \\sim \\operatorname{Unif}(a,b)\\), \\(a&lt;b\\).\nSupport. \\(a&lt;x&lt;b\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{1}{b-a}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a+b}{2},\\quad \\operatorname{Var}(X)=\\frac{(b-a)^2}{12}.\\)",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html#exp",
    "href": "appendices/formula_sheet.html#exp",
    "title": "7  Formula Sheet",
    "section": "7.4 Exponential",
    "text": "7.4 Exponential\nModel. \\(X \\sim \\operatorname{Exp}(\\lambda)\\) with rate \\(\\lambda&gt;0\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\lambda)=\\lambda e^{-\\lambda x}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{1}{\\lambda},\\quad \\operatorname{Var}(X)=\\frac{1}{\\lambda^2}.\\)\n\n7.4.1 Gamma\nModel. \\(X \\sim \\operatorname{Gamma}(a,b)\\) with shape \\(a&gt;0\\) and rate \\(b&gt;0\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{b^a}{\\Gamma(a)} x^{a-1} e^{-bx}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a}{b},\\quad \\operatorname{Var}(X)=\\frac{a}{b^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nMany sources use scale \\(\\beta=1/b\\): then \\(f(x)=\\frac{1}{\\Gamma(a)\\beta^a}x^{a-1}e^{-x/\\beta}\\) and \\(\\mathbb{E}[X]=a\\beta\\).\n\n\n\n\n7.4.2 Normal\nModel. \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\sigma&gt;0\\).\nSupport. \\(x\\in\\mathbb{R}\\).\nPDF. \\[\nf_X(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\mu,\\quad \\operatorname{Var}(X)=\\sigma^2.\\)\n\n\n7.4.3 Lognormal\nModel. \\(X \\sim \\operatorname{LogNormal}(\\mu,\\sigma^2)\\) meaning \\(\\ln X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\mu,\\sigma^2)=\\frac{1}{x\\sqrt{2\\pi\\sigma^2}}\n\\exp\\!\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right).\n\\] Mean/Var. \\(\\mathbb{E}[X]=e^{\\mu+\\sigma^2/2},\\quad \\operatorname{Var}(X)=\\big(e^{\\sigma^2}-1\\big)e^{2\\mu+\\sigma^2}.\\)\n\n\n7.4.4 Chi-squared\nModel. \\(X \\sim \\chi^2_\\nu\\) with \\(\\nu&gt;0\\) degrees of freedom.\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\nu)=\\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{\\nu/2-1}e^{-x/2}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\nu,\\quad \\operatorname{Var}(X)=2\\nu.\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(\\chi^2_\\nu\\) is a special case of Gamma with \\(a=\\nu/2\\), \\(b=1/2\\).\n\n\n\n\n7.4.5 Student-\\(t\\)\nModel. \\(X \\sim t_\\nu\\) with \\(\\nu&gt;0\\) degrees of freedom.\nSupport. \\(x\\in\\mathbb{R}\\).\nPDF. \\[\nf_X(x;\\nu)=\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\n\\left(1+\\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=0\\) for \\(\\nu&gt;1\\); undefined for \\(\\nu\\le 1\\).\n\\(\\operatorname{Var}(X)=\\frac{\\nu}{\\nu-2}\\) for \\(\\nu&gt;2\\); infinite for \\(1&lt;\\nu\\le 2\\); undefined for \\(\\nu\\le 1\\).\n\n\n7.4.6 \\(F\\) distribution\nModel. \\(X \\sim F_{d_1,d_2}\\) with \\(d_1,d_2&gt;0\\) degrees of freedom.\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;d_1,d_2)=\\frac{\\Gamma\\!\\left(\\frac{d_1+d_2}{2}\\right)}{\\Gamma\\!\\left(\\frac{d_1}{2}\\right)\\Gamma\\!\\left(\\frac{d_2}{2}\\right)}\n\\left(\\frac{d_1}{d_2}\\right)^{d_1/2}\\frac{x^{d_1/2-1}}{\\left(1+\\frac{d_1}{d_2}x\\right)^{(d_1+d_2)/2}}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{d_2}{d_2-2}\\) for \\(d_2&gt;2\\).\n\\(\\operatorname{Var}(X)=\\frac{2d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)}\\) for \\(d_2&gt;4\\).",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html#beta",
    "href": "appendices/formula_sheet.html#beta",
    "title": "7  Formula Sheet",
    "section": "7.5 Beta",
    "text": "7.5 Beta\nModel. \\(X \\sim \\operatorname{Beta}(a,b)\\), \\(a&gt;0, b&gt;0\\).\nSupport. \\(0&lt;x&lt;1\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\,x^{a-1}(1-x)^{b-1}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a}{a+b},\\quad \\operatorname{Var}(X)=\\frac{ab}{(a+b)^2(a+b+1)}.\\)\n\n7.5.1 Multivariate Normal\nModel. \\(\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu},\\Sigma)\\) with \\(\\Sigma\\) positive definite.\nSupport. \\(\\mathbf{x}\\in\\mathbb{R}^p\\).\nPDF. \\[\nf_{\\mathbf{X}}(\\mathbf{x};\\boldsymbol{\\mu},\\Sigma)=\\frac{1}{(2\\pi)^{p/2}(\\det\\Sigma)^{1/2}}\n\\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right).\n\\] Mean/Cov. \\(\\mathbb{E}[\\mathbf{X}]=\\boldsymbol{\\mu},\\quad \\operatorname{Var}(\\mathbf{X})=\\Sigma.\\)",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula_sheet.html#integral-reminder",
    "href": "appendices/formula_sheet.html#integral-reminder",
    "title": "7  Formula Sheet",
    "section": "7.6 Integral reminder",
    "text": "7.6 Integral reminder\nThe Gamma function: \\[\n\\Gamma(a)=\\int_0^\\infty x^{a-1}e^{-x}\\,dx, \\qquad a&gt;0.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nCommon pitfalls. - Rate vs scale for Exponential/Gamma: check whether the parameter is \\(\\lambda\\) (rate) or \\(\\beta\\) (scale). - Geometric/Negative Binomial supports vary by convention (counting failures vs trials). Always verify support and mean/variance before using formulas. - In likelihood work, write \\(f_X(x;\\theta)\\) (semicolon) to emphasise data vs parameters.",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2_freq_vs_bayes.html",
    "href": "lectures/part1/2_freq_vs_bayes.html",
    "title": "3  Types of Inference",
    "section": "",
    "text": "3.1 Frequentist Inference\nThere are two fundamentally different approaches to statistical inference:\nIn practice, the distinction between frequentist and Bayesian inference comes down to what is treated as random and what is not. The table below summarises this key philosophical difference:\nFrequentist inference",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2_freq_vs_bayes.html#bayesian-inference",
    "href": "lectures/part1/2_freq_vs_bayes.html#bayesian-inference",
    "title": "3  Types of Inference",
    "section": "3.2 Bayesian Inference",
    "text": "3.2 Bayesian Inference\nBayesian inference specifies a prior over \\(\\theta\\).",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1_stat_inference.html",
    "href": "lectures/part1/1_stat_inference.html",
    "title": "2  What is Statistical Inference?",
    "section": "",
    "text": "2.1 Statistical Inference as Inverse Probability Theory\nStatistical inference underpins experimental science, machine learning, clinical trials, and more. The overall aim is:\nTypical conclusions include estimating a proportion or mean, quantifying uncertainty, comparing groups, or predicting future outcomes.\nExamples: 1. Simple one-dimensional example 2. Slightly more convoluted two-dimensional example 3. Machine learning (images of cats)\nWe assume the population can be represented by a probability distribution \\(f(x\\mid \\theta)\\), where \\(\\theta\\) is an unknown parameter (or vector of parameters). Our data are usually modelled as an independent and identically distributed (iid) sample:\n\\[\nX_1,\\ldots,X_n \\stackrel{\\text{iid}}{\\sim} f(x\\mid \\theta).\n\\]\nInference is the process of learning about \\(\\theta\\) (and hence about the population distribution) from the observed sample. In practice, always ask whether iid sampling is plausible for your study design.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1_stat_inference.html#statistical-inference-as-inverse-probability-theory",
    "href": "lectures/part1/1_stat_inference.html#statistical-inference-as-inverse-probability-theory",
    "title": "2  What is Statistical Inference?",
    "section": "",
    "text": "Independent: knowing one \\(X_i\\) tells you nothing about another.\nIdentically distributed: each \\(X_i\\) has the same distribution \\(f\\).\n\n\n\n2.1.1 Statistical Inference by Eye\nAs an informal starting point, suppose \\(X_1,\\ldots,X_n\\) are sampled from a \\(\\mathcal{N}(\\mu,\\sigma^2)\\) distribution, with \\(\\mu\\) and \\(\\sigma^2\\) unknown. Try to “guess” \\(\\mu\\) and \\(\\sigma\\) by eye from the data:\n\nhtml`\n&lt;style&gt;\n.controls-grid {\n  display: grid;\n  grid-template-columns: 1fr 1fr 1fr;   /* 3 columns */\n  gap: 0.8rem;\n  align-items: end;\n}\n@media (max-width: 800px){\n  .controls-grid { grid-template-columns: 1fr; }\n}\n&lt;/style&gt;\n`\n\n\n\n\n\n\n\nviewof n = Inputs.range([50, 1000], { value: 200, step: 50, label: \"n\" })\n\ntrueChoices = new Map([\n  [\"N(0, 1²)\",    {mu: 0,  sigma2: 1}],\n  [\"N(0, 2²)\",    {mu: 0,  sigma2: 4}],\n  [\"N(1, 1²)\",    {mu: 1,  sigma2: 1}],\n  [\"N(-2, 0.5²)\", {mu: -2, sigma2: 0.25}],\n  [\"N(2, 1.5²)\",  {mu: 2,  sigma2: 2.25}]\n])\n\nviewof trueDist = Inputs.select(trueChoices, {\n  label: \"True Distribution\",\n  value: trueChoices.get(\"N(0, 1²)\")\n})\n\nviewof approxVar = Inputs.range([0.1, 9], { value: 1, step: 0.1, label: \"Approx. variance (σ²)\" })\nviewof approxMu  = Inputs.range([-5, 5], { value: 0, step: 0.1, label: \"Approx. mean (μ)\" })\n\nviewof resample = Inputs.button(\"Generate new sample\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const box = html`&lt;div class=\"controls-grid\"&gt;&lt;/div&gt;`;\n  box.append(\n    viewof n,\n    viewof trueDist,\n    viewof approxMu,\n    viewof approxVar,\n    viewof resample\n  );\n  return box;\n}\n\n\n\n\n\n\n\nmu = trueDist.mu\nsigma2 = trueDist.sigma2\n\n\n\n// data\ndata = {\n  resample;\n  const sigma = Math.sqrt(sigma2);\n  const rnd = d3.randomNormal(mu, sigma);\n  return Array.from({length: n}, () =&gt; rnd());\n}\n\napproxSigma = Math.sqrt(approxVar)\n\n// plot\n{\n  const xMin = d3.min(data), xMax = d3.max(data);\n  const pad = 0.1 * (xMax - xMin || 1);\n  const domain = [xMin - pad, xMax + pad];\n\n  const xs = d3.range(domain[0], domain[1], (domain[1]-domain[0]) / 200);\n  const normalPdf = (x, mu, sigma) =&gt; (1/(sigma*Math.sqrt(2*Math.PI))) * Math.exp(-0.5*((x-mu)/sigma)**2);\n  const curve = xs.map(x =&gt; ({x, y: normalPdf(x, approxMu, approxSigma)}));\n\n  const binCount = 30;\n  const binWidth = (domain[1] - domain[0]) / binCount;\n  const scaledCurve = curve.map(d =&gt; ({x: d.x, y: d.y * n * binWidth}));\n\n  return Plot.plot({\n    marginLeft: 48,\n    marginBottom: 40,\n    width: 700,\n    height: 420,\n    x: {label: \"x\", domain},\n    y: {label: \"Count\"},\n    marks: [\n      Plot.rectY(data, Plot.binX({y: \"count\"}, {thresholds: binCount})),\n      Plot.line(scaledCurve, {x: \"x\", y: \"y\", strokeWidth: 2})\n    ]\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(This intuition will be formalised later when we develop estimators and uncertainty quantification.)",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1_stat_inference.html#exercises",
    "href": "lectures/part1/1_stat_inference.html#exercises",
    "title": "2  What is Statistical Inference?",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\n\nFor a study recording daily counts of emails received by a helpdesk, propose a one-parameter model and state the parameter space.\nIn a satisfaction survey with responses \\(\\{\\text{poor},\\text{OK},\\text{good}\\}\\), suggest a simple probabilistic model and identify the parameter(s).\nState what “iid” means in one sentence, and give one example of how it could fail in practice.\nRevisit {ref}stat-by-eye: if the spread of points increases, which parameter of \\(\\mathcal{N}(\\mu,\\sigma^2)\\) has changed, and how would you describe that change informally?",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/0_prob_theory.html",
    "href": "lectures/part1/0_prob_theory.html",
    "title": "1  Probability Theory",
    "section": "",
    "text": "1.1 Probability and Events\nBefore we delve into statistical inference, we briefly review key ideas from probability theory.\nWhen working with probability we need three basic ingredients1\nviewof venn = {\n  const width = 240, height = 180;\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height);\n\n  // Circle A\n  svg.append(\"circle\")\n      .attr(\"cx\", 90).attr(\"cy\", 90).attr(\"r\", 60)\n      .attr(\"fill\", \"steelblue\").attr(\"fill-opacity\", 0.4);\n\n  // Circle B\n  svg.append(\"circle\")\n      .attr(\"cx\", 150).attr(\"cy\", 90).attr(\"r\", 60)\n      .attr(\"fill\", \"tomato\").attr(\"fill-opacity\", 0.4);\n\n  // Labels\n  svg.append(\"text\")\n      .attr(\"x\", 70).attr(\"y\", 85)\n      .text(\"A\")\n      .style(\"font-size\", \"14px\")\n      .style(\"font-weight\", \"bold\");\n\n  svg.append(\"text\")\n      .attr(\"x\", 165).attr(\"y\", 85)\n      .text(\"B\")\n      .style(\"font-size\", \"14px\")\n      .style(\"font-weight\", \"bold\");\n\n  return svg.node();\n}\n\n\n\n\n\n\n\n\nFigure 1.1: Venn diagram illustrating union and intersection.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/0_prob_theory.html#probability-of-events",
    "href": "lectures/part1/0_prob_theory.html#probability-of-events",
    "title": "1  Probability Theory",
    "section": "",
    "text": "Intersection and union. \\(A\\cap B\\) means both \\(A\\) and \\(B\\) occur. \\(A\\cup B\\) means \\(A\\) or \\(B\\) (or both) occur.\nAddition rule (inclusion–exclusion).\n\\[\n\\Pr(A\\cup B)=\\Pr(A)+\\Pr(B)-\\Pr(A\\cap B).\n\\]\nMutually exclusive events. \\(A\\) and \\(B\\) are exclusive if they cannot both occur; then \\(\\Pr(A\\cap B)=0\\).\nExhaustive events. \\(A\\) and \\(B\\) are exhaustive if at least one must occur; then \\(\\Pr(A\\cup B)=1\\). (The ideas generalise to more than two events.)\nIndependence. Events \\(A\\) and \\(B\\) are independent if\n\\[\n\\Pr(A\\cap B)=\\Pr(A)\\Pr(B).\n\\]\nConditional probability. If \\(\\Pr(B)&gt;0\\),\n\\[\n\\Pr(A\\mid B)=\\frac{\\Pr(A\\cap B)}{\\Pr(B)}.\n\\]\nBayes’ theorem. If \\(\\Pr(B)&gt;0\\),\n\\[\n\\Pr(A\\mid B)=\\frac{\\Pr(B\\mid A)\\Pr(A)}{\\Pr(B)}.\n\\]\nPartitions and the law of total probability. Events \\(B_1,\\dots,B_n\\) form a partition if exactly one occurs (mutually exclusive and exhaustive). Then, for any event \\(A\\),\n\\[\n\\Pr(A)=\\sum_{i=1}^n \\Pr(A\\mid B_i)\\Pr(B_i).\n\\]\n\n\n\n\n\n\n\n\nExample 1.1 (A quick inclusion–exclusion check) Roll a fair six-sided die. Let \\(A=\\{\\text{even}\\}\\) and \\(B=\\{\\text{number} \\ge 4\\}\\). Compute \\(\\Pr(A\\cup B)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.1. \\(\\Pr(A)=3/6=1/2\\) (even outcomes \\(2,4,6\\)).\n\\(\\Pr(B)=3/6=1/2\\) (outcomes \\(4,5,6\\)).\n\\(\\Pr(A\\cap B)=2/6=1/3\\) (outcomes \\(4,6\\)).\nBy inclusion–exclusion,\n\\[\n\\Pr(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{2}-\\tfrac{1}{3}=\\tfrac{2}{3}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 1.2 (Student Example) Seventy percent of students are attentive in lectures and thirty percent are not. If a student is attentive the probability of passing the course is \\(0.8\\). If a student is not attentive the probability of passing the course is \\(0.1\\).\n\nA student is selected at random. What is the probability that they pass the course?\nGiven that the student passed the course, what is the probability that they were attentive?\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.2. Let\n\n\\(P\\) denote the event of a student passing the course.\n\\(A\\) denote the event of a student being attentive.\n\\(A^c\\) denote the event of a student not being attentive.\n\n\nAttentive and inattentive form a partition. By the law of total probability,\n\n\\[\n\\Pr(P)=\\Pr(P\\mid A)\\Pr(A)+\\Pr(P\\mid A^c)\\Pr(A^c)=0.8\\times0.7+0.1\\times0.3=0.59.\n\\]\n\nUsing Bayes’ theorem,\n\n\\[\n\\Pr(A\\mid P)=\\frac{\\Pr(P\\mid A)\\Pr(A)}{\\Pr(P)}\n=\\frac{0.8\\times0.7}{0.59}=0.949\\ \\text{(3 s.f.)}.\n\\]",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/0_prob_theory.html#random-variables",
    "href": "lectures/part1/0_prob_theory.html#random-variables",
    "title": "1  Probability Theory",
    "section": "1.2 Random Variables",
    "text": "1.2 Random Variables\nWe use upper-case letters for random variables, e.g. \\(X, Y, X_1, X_2,\\dots\\).\n\nCumulative distribution function (cdf). For any random variable \\(X\\),\n\\[\nF(x)=\\Pr(X\\le x).\n\\]\nDiscrete case (pmf). If \\(X\\) is discrete, its probability mass function is\n\\[\np(x)=\\Pr(X=x),\\qquad \\sum_x p(x)=1.\n\\]\nContinuous case (pdf). If \\(X\\) is continuous, its probability density function satisfies\n\\[\nf(x)=\\frac{dF(x)}{dx},\\qquad \\Pr(a&lt;X\\le b)=\\int_a^b f(x)\\,dx,\\qquad \\int_{-\\infty}^{\\infty} f(x)\\,dx=1.\n\\]\n\n\n\n\n\n\n\n\nExample 1.3 (Discrete CDF.) Let \\(X\\) be the outcome of a fair six-sided die. Find \\(p(x)\\) and \\(F(3)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.3. \\(p(x)=\\Pr(X=x)=1/6\\) for \\(x\\in\\{1,2,3,4,5,6\\}\\), and \\(0\\) otherwise.\n\\(F(3)=\\Pr(X\\le 3)=\\Pr(\\{1,2,3\\})=3\\times(1/6)=1/2.\\)\n\n\n\n\n\n\n\n\n\n\n\nExample 1.4 Let \\(X\\sim \\mathrm{Uniform}(0,1)\\). Compute \\(\\Pr(0.2&lt;X&lt;0.5)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.4. Here \\(f(x)=1\\) for \\(0&lt;x&lt;1\\) and \\(0\\) otherwise.\n\\[\n\\Pr(0.2&lt;X&lt;0.5)=\\int_{0.2}^{0.5} 1\\,dx = 0.3.\n\\]\n\n\n\n\n\n1.2.1 Law of Total Probability and Conditional PDFs",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part2/3_freq_point_estimation.html",
    "href": "lectures/part2/3_freq_point_estimation.html",
    "title": "4  Frequentist Point Estimation",
    "section": "",
    "text": "4.1 Estimators vs. Estimates\nA statistic is any function of the sample\n\\[\nT \\;=\\; T(X_1,\\dots,X_n).\n\\]\nBecause it is built from the random variables \\(X_1,\\dots,X_n\\), the statistic \\(T\\) itself is a random quantity: its value varies from sample to sample. Once the data have been observed \\((x_1,\\dots,x_n)\\), plugging them into the same formula produces a single non-random number, which we denote by \\(t_{\\mathrm{obs}}\\) (or simply \\(t\\)).\nWhen we pick a statistic for the specific purpose of approximating a parameter \\(\\theta\\), we call it an estimator. Its realised value is the estimate.\nAn estimator is a random variable; an estimate is its realised value.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/3_freq_point_estimation.html#estimators-vs.-estimates",
    "href": "lectures/part2/3_freq_point_estimation.html#estimators-vs.-estimates",
    "title": "4  Frequentist Point Estimation",
    "section": "",
    "text": "Concept\nNotation\nDescription\n\n\n\n\nEstimator\n\\(\\hat{\\theta} = \\hat{\\theta}(X_1, \\dots, X_n)\\)\nA function of the data. A rule for producing an estimate.\n\n\nEstimate\n\\(\\hat{\\theta}_{\\text{obs}} = \\hat{\\theta}(x_1, \\dots, x_n)\\)\nThe numerical value you get after plugging in your data.\n\n\n\n\n\n4.1.1 Sample Mean and Variance\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 4.1 Sample Mean.\nThe sample mean of a random sample \\(X_1,\\ldots,X_n\\) is\n\\[\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\n\n\n\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 4.2 Sample Variance.\nFor a random sample \\(X_1,\\ldots,X_n\\), the sample variance is\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2.\n\\]\n\n\n\n\n\n4.1.2 The Randomness of Estimators\nThe randomness of an estimator \\(\\hat{\\theta}\\) is due to randomness of the sample being plugged into the function.\n\nmutable rngSeed = 1\nmutable meanHistory = []\nmutable resampleClicks = 0\nmutable resetClicks = 0\nmutable lastXbar = null\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  if (reset &gt; resetClicks) {\n    mutable meanHistory = [];\n    mutable resetClicks = reset;\n    mutable lastXbar = null;  // Also reset the stored xbar\n  }\n  return html``; // &lt;- prevents \"undefined\" from being shown\n}\n\n\n\n\n\n\n\n{\n  if (resample &gt; resampleClicks) {\n    // If we have a previous mean stored, add it to history\n    if (lastXbar !== null) {\n      mutable meanHistory = [...meanHistory, lastXbar];\n    }\n    // Store current mean for next time\n    mutable lastXbar = xbar;\n    // Then bump seed for new sample\n    mutable rngSeed = rngSeed + 1;\n    mutable resampleClicks = resample;\n  }\n  return html``; // &lt;- prevents \"undefined\" from being shown\n}\n\n\n\n\n\n\n\n\n\nfunction styledButton(label, cls, options = {}) {\n  const el = Inputs.button(label, options);              // &lt;form&gt;…&lt;button&gt;…&lt;/button&gt;&lt;/form&gt;\n  const btn = el.querySelector('button');\n  if (btn) btn.classList.add(cls);                       // add your custom class\n  return el;\n}\n\n// Define themed buttons\nviewof resample = styledButton(\"Resample\", \"btn-resample\", {\n  reduce: c =&gt; (c ?? 0) + 1\n});\n\nviewof reset = styledButton(\"Reset\", \"btn-reset\", {\n  reduce: c =&gt; (c ?? 0) + 1\n});\n\n\n// viewof resample = Inputs.button(\"Resample\", { reduce: c =&gt; (c ?? 0) + 1 }) \n// viewof reset = Inputs.button(\"Reset\", { reduce: c =&gt; (c ?? 0) + 1 })\n\n// --- Current sample depends on rngSeed + controls ---\nX = {\n  const rand = d3.randomNormal.source(d3.randomLcg(rngSeed))(controls.trueMu, controls.trueSigma);\n  return d3.range(controls.n).map(rand);\n}\nxbar = d3.mean(X)\n\n// ========= Helpers =========\nmakeDensityReducer = (total) =&gt; (values, extent) =&gt;\n  values.length / Math.max(1, total) / Math.max(1e-9, extent.x2 - extent.x1);\n\n// ========= Left panel: current sample (histogram + dots at y=0) =========\nleftDensity = makeDensityReducer(controls.n)\nxExtent = d3.extent(X)\nxPad = 3 * controls.trueSigma\nxDomain = [xExtent[0] - 0.1 * (xPad || 1), xExtent[1] + 0.1 * (xPad || 1)]\n\n// === Compute yMax from actual binned densities ===\nbins = d3.bin().domain(xDomain).thresholds(controls.bins)(X);\ndensities = bins.map(b =&gt;\n  (b.length / Math.max(1, controls.n)) / Math.max(1e-9, (b.x1 - b.x0))\n);\nyMax = d3.max(densities);\nstickHeight = 0.1 * yMax; // 10% of the current y-range used by the histogram\n\nleftPlot = Plot.plot({\n  width: Math.min(600, Math.max(320, width/2 - 20)),\n  height: 320,\n  style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n  },\n  x: { label: \"Data\", domain: xDomain },\n  y: { label: \"Density\" },\n  marks: [\n    // histogram\n    Plot.rectY(\n      X,\n      Plot.binX(\n        { y: leftDensity },\n        {\n          x: d =&gt; d,\n          thresholds: controls.bins,\n          inset: 0,\n          fillOpacity: 0.4,\n          fill: \"var(--brand-teal)\"\n        }\n      )\n    ),\n    // raw points\n    // Plot.dot(\n    //   X.map(x =&gt; ({ x, y: 0 })),\n    //   {\n    //     x: \"x\",\n    //     y: \"y\",\n    //     r: 2,\n    //     fillOpacity: 0.9,\n    //     fill: \"var(--brand-teal)\",\n    //     symbol: \"cross\"\n    //   }\n    // ),\n    Plot.ruleX(\n    X,\n    { y1: 0, y2: yMax * 0.05, stroke: \"var(--brand-teal)\", strokeWidth: 3, strokeOpacity: 1.}\n    ),\n    // xbar reference\n    Plot.ruleX([xbar], { stroke: \"currentColor\", strokeDasharray: \"4,3\" }),\n    Plot.dot([{ x: xbar, y: 0 }], { x: \"x\", y: \"y\", r: 3, fill: \"currentColor\" }),\n    Plot.ruleY([0])\n  ]\n})\n\n// ========= Right panel: saved sample means (strip + hist + optional true curve) =========\nmeans = meanHistory\nrightDensity = makeDensityReducer(means.length)\n\nmPad = 3 * controls.trueSigma / Math.sqrt(controls.n)\nmeansDomain = means.length ? d3.extent(means) : [controls.trueMu - mPad, controls.trueMu + mPad]\nmDomain = [\n  Math.min(meansDomain[0], controls.trueMu - mPad),\n  Math.max(meansDomain[1], controls.trueMu + mPad)\n]\n\nsampleMeanPdf = (x) =&gt;\n  (1 / Math.sqrt(2 * Math.PI * (controls.trueSigma**2 / controls.n))) *\n  Math.exp(-((x - controls.trueMu)**2) / (2 * (controls.trueSigma**2 / controls.n)))\n\npdfXs = d3.scaleLinear().domain(mDomain).ticks(200)\npdfData = pdfXs.map(x =&gt; ({ x, y: sampleMeanPdf(x) }))\n\nrightPlot = Plot.plot({\n  width: Math.min(600, Math.max(320, width/2 - 20)),\n  height: 320,\n  style: {                     // make the figure match your theme\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"   // this drives 'currentColor'\n  },\n  x: { label: \"x̄\", domain: mDomain },\n  y: { label: \"Density\" },\n  marks: [\n    Plot.rectY(\n      means,\n      Plot.binX(\n        { y: rightDensity },\n        { x: d =&gt; d, thresholds: controls.bins, inset: 0, fillOpacity: 0.3,\n          fill: \"var(--brand-red)\" }\n      )\n    ),\n    Plot.dot(means.map(m =&gt; ({ x: m, y: 0 })), {\n      x: \"x\", y: \"y\", r: 2, fillOpacity: 0.7, fill: \"var(--brand-red)\"\n    }),\n    Plot.dot([{ x: xbar, y: 0 }], {\n      x: \"x\", y: \"y\", r: 4, fill: \"currentColor\"     // uses style.color above\n    }),\n    ...(controls.showTrue\n      ? [Plot.line(pdfData, { x: \"x\", y: \"y\", strokeWidth: 2, stroke: \"var(--brand-red)\" })]\n      : []\n    ),\n    Plot.ruleY([0])\n  ]\n})\n\n// ========= Display =========\n\n\n// html`&lt;div style=\"\n//   display:flex;\n//   align-items:center;\n//   gap:10px;\n//   margin-bottom:6px;\n//   width:100%;\n// \"&gt;\n//   &lt;span style=\"display:inline-flex;\"&gt;${viewof resample}&lt;/span&gt;\n//   &lt;span style=\"flex:1 1 auto; min-width:100px;\"&gt;&lt;/span&gt;  &lt;!-- flexible spacer that can shrink --&gt;\n//   &lt;span style=\"display:inline-flex;\"&gt;${viewof reset}&lt;/span&gt;\n// &lt;/div&gt;`\n{\n  // Make the forms behave nicely in the grid\n  for (const f of [viewof resample, viewof reset]) {\n    f.style.margin = \"0\";\n    f.style.width = \"100%\";      // inline style beats the library rule\n    f.style.maxWidth = \"none\";\n    f.style.display = \"flex\";    // so we can push the button left/right\n  }\n  // Left button to the left, right button to the right\n  viewof resample.style.justifyContent = \"flex-start\";\n  viewof reset.style.justifyContent    = \"flex-end\";\n\n  return html`&lt;div class=\"ojs-toolbar-grid\"&gt;\n    &lt;div class=\"left\"&gt;${viewof resample}&lt;/div&gt;\n    &lt;div class=\"center\"&gt;\n      ${md`${tex`\\bar{x} = ${xbar.toFixed(2)}`}`}\n    &lt;/div&gt;\n    &lt;div class=\"right\"&gt;${viewof reset}&lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;style&gt;\n    .ojs-toolbar-grid{\n      display:grid;\n      grid-template-columns:1fr auto 1fr; /* left, middle, right */\n      align-items:center;\n      width:100%;\n      box-sizing:border-box;\n      margin:0; padding:0;\n    }\n    .ojs-toolbar-grid form{\n      margin:0 !important;\n      width:100% !important;\n      max-width:none !important;\n      display:flex !important;\n    }\n    .ojs-toolbar-grid .left  form{ justify-content:flex-start !important; }\n    .ojs-toolbar-grid .right form{ justify-content:flex-end   !important; }\n\n    .btn.btn-resample{ background: var(--brand-teal); color:#fff; border:none; }\n    .btn.btn-resample:hover{ background: var(--brand-teal-hover); }\n    .btn.btn-reset{ background: var(--brand-red); color:#fff; border:none; }\n    .btn.btn-reset:hover{ background: var(--brand-red-hover); }\n    .btn.btn-resample:focus-visible,\n    .btn.btn-reset:focus-visible{\n      outline: 2px solid currentColor;\n      outline-offset: 2px;\n    }\n\n    .ojs-toolbar-grid .center{\n      text-align:center;\n      font-size:0.9rem;\n      color: var(--brand-fg);\n    }\n  &lt;/style&gt;`;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"max-width:100%; display:flex; gap:14px; align-items:flex-start; flex-wrap:nowrap;\"&gt;\n  &lt;div&gt;${leftPlot}&lt;/div&gt;\n  &lt;div&gt;${rightPlot}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\nhtml`&lt;style&gt;\ndetails.controls-root &gt; summary {\n  list-style: none; cursor: pointer; user-select: none;\n  background: var(--surface);\n  border: 1px solid var(--border);\n  border-radius: 10px;\n  padding: 8px 10px;\n  font-weight: 600; font-size: 13px;\n  color: var(--fg-strong);\n  display: flex; align-items: center; gap: 8px;\n}\ndetails.controls-root &gt; summary::-webkit-details-marker { display: none; }\ndetails.controls-root[open] &gt; summary { border-bottom-left-radius: 0; border-bottom-right-radius: 0; }\n\n/* ---------- Compact Inputs (fixed slider alignment) ---------- */\n.controls-col label { font-size: 12px !important; }\n\n.controls-col input[type=\"range\"] {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 100%;\n  height: 18px;                     /* overall control box height */\n  background: transparent;\n  vertical-align: middle;\n  accent-color: var(--brand-teal, #55C3CB);\n}\n\n/* WebKit track */\n.controls-col input[type=\"range\"]::-webkit-slider-runnable-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* WebKit thumb */\n.controls-col input[type=\"range\"]::-webkit-slider-thumb {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n  /* Center the 12px thumb over the 4px track: (4 - 12)/2 = -4px */\n  margin-top: -4px;\n}\n\n/* Firefox track */\n.controls-col input[type=\"range\"]::-moz-range-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* Firefox thumb (auto-centred; no negative margin needed) */\n.controls-col input[type=\"range\"]::-moz-range-thumb {\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n}\n\n/* Optional: hide the default filled \"progress\" colour in Firefox */\n.controls-col input[type=\"range\"]::-moz-range-progress {\n  height: 4px; border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n.controls-grid {\n  display: grid;\n  grid-template-columns: minmax(260px,1fr) minmax(260px,1fr);\n  gap: 12px;\n  border: 1px solid var(--border); border-top: none;\n  border-bottom-left-radius: 10px; border-bottom-right-radius: 10px;\n  padding: 10px;\n  background: var(--surface-weak);\n  overflow-x: auto;\n}\n.controls-col { min-width: 260px; }\n.controls-col h4 {\n  margin: 0 0 6px 0; font-size: 12px; font-weight: 700;\n  color: var(--fg-muted);\n}\n\n/* chevron */\ndetails.controls-root &gt; summary::before {\n  content: \"\"; display: inline-block; margin-right: 8px; width: 0; height: 0;\n  border-style: solid; border-width: 3px 0 3px 6px;\n  border-color: transparent transparent transparent var(--fg-strong);\n  transform: rotate(0deg); transform-origin: 3px 50%; transition: transform 50ms ease;\n}\ndetails.controls-root[open] &gt; summary::before { transform: rotate(90deg); }\n&lt;/style&gt;`\n\n\n\n\n\n\n\nviewof controls = {\n\n  // Left column: data & truth\n  const leftForm = Inputs.form({\n    n: Inputs.range([1, 400], { value: 10, step: 1, label: md`${tex`n`}` }),\n    trueMu: Inputs.range([-2, 2], { value: 0, step: 0.1, label: md`${tex`\\mu_{\\text{true}}`}` }),\n    trueSigma: Inputs.range([0.3, 3], { value: 1, step: 0.1, label: md`${tex`\\sigma_{\\text{true}}`}` })\n  }, { submit: false });\n\n  // Right column: display & actions\n  const rightForm = Inputs.form({\n    bins: Inputs.range([10, 60], { value: 24, step: 1, label: \"Histogram bins\" }),\n    showTrue: Inputs.toggle({ label: md`Show ${tex`\\bar{X}`} distribution`, value: false }),\n    // include the buttons as inputs so their *click counts* become part of form.value\n  }, { submit: false });\n\n  // Collapsible wrapper\n  const root = html`&lt;details class=\"controls-root\" open&gt;\n    &lt;summary&gt;Simulation controls&lt;/summary&gt;\n    &lt;div class=\"controls-grid\"&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Data & truth&lt;/h4&gt;&lt;/div&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Display & actions&lt;/h4&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/details&gt;`;\n  root.querySelector(\".controls-grid\").children[0].append(leftForm);\n  root.querySelector(\".controls-grid\").children[1].append(rightForm);\n\n  // Merge both forms into a single value object\n  const getValue = () =&gt; ({ ...leftForm.value, ...rightForm.value });\n  const update = () =&gt; {\n    root.value = getValue();\n    root.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  };\n\n  leftForm.addEventListener(\"input\", update);\n  rightForm.addEventListener(\"input\", update);\n  // make sure button clicks trigger updates even if no 'input' is fired\n\n  queueMicrotask(update);\n  return root;\n}",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/3_freq_point_estimation.html#bias-variance-mse-and-consistency",
    "href": "lectures/part2/3_freq_point_estimation.html#bias-variance-mse-and-consistency",
    "title": "4  Frequentist Point Estimation",
    "section": "4.2 Bias, Variance, MSE and Consistency",
    "text": "4.2 Bias, Variance, MSE and Consistency\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 4.3 Bias.\nThe bias of an estimator \\(\\hat{\\theta}\\) for parameter \\(\\theta\\) is\n\\[\n\\mathrm{Bias}(\\hat{\\theta}) = \\mathrm{E}[\\hat{\\theta}] - \\theta.\n\\] If \\(\\mathrm{Bias}(\\hat{\\theta}) = 0\\), the estimator is unbiased.\n\n\n\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 4.4 Variance.\nThe variance of an estimator \\(\\hat{\\theta}\\) is simply\n\\[\n\\mathrm{Var}(\\hat{\\theta}) = \\mathrm{E}\\!\\big[(\\hat{\\theta}-\\mathrm{E}[\\hat{\\theta}])^2\\big].\n\\]\n\n\n\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 4.5 Mean Squared Error (MSE).\nThe MSE of \\(\\hat{\\theta}\\) is defined as\n\\[\n\\mathrm{MSE}(\\hat{\\theta})\n= \\mathrm{E}\\!\\big[(\\hat{\\theta}-\\theta)^2\\big]\n= \\mathrm{Var}(\\hat{\\theta}) + \\mathrm{Bias}(\\hat{\\theta})^2.\n\\]\n\n\n\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 4.6 Consistency.\nAn estimator \\(\\hat{\\theta}_n\\) (depending on sample size \\(n\\)) is consistent if\n\\[\n\\hat{\\theta}_n \\;\\xrightarrow{p}\\; \\theta\n\\quad \\text{as } n \\to \\infty,\n\\]\ni.e. it converges in probability to the true parameter value.\n\n\n\n\n4.2.1 Examples\n\n\n\n\n\n\n\nExample 4.1 (Bias of the Sample Mean) For \\(X_1,\\dots,X_n \\overset{iid}{\\sim}\\) distribution with mean \\(\\mu\\):\n\\[\n\\mathrm{E}[\\bar{X}] = \\mu\n\\quad\\implies\\quad\n\\mathrm{Bias}(\\bar{X}) = 0.\n\\]\nSo the sample mean is unbiased for \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\nExample 4.2 (Variance of the Sample Mean) If \\(\\mathrm{Var}(X_i) = \\sigma^2\\):\n\\[\n\\mathrm{Var}(\\bar{X})\n= \\frac{1}{n^2} \\sum_{i=1}^n \\mathrm{Var}(X_i)\n= \\frac{\\sigma^2}{n}.\n\\]\nSo as \\(n\\) grows, the estimator becomes more concentrated around \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\nExample 4.3 (Bias of the Sample Variance) If we instead used the “naïve” variance\n\\[\ns^2_{naive} = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2,\n\\]\nthen\n\\[\n\\mathrm{E}[s^2_{naive}] = \\frac{n-1}{n}\\sigma^2,\n\\]\nwhich underestimates \\(\\sigma^2\\).\nThat is why we divide by \\(n-1\\) instead of \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\nExample 4.4 (MSE of Sample Mean) Suppose we estimate \\(\\mu\\) with \\(\\hat{\\mu} = c\\bar{X}\\), where \\(c\\) is a constant.\n\nBias:\n\\[\n\\mathrm{Bias}(\\hat{\\mu}) = (c-1)\\mu.\n\\]\nVariance:\n\\[\n\\mathrm{Var}(\\hat{\\mu}) = c^2 \\frac{\\sigma^2}{n}.\n\\]\nMSE:\n\\[\n\\mathrm{MSE}(\\hat{\\mu})\n= \\frac{c^2\\sigma^2}{n} + (c-1)^2 \\mu^2.\n\\]\n\nThis shows how a biased estimator can still have low MSE if variance is reduced.\n\n\n\n\n\n\n\n\n\n\n\nExample 4.5 (Consistency of \\(\\bar{X}\\)) By the Law of Large Numbers,\n\\[\n\\bar{X} \\;\\xrightarrow{p}\\; \\mu,\n\\]\nso the sample mean is a consistent estimator of \\(\\mu\\).",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/3_freq_point_estimation.html#central-limit-theorem-clt",
    "href": "lectures/part2/3_freq_point_estimation.html#central-limit-theorem-clt",
    "title": "4  Frequentist Point Estimation",
    "section": "4.3 Central Limit Theorem (CLT)",
    "text": "4.3 Central Limit Theorem (CLT)\n\n\n\n\n\n\nNoteDefinition (Central Limit Theorem)\n\n\n\n\nDefinition 4.7 (Central Limit Theorem) If \\(X_1,\\dots,X_n \\overset{iid}{\\sim}\\) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), then\n\\[\n\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\;\\xrightarrow{d}\\; N(0,1).\n\\]\n\n\n\nThe CLT explains why many estimators (like \\(\\bar{X}\\)) are approximately normal in large samples, which is crucial for constructing confidence intervals and hypothesis tests.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1_stat_inference.html#footnotes",
    "href": "lectures/part1/1_stat_inference.html#footnotes",
    "title": "2  What is Statistical Inference?",
    "section": "",
    "text": "Set notation. There are several equivalent ways to write sets:\n\nSet-builder notation: \\(\\{\\lambda \\mid \\lambda &gt; 0\\}\\)\n\nInterval notation: \\((0,\\infty)\\)\n\nNamed set: \\(\\mathbb{R}^+\\)\n\nAll three describe the same set. In this course, we recommend using set-builder notation when writing statistical models and their parameter spaces.↩︎\nSet notation (multiple parameters). When a model has several parameters, it is best to use set-builder notation and collect the parameters into a vector \\[\n    \\underline{\\theta} = (\\theta_1, \\theta_2, \\ldots, \\theta_p).\n  \\] The parameter space can then be written as \\[\n    \\Theta = \\{(\\theta_1,\\ldots,\\theta_p) \\mid \\theta_1 \\in S_1,\\ \\ldots,\\ \\theta_p \\in S_p\\}.\n  \\]\nHere each \\(S_j\\) is the set of allowed values for parameter \\(\\theta_j\\). To construct \\(\\Theta\\), simply read off the constraints on each parameter from the definition of the distribution (see ?sec-formula-sheet).\nFor example, in the Normal model:\n\n\\(\\mu \\in \\mathbb{R}\\) (any real number),\n\n\\(\\sigma^2 \\in (0,\\infty)\\) (variance must be positive),\n\nso the parameter space is\n\\[\n    \\Theta = \\mathbb{R} \\times (0,\\infty).\n  \\]\nThis is called a Cartesian product: we form the parameter space by taking all possible pairs \\((\\mu, \\sigma^2)\\) where the first component is from \\(\\mathbb{R}\\) and the second from \\((0,\\infty)\\).\nIntuitively, it’s just “all combinations” of the allowed values for each parameter.↩︎",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1_stat_inference.html#statistical-models",
    "href": "lectures/part1/1_stat_inference.html#statistical-models",
    "title": "2  What is Statistical Inference?",
    "section": "2.2 Statistical Models",
    "text": "2.2 Statistical Models\nWhenever we perform statistical inference, we must specify a statistical model.\n\n\n\n\n\n\n\nDefinition 2.1 (Statistical Model) A statistical model is a set of candidate population distributions: \\[\n\\mathcal{P}=\\{P_\\theta \\mid \\theta\\in\\Theta\\},\n\\] where \\(P_\\theta\\) is a probability distribution indexed by a parameter \\(\\theta\\), and \\(\\Theta\\) is the parameter space (the set of allowable parameter values).\n\n\n\n\nTerminology checkpoint. A parameter \\(\\theta\\) belongs to the model/population; a statistic (e.g. a sample mean) is computed from the data to learn about \\(\\theta\\). Remember: models are working assumptions to be checked.\n\n\n\n\n\n\n\nExample 2.1 (Exponential model) Suppose we are interested in modelling waiting times until some event occurs\n(e.g. the time between customer arrivals at a service desk).\nA simple model is to assume the data are i.i.d. from an exponential distribution: \\[\nX_i \\mid \\lambda \\sim \\mathrm{Exp}(\\lambda).\n\\] Write this formally as a statistical model and write down the parameter space.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.1. Each data follows the exponential distribution with parameter \\(\\lambda\\). Since \\(\\lambda &gt; 0\\), we have the model: \\[\n\\mathcal{P}=\\big\\{\\mathrm{Exp}(\\lambda)\\ \\big|\\ \\lambda&gt;0\\big\\},\n\\] with parameter \\(\\theta=\\lambda\\) and parameter space1 \\[\n\\Theta = \\{\\lambda \\mid \\lambda &gt; 0\\} = (0,\\infty) = \\mathbb{R}^+.\n\\]\nThe exponential distribution is often used for modelling the time between events in a Poisson process — situations where events occur continuously and independently at a constant average rate.\nFor example:\n- the time between buses arriving at a stop if buses come at random,\n- the time until the next radioactive decay of an atom,\n- the waiting time until the next customer enters a shop.\nIn each case, the exponential model is a natural first assumption.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.2 (Normal model) Write down the statistical model and the corresponding parameter space used in Section 2.1.1.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.2. We assumed the data are i.i.d. Normally distributed with unknown mean and unknown variance: \\[\nX_i \\mid \\mu, \\sigma^2 \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2).\n\\]\nThe statistical model is \\[\n\\mathcal{P} = \\{ \\mathcal{N}(\\mu,\\sigma^2) \\mid \\mu \\in \\mathbb{R},\\ \\sigma^2 &gt; 0 \\}.\n\\]\nHere there are two parameters, \\(\\mu\\) and \\(\\sigma^2\\), which we group into a parameter vector \\(\\underline{\\theta} = (\\mu, \\sigma^2)\\).\nThe parameter space is2\n\\[\n\\Theta = \\{ (\\mu, \\sigma^2) \\mid \\mu \\in \\mathbb{R},\\ \\sigma^2 &gt; 0 \\}\n       = \\mathbb{R} \\times (0,\\infty)\n       = \\mathbb{R} \\times \\mathbb{R}^+.\n\\]\nThe normal model is widely used in practice, for example when data are approximately symmetric and unimodal, such as measurement errors in physics or biological traits like human heights.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.3 (From words to a model (one-dimensional)) A questionnaire records whether each respondent would recommend a service: “yes” or “no”. Write down a simple model and identify the parameter.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.3. Let \\(X_i=1\\) if respondent \\(i\\) says “yes”, and \\(0\\) otherwise.\nModel \\(X_1,\\ldots,X_n \\stackrel{\\text{iid}}{\\sim}\\mathrm{Bernoulli}(p)\\), where \\(p=\\Pr(X_i=1)\\) is the (unknown) long-run recommendation proportion.\nHere \\(\\theta=p\\) and \\(\\Theta=(0,1)\\). The population is all (current/future) potential respondents of interest; the sample is the \\(n\\) surveyed respondents.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.4 (A slightly richer example (multinomial)) A bag contains sweets of three colours \\(\\{\\text{red},\\text{blue},\\text{green}\\}\\). You draw \\(n\\) sweets at random with replacement and record counts \\((R,B,G)\\). Propose a model and name the parameter(s).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.4. Model \\((R,B,G) \\sim \\mathrm{Multinomial}\\!\\left(n;\\,p_R,p_B,p_G\\right)\\) with \\(p_R,p_B,p_G\\ge 0\\) and \\(p_R+p_B+p_G=1\\).\nThe parameter is \\(\\theta=(p_R,p_B,p_G)\\), and the parameter space is the 2-simplex \\(\\Theta=\\{(p_R,p_B,p_G)\\in[0,1]^3:\\sum p_\\cdot=1\\}\\). The population is the long-run distribution of colours produced by the bag; the sample is the observed draws.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.5 (Image Classification from Machine Learning) In image classification (cat vs not-cat), each training example is a pair \\((X_i,Y_i)\\) where \\(X_i\\) is an image and \\(Y_i\\in\\{0,1\\}\\). A generic probabilistic model writes the conditional distribution \\[\n\\mathrm{Pr}_\\theta(Y=1\\mid X=x)=g_\\theta(x),\n\\] for some function family \\(g_\\theta\\) (e.g. logistic regression or a neural network). What are the sample, population, model, and parameter?\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.5. \n\nSample: the labelled training set \\(\\{(X_i,Y_i)\\}_{i=1}^n\\).\n\nPopulation: all (future) images and labels of interest.\n\nModel: the family of conditional distributions \\(\\{ \\Pr_\\theta(Y=1\\mid X=x)=g_\\theta(x)\\}\\).\n\nParameter: \\(\\theta\\) (e.g. regression co\n\n\n\n\n\n\n\n\n\n\n\nWarningNote on Statistical Models\n\n\n\nIn this course we focus on how to perform statistical inference once a statistical model has been specified.\nWe will assume the model is given, and our task is to carry out inference by finding good parameter values using techniques such as:\n\nParameter estimation\n\nBayesian inference\n\nInterval estimation (e.g. confidence intervals, HDIs)\n\nHypothesis testing",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html",
    "href": "appendices/formula-sheet.html",
    "title": "Formula Sheet",
    "section": "",
    "text": "Discrete distributions",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#discrete-distributions",
    "href": "appendices/formula-sheet.html#discrete-distributions",
    "title": "Formula Sheet",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nBernoulli\nModel. \\(X \\sim \\operatorname{Bernoulli}(p)\\) with \\(0&lt;p&lt;1\\).\nSupport. \\(x \\in \\{0,1\\}\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = p^{\\,x}(1-p)^{1-x}, \\quad x\\in\\{0,1\\}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=p,\\quad \\operatorname{Var}(X)=p(1-p).\\)\n\n\nBinomial\nModel. \\(X \\sim \\operatorname{Bin}(n,p)\\), \\(n\\in\\mathbb{N}\\), \\(0&lt;p&lt;1\\).\nSupport. \\(x=0,1,\\ldots,n\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = \\binom{n}{x} p^x (1-p)^{n-x}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=np,\\quad \\operatorname{Var}(X)=np(1-p).\\)\n\n\nPoisson\nModel. \\(X \\sim \\operatorname{Po}(\\lambda)\\), \\(\\lambda&gt;0\\).\nSupport. \\(x=0,1,2,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x)=\\frac{\\lambda^x e^{-\\lambda}}{x!}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\lambda,\\quad \\operatorname{Var}(X)=\\lambda.\\)",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#geom",
    "href": "appendices/formula-sheet.html#geom",
    "title": "7  Formula Sheet",
    "section": "7.2 Geometric",
    "text": "7.2 Geometric\nModel. \\(X \\sim \\operatorname{Geom}(p)\\) (trials until first success, counting the success), \\(0&lt;p&lt;1\\).\nSupport. \\(x=1,2,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x)=p(1-p)^{x-1}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{1}{p},\\quad \\operatorname{Var}(X)=\\frac{1-p}{p^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nAlternative convention. Some texts start at \\(x=0\\) (failures before the first success). Adjust mean/variance accordingly.\n\n\n\n7.2.1 Negative Binomial\nModel. \\(X \\sim \\operatorname{NegBin}(r,p)\\): number of trials to achieve the \\(r\\)-th success (counts the successes), \\(r&gt;0\\) (often integer), \\(0&lt;p&lt;1\\).\nSupport. \\(x=r,r+1,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = \\binom{x-1}{r-1} p^{\\,r} (1-p)^{x-r}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{r}{p},\\quad \\operatorname{Var}(X)=\\frac{r(1-p)}{p^2}.\\)\n\n\n\n\n\n\nWarning\n\n\n\nParameterisation alert. Another common version counts failures before the \\(r\\)-th success (\\(x=0,1,\\dots\\)) with different mean/variance. Always check which one is used.",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#continuous-distributions",
    "href": "appendices/formula-sheet.html#continuous-distributions",
    "title": "Formula Sheet",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nUniform (continuous)\nModel. \\(X \\sim \\operatorname{Unif}(a,b)\\), \\(a&lt;b\\).\nSupport. \\(a&lt;x&lt;b\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{1}{b-a}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a+b}{2},\\quad \\operatorname{Var}(X)=\\frac{(b-a)^2}{12}.\\)",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#exp",
    "href": "appendices/formula-sheet.html#exp",
    "title": "7  Formula Sheet",
    "section": "7.4 Exponential",
    "text": "7.4 Exponential\nModel. \\(X \\sim \\operatorname{Exp}(\\lambda)\\) with rate \\(\\lambda&gt;0\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\lambda)=\\lambda e^{-\\lambda x}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{1}{\\lambda},\\quad \\operatorname{Var}(X)=\\frac{1}{\\lambda^2}.\\)\n\n7.4.1 Gamma\nModel. \\(X \\sim \\operatorname{Gamma}(a,b)\\) with shape \\(a&gt;0\\) and rate \\(b&gt;0\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{b^a}{\\Gamma(a)} x^{a-1} e^{-bx}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a}{b},\\quad \\operatorname{Var}(X)=\\frac{a}{b^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nMany sources use scale \\(\\beta=1/b\\): then \\(f(x)=\\frac{1}{\\Gamma(a)\\beta^a}x^{a-1}e^{-x/\\beta}\\) and \\(\\mathbb{E}[X]=a\\beta\\).\n\n\n\n\n7.4.2 Normal\nModel. \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\sigma&gt;0\\).\nSupport. \\(x\\in\\mathbb{R}\\).\nPDF. \\[\nf_X(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\mu,\\quad \\operatorname{Var}(X)=\\sigma^2.\\)\n\n\n7.4.3 Lognormal\nModel. \\(X \\sim \\operatorname{LogNormal}(\\mu,\\sigma^2)\\) meaning \\(\\ln X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\mu,\\sigma^2)=\\frac{1}{x\\sqrt{2\\pi\\sigma^2}}\n\\exp\\!\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right).\n\\] Mean/Var. \\(\\mathbb{E}[X]=e^{\\mu+\\sigma^2/2},\\quad \\operatorname{Var}(X)=\\big(e^{\\sigma^2}-1\\big)e^{2\\mu+\\sigma^2}.\\)\n\n\n7.4.4 Chi-squared\nModel. \\(X \\sim \\chi^2_\\nu\\) with \\(\\nu&gt;0\\) degrees of freedom.\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\nu)=\\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{\\nu/2-1}e^{-x/2}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\nu,\\quad \\operatorname{Var}(X)=2\\nu.\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(\\chi^2_\\nu\\) is a special case of Gamma with \\(a=\\nu/2\\), \\(b=1/2\\).\n\n\n\n\n7.4.5 Student-\\(t\\)\nModel. \\(X \\sim t_\\nu\\) with \\(\\nu&gt;0\\) degrees of freedom.\nSupport. \\(x\\in\\mathbb{R}\\).\nPDF. \\[\nf_X(x;\\nu)=\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\n\\left(1+\\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=0\\) for \\(\\nu&gt;1\\); undefined for \\(\\nu\\le 1\\).\n\\(\\operatorname{Var}(X)=\\frac{\\nu}{\\nu-2}\\) for \\(\\nu&gt;2\\); infinite for \\(1&lt;\\nu\\le 2\\); undefined for \\(\\nu\\le 1\\).\n\n\n7.4.6 \\(F\\) distribution\nModel. \\(X \\sim F_{d_1,d_2}\\) with \\(d_1,d_2&gt;0\\) degrees of freedom.\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;d_1,d_2)=\\frac{\\Gamma\\!\\left(\\frac{d_1+d_2}{2}\\right)}{\\Gamma\\!\\left(\\frac{d_1}{2}\\right)\\Gamma\\!\\left(\\frac{d_2}{2}\\right)}\n\\left(\\frac{d_1}{d_2}\\right)^{d_1/2}\\frac{x^{d_1/2-1}}{\\left(1+\\frac{d_1}{d_2}x\\right)^{(d_1+d_2)/2}}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{d_2}{d_2-2}\\) for \\(d_2&gt;2\\).\n\\(\\operatorname{Var}(X)=\\frac{2d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)}\\) for \\(d_2&gt;4\\).",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#beta",
    "href": "appendices/formula-sheet.html#beta",
    "title": "7  Formula Sheet",
    "section": "7.5 Beta",
    "text": "7.5 Beta\nModel. \\(X \\sim \\operatorname{Beta}(a,b)\\), \\(a&gt;0, b&gt;0\\).\nSupport. \\(0&lt;x&lt;1\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\,x^{a-1}(1-x)^{b-1}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a}{a+b},\\quad \\operatorname{Var}(X)=\\frac{ab}{(a+b)^2(a+b+1)}.\\)\n\n7.5.1 Multivariate Normal\nModel. \\(\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu},\\Sigma)\\) with \\(\\Sigma\\) positive definite.\nSupport. \\(\\mathbf{x}\\in\\mathbb{R}^p\\).\nPDF. \\[\nf_{\\mathbf{X}}(\\mathbf{x};\\boldsymbol{\\mu},\\Sigma)=\\frac{1}{(2\\pi)^{p/2}(\\det\\Sigma)^{1/2}}\n\\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right).\n\\] Mean/Cov. \\(\\mathbb{E}[\\mathbf{X}]=\\boldsymbol{\\mu},\\quad \\operatorname{Var}(\\mathbf{X})=\\Sigma.\\)",
    "crumbs": [
      "Additional Material",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Formula Sheet</span>"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#integral-reminder",
    "href": "appendices/formula-sheet.html#integral-reminder",
    "title": "Formula Sheet",
    "section": "Integral reminder",
    "text": "Integral reminder\nThe Gamma function: \\[\n\\Gamma(a)=\\int_0^\\infty x^{a-1}e^{-x}\\,dx, \\qquad a&gt;0.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nCommon pitfalls. - Rate vs scale for Exponential/Gamma: check whether the parameter is \\(\\lambda\\) (rate) or \\(\\beta\\) (scale). - Geometric/Negative Binomial supports vary by convention (counting failures vs trials). Always verify support and mean/variance before using formulas. - In likelihood work, write \\(f_X(x;\\theta)\\) (semicolon) to emphasise data vs parameters.",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "lectures/part2/5_bayes_parameter_estimation.html#introduction-to-the-bayesian-approach",
    "href": "lectures/part2/5_bayes_parameter_estimation.html#introduction-to-the-bayesian-approach",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "In the frequentist view, the parameter \\(\\theta\\) is fixed but unknown.\nIn the Bayesian view, the parameter \\(\\theta\\) is treated as a random variable.\n\n\n\n6.1.1 Why is this useful?\nA key advantage of Bayesian inference is that it allows us to incorporate prior information1 about the parameter.\n\n\n\n\n\n\n\nDefinition 6.1 (Prior Distribution.) Our uncertainty about \\(\\theta\\) is represented by a probability distribution, called the prior distribution:\n\\[\n\\theta \\sim \\pi(\\theta).\n\\]\nHere \\(\\pi(\\theta)\\) denotes the prior density (PDF or PMF).\n\n\n\n\n\n\n\n\n\n\n\nExample 6.1 Suppose \\(\\theta\\) is the average exam score of students in a module. From past experience, the average score tends to be around 60, with most cohorts lying within about 10 marks of this.\nWe could encode this information as a Normal prior:\n\\[\n\\theta \\sim \\mathcal{N}(\\mu=60,\\;\\sigma^2=10^2).\n\\]\nThis prior expresses our belief before seeing the new data.\n\n\n\n\n\nPythonR\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ndef normal_pdf(theta, mean, sd):\n    const = 1 / (sd * (2 * math.pi) ** 0.5)\n    exp_term = np.exp(-((theta - mean) / sd) ** 2 / 2)\n    return const * exp_term\n\nmean = 100.0\nsd   = 10.0\n\ntheta_grid = np.linspace(mean - 3*sd, mean + 3*sd, 200)\npdf_values = normal_pdf(theta_grid, mean=mean, sd=sd)\n\nplt.figure()\nplt.plot(theta_grid, pdf_values)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"Density\")\nplt.title(\"Normal PDF: mean = 100, sd = 10\")\nplt.tight_layout()\nplt.show()  # &lt;- ensures only the figure is rendered\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\nmean &lt;- 100\nsd   &lt;- 10\n\ntheta_grid &lt;- seq(mean - 3*sd, mean + 3*sd, length.out = 200)\npdf_values &lt;- dnorm(theta_grid, mean = mean, sd = sd)\n\nplot(theta_grid, pdf_values, type = \"l\",\n     xlab = expression(theta),\n     ylab = \"Density\",\n     main = \"Normal PDF: mean = 100, sd = 10\")",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5_bayes_parameter_estimation.html#footnotes",
    "href": "lectures/part2/5_bayes_parameter_estimation.html#footnotes",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "Prior distribution. In Bayesian inference, the prior represents our beliefs about the parameter \\(\\theta\\) before seeing any data. It is written as a probability distribution \\(\\pi(\\theta)\\), which can be based on previous studies, expert knowledge, or simply chosen to reflect uncertainty (e.g. a flat prior).↩︎\nBinomial distribution. If we assume each coin throw \\(C_i\\) is an i.i.d. \\(\\textrm{Bernoulli}(\\theta)\\) trial with a \\(1\\) representing a heads landing, and a \\(0\\) representing a tails, the total number of heads out of \\(5\\) is \\(X = C_1 + C_2 + C_3 + C_4 + C_5\\). Therefore: \\[\n    X \\mid \\theta \\sim \\textrm{Bin}(5,\\theta).\n\\]↩︎",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#sec-geom",
    "href": "appendices/formula-sheet.html#sec-geom",
    "title": "Formula Sheet",
    "section": "Geometric",
    "text": "Geometric\nModel. \\(X \\sim \\operatorname{Geom}(p)\\) (trials until first success, counting the success), \\(0&lt;p&lt;1\\).\nSupport. \\(x=1,2,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x)=p(1-p)^{x-1}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{1}{p},\\quad \\operatorname{Var}(X)=\\frac{1-p}{p^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nAlternative convention. Some texts start at \\(x=0\\) (failures before the first success). Adjust mean/variance accordingly.\n\n\n\nNegative Binomial\nModel. \\(X \\sim \\operatorname{NegBin}(r,p)\\): number of trials to achieve the \\(r\\)-th success (counts the successes), \\(r&gt;0\\) (often integer), \\(0&lt;p&lt;1\\).\nSupport. \\(x=r,r+1,\\ldots\\).\nPMF. \\[\n\\mathrm{Pr}(X=x) = \\binom{x-1}{r-1} p^{\\,r} (1-p)^{x-r}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{r}{p},\\quad \\operatorname{Var}(X)=\\frac{r(1-p)}{p^2}.\\)\n\n\n\n\n\n\nWarning\n\n\n\nParameterisation alert. Another common version counts failures before the \\(r\\)-th success (\\(x=0,1,\\dots\\)) with different mean/variance. Always check which one is used.",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#sec-exp",
    "href": "appendices/formula-sheet.html#sec-exp",
    "title": "Formula Sheet",
    "section": "Exponential",
    "text": "Exponential\nModel. \\(X \\sim \\operatorname{Exp}(\\lambda)\\) with rate \\(\\lambda&gt;0\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\lambda)=\\lambda e^{-\\lambda x}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{1}{\\lambda},\\quad \\operatorname{Var}(X)=\\frac{1}{\\lambda^2}.\\)\n\nGamma\nModel. \\(X \\sim \\operatorname{Gamma}(a,b)\\) with shape \\(a&gt;0\\) and rate \\(b&gt;0\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{b^a}{\\Gamma(a)} x^{a-1} e^{-bx}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a}{b},\\quad \\operatorname{Var}(X)=\\frac{a}{b^2}.\\)\n\n\n\n\n\n\nTip\n\n\n\nMany sources use scale \\(\\beta=1/b\\): then \\(f(x)=\\frac{1}{\\Gamma(a)\\beta^a}x^{a-1}e^{-x/\\beta}\\) and \\(\\mathbb{E}[X]=a\\beta\\).\n\n\n\n\nNormal\nModel. \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\sigma&gt;0\\).\nSupport. \\(x\\in\\mathbb{R}\\).\nPDF. \\[\nf_X(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\mu,\\quad \\operatorname{Var}(X)=\\sigma^2.\\)\n\n\nLognormal\nModel. \\(X \\sim \\operatorname{LogNormal}(\\mu,\\sigma^2)\\) meaning \\(\\ln X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\).\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\mu,\\sigma^2)=\\frac{1}{x\\sqrt{2\\pi\\sigma^2}}\n\\exp\\!\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right).\n\\] Mean/Var. \\(\\mathbb{E}[X]=e^{\\mu+\\sigma^2/2},\\quad \\operatorname{Var}(X)=\\big(e^{\\sigma^2}-1\\big)e^{2\\mu+\\sigma^2}.\\)\n\n\nChi-squared\nModel. \\(X \\sim \\chi^2_\\nu\\) with \\(\\nu&gt;0\\) degrees of freedom.\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;\\nu)=\\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{\\nu/2-1}e^{-x/2}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\nu,\\quad \\operatorname{Var}(X)=2\\nu.\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(\\chi^2_\\nu\\) is a special case of Gamma with \\(a=\\nu/2\\), \\(b=1/2\\).\n\n\n\n\nStudent-\\(t\\)\nModel. \\(X \\sim t_\\nu\\) with \\(\\nu&gt;0\\) degrees of freedom.\nSupport. \\(x\\in\\mathbb{R}\\).\nPDF. \\[\nf_X(x;\\nu)=\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\n\\left(1+\\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=0\\) for \\(\\nu&gt;1\\); undefined for \\(\\nu\\le 1\\).\n\\(\\operatorname{Var}(X)=\\frac{\\nu}{\\nu-2}\\) for \\(\\nu&gt;2\\); infinite for \\(1&lt;\\nu\\le 2\\); undefined for \\(\\nu\\le 1\\).\n\n\n\\(F\\) distribution\nModel. \\(X \\sim F_{d_1,d_2}\\) with \\(d_1,d_2&gt;0\\) degrees of freedom.\nSupport. \\(x&gt;0\\).\nPDF. \\[\nf_X(x;d_1,d_2)=\\frac{\\Gamma\\!\\left(\\frac{d_1+d_2}{2}\\right)}{\\Gamma\\!\\left(\\frac{d_1}{2}\\right)\\Gamma\\!\\left(\\frac{d_2}{2}\\right)}\n\\left(\\frac{d_1}{d_2}\\right)^{d_1/2}\\frac{x^{d_1/2-1}}{\\left(1+\\frac{d_1}{d_2}x\\right)^{(d_1+d_2)/2}}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{d_2}{d_2-2}\\) for \\(d_2&gt;2\\).\n\\(\\operatorname{Var}(X)=\\frac{2d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)}\\) for \\(d_2&gt;4\\).",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "appendices/formula-sheet.html#sec-beta",
    "href": "appendices/formula-sheet.html#sec-beta",
    "title": "Formula Sheet",
    "section": "Beta",
    "text": "Beta\nModel. \\(X \\sim \\operatorname{Beta}(a,b)\\), \\(a&gt;0, b&gt;0\\).\nSupport. \\(0&lt;x&lt;1\\).\nPDF. \\[\nf_X(x;a,b)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\,x^{a-1}(1-x)^{b-1}.\n\\] Mean/Var. \\(\\mathbb{E}[X]=\\frac{a}{a+b},\\quad \\operatorname{Var}(X)=\\frac{ab}{(a+b)^2(a+b+1)}.\\)\n\nMultivariate Normal\nModel. \\(\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu},\\Sigma)\\) with \\(\\Sigma\\) positive definite.\nSupport. \\(\\mathbf{x}\\in\\mathbb{R}^p\\).\nPDF. \\[\nf_{\\mathbf{X}}(\\mathbf{x};\\boldsymbol{\\mu},\\Sigma)=\\frac{1}{(2\\pi)^{p/2}(\\det\\Sigma)^{1/2}}\n\\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right).\n\\] Mean/Cov. \\(\\mathbb{E}[\\mathbf{X}]=\\boldsymbol{\\mu},\\quad \\operatorname{Var}(\\mathbf{X})=\\Sigma.\\)",
    "crumbs": [
      "Additional Material",
      "Formula Sheet"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#computing-likelihood-functions",
    "href": "lectures/part2/4_likelihood.html#computing-likelihood-functions",
    "title": "5  Likelihood",
    "section": "5.3 Computing Likelihood Functions",
    "text": "5.3 Computing Likelihood Functions",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#computing-likelihood-functions-1",
    "href": "lectures/part2/4_likelihood.html#computing-likelihood-functions-1",
    "title": "5  Likelihood",
    "section": "5.4 Computing Likelihood Functions",
    "text": "5.4 Computing Likelihood Functions\n\n\n\n\n\n\n\nExample 5.3 (Bernoulli Likelihood) Suppose we have \\(n\\) i.i.d. data distributed according to he Bernoulli distribution \\[\n  Y_i \\mid p \\sim \\textrm{Bernoulli}(p).\n  \\] Derive the likelihood function.\n\n\n\n\n:::",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#maximum-likelihood-estimation-1",
    "href": "lectures/part2/4_likelihood.html#maximum-likelihood-estimation-1",
    "title": "5  Likelihood",
    "section": "5.5 Maximum Likelihood Estimation",
    "text": "5.5 Maximum Likelihood Estimation\n\n\n\n\n\n\nNoteDefinition\n\n\n\n\nDefinition 5.3 ### Maximum Likelihood Estimator (MLE) The MLE is the parameter value $$ that maximises the likelihood function: \\[\n  \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_{\\theta \\in \\Theta} L(\\theta \\mid \\underline{x}).\n  \\]\n\n\n\nBecause logs are monotone, it is usually easier to maximise the log-likelihood:\n\\[\n\\ell(\\theta \\mid \\underline{x}) = \\log L(\\theta \\mid \\underline{x}).\n\\]\n\n\n\n\n\n\n\nExample 5.4 (Bernoulli MLE) From Example 5.3, we have\n\\[\nL(\\theta ; x)\n\\]\nCompute the MLE of \\(p\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.1. From the provided likelihood, we first compute the log-likelihood:\n\\[\n\\ell(p \\mid \\underline{x})\n= \\Big(\\sum_i x_i \\Big)\\log p + \\Big(n-\\sum_i x_i\\Big)\\log(1-p).\n\\] Differentiate and solve: \\[\n\\frac{\\partial}{\\partial p}\\ell(p \\mid \\underline{x})\n= \\frac{\\sum_i x_i}{p} - \\frac{n - \\sum_i x_i}{1-p} = 0.\n\\] This gives \\[\n\\hat{p}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nthe sample proportion of successes.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.5 (Binomial MLE) Suppose we have a single Binomial 7.1.2 observation \\[\nX \\mid \\theta \\sim \\textrm{Bin}(n, \\theta).\n\\] Derive its likelihood function.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.2. Since we only have one observation, the likelihood function is just the distribution of the observation: \\[\nL(\\theta ; x) = f(x \\mid \\theta) = \\binom{n}{x} \\theta^x(1-\\theta)^{n-x}.\n\\] The log-likelihood is thus \\[\n\\ell(\\theta; x) = \\log f(x \\mid \\theta) = \\log\\left(\\binom{n}{x}\\right) + x\\log\\theta + (n-x)\\log(1-\\theta).\n\\] Differentiating[^diff-log], we obtain \\[\n\\frac{\\partial}{\\partial \\theta}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 5.6 (Binomial MLE) Suppose we have \\(n\\) independent Binomial data: \\[\nX_i \\mid \\theta \\sim \\textrm{Bin}(n_i, \\theta).\n\\] Note here that each Binomial data has a different number of Bernoulli trials \\(n_i\\), which are known.\nDerive its likelihood function.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.3. The likelihood function is the joint distribution of the \\(n\\) observations \\[\nf(\\underline{x} \\mid \\theta) = \\prod_{i=1}^n \\binom{n_i}{x_i} \\theta^{x_i}(1-\\theta)^{n_i - x_i}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nImportant✨ Big picture:\n\n\n\n\nLikelihood tells us how plausible each parameter value is, given the data.\nThe MLE chooses the parameter value that makes the observed data most likely.\nLater, in Bayesian inference, we will combine the likelihood with a prior distribution on $$ to obtain a posterior distribution.\n\n\n\n[^diff-log] Differentiating the log.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "appendices/revision-sheet.html",
    "href": "appendices/revision-sheet.html",
    "title": "Revision Sheet",
    "section": "",
    "text": "Under Construction…",
    "crumbs": [
      "Additional Material",
      "Revision Sheet"
    ]
  },
  {
    "objectID": "lectures/part2/4_likelihood.html#footnotes",
    "href": "lectures/part2/4_likelihood.html#footnotes",
    "title": "5  Likelihood",
    "section": "",
    "text": "Differentiating the log.↩︎",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part1/0_prob_theory.html#probability-and-events",
    "href": "lectures/part1/0_prob_theory.html#probability-and-events",
    "title": "1  Probability Theory",
    "section": "",
    "text": "The sample space \\(\\Omega\\). all possible outcomes of an experiment.\nExample: for a coin toss, \\(\\Omega = \\{\\text{Heads}, \\text{Tails}\\}\\).\n\nEvents subsets of the sample space. We use capital letters \\(A, B, \\ldots\\) to denote events.\n\nThe complement of \\(A\\) is \\(A^c\\) (“\\(A\\) does not occur”).\n\nThe intersection \\(A \\cap B\\) means \\(A\\) and \\(B\\) occur.\n\nThe union \\(A \\cup B\\) means \\(A\\) or \\(B\\) (or both) occur.\n\n\nA probability distribution \\(\\mathrm{Pr}\\). Assigns numbers between \\(0\\) and \\(1\\) to events, with \\(\\mathrm{Pr}(\\Omega)=1\\).\n\n\n\n\n\n\n\n\nDefinition 1.1 (Intersection and Union)  \n\nIntersection: \\(A\\cap B = \\{c \\in \\Omega \\mid c \\in A \\text{ and } c \\in B\\}\\).\n\nUnion: \\(A\\cup B = \\{c \\in \\Omega \\mid c \\in A \\text{ or } c \\in B\\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Addition rule (inclusion–exclusion)) \\[\n\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Mutually exclusive events) Two events \\(A\\) and \\(B\\) are mutually exclusive if they cannot both occur: \\[\n\\Pr(A \\cap B) = 0.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Exhaustive events) Two events \\(A\\) and \\(B\\) are exhaustive if at least one must occur: \\[\n\\Pr(A \\cup B) = 1.\n\\] (More generally, a collection of events is exhaustive if their union equals the whole sample space.)\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.5 (Independence) Two events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other: \\[\n\\Pr(A \\cap B) = \\Pr(A)\\Pr(B).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.6 (Conditional probability) If \\(\\Pr(B) &gt; 0\\), the probability of \\(A\\) given \\(B\\) is \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Bayes’ Theorem) If \\(\\Pr(B) &gt; 0\\), then \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}.\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof 1.1. From the definition of conditional probability 1.6,\n\\[\n\\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nBut the intersection \\(A \\cap B\\) can also be written the other way round: \\[\n\\Pr(A \\cap B) = \\Pr(B \\cap A).\n\\]\nUsing the definition of conditional probability again, \\[\n\\Pr(B \\cap A) = \\Pr(B \\mid A)\\Pr(A).\n\\]\nSubstituting this into the first expression gives \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}.\n\\]\nThis completes the proof.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Law of Total Probability) If \\(B_1,\\dots,B_n\\) form a partition of the sample space (mutually exclusive and exhaustive events), then for any event \\(A\\): \\[\n\\Pr(A) = \\sum_{i=1}^n \\Pr(A \\mid B_i)\\Pr(B_i).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 1.1 (A quick inclusion–exclusion check) Roll a fair six-sided die. Let \\(A=\\{\\text{even}\\}\\) and \\(B=\\{\\text{number} \\ge 4\\}\\). Compute \\(\\Pr(A\\cup B)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.1. \\(\\Pr(A)=3/6=1/2\\) (even outcomes \\(2,4,6\\)).\n\\(\\Pr(B)=3/6=1/2\\) (outcomes \\(4,5,6\\)).\n\\(\\Pr(A\\cap B)=2/6=1/3\\) (outcomes \\(4,6\\)).\nBy inclusion–exclusion,\n\\[\n\\Pr(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{2}-\\tfrac{1}{3}=\\tfrac{2}{3}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 1.2 (Student Example) Seventy percent of students are attentive in lectures and thirty percent are not. If a student is attentive the probability of passing the course is \\(0.8\\). If a student is not attentive the probability of passing the course is \\(0.1\\).\n\nA student is selected at random. What is the probability that they pass the course?\nGiven that the student passed the course, what is the probability that they were attentive?\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.2. Let\n\n\\(P\\) denote the event of a student passing the course.\n\\(A\\) denote the event of a student being attentive.\n\\(A^c\\) denote the event of a student not being attentive.\n\n\nAttentive and inattentive form a partition. By the law of total probability,\n\n\\[\n\\Pr(P)=\\Pr(P\\mid A)\\Pr(A)+\\Pr(P\\mid A^c)\\Pr(A^c)=0.8\\times0.7+0.1\\times0.3=0.59.\n\\]\n\nUsing Bayes’ theorem,\n\n\\[\n\\Pr(A\\mid P)=\\frac{\\Pr(P\\mid A)\\Pr(A)}{\\Pr(P)}\n=\\frac{0.8\\times0.7}{0.59}=0.949\\ \\text{(3 s.f.)}.\n\\]",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/0_prob_theory.html#expectations-variances-and-covariances",
    "href": "lectures/part1/0_prob_theory.html#expectations-variances-and-covariances",
    "title": "1  Probability Theory",
    "section": "1.3 Expectations, Variances and Covariances",
    "text": "1.3 Expectations, Variances and Covariances",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/0_prob_theory.html#footnotes",
    "href": "lectures/part1/0_prob_theory.html#footnotes",
    "title": "1  Probability Theory",
    "section": "",
    "text": "Formally, a probability space is written as a triple \\((\\Omega,\\Sigma,P)\\),\nwhere \\(\\Sigma\\) is the collection of “measurable” events. This level of mathematical detail goes beyond what we need in this course\nand will not be assessed. (See the Wikipedia article on probability spaces\nif you are curious.)↩︎",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1-prob-theory.html",
    "href": "lectures/part1/1-prob-theory.html",
    "title": "1  Probability Theory",
    "section": "",
    "text": "1.1 Probability and Events\nBefore we delve into statistical inference, we briefly review key ideas from probability theory.\nWhen working with probability we need three basic ingredients1\nWe can write the definitions of complement, intersection and union formally with sets:\nVisualisation:\n// Toggle: highlight selection (use explicit values for robust comparisons)\nviewof vennOp = Inputs.radio(\n  [\"None\", \"A\", \"B\", \"Aᶜ\", \"Union\", \"Intersection\"],\n  { value: \"None\", label: \"Highlight:\" }\n)\nvenn = {\n  const width = 380, height = 280;\n  const pad = 18;\n\n  const cxA = 145, cyA = 140, r = 90;   // circle A\n  const cxB = 235, cyB = 140;           // circle B\n\n  const svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [0, 0, width, height]);\n\n  // Sample space Ω (bounding rectangle)\n  const omega = svg.append(\"rect\")\n    .attr(\"x\", pad).attr(\"y\", pad)\n    .attr(\"width\", width - 2*pad).attr(\"height\", height - 2*pad)\n    .attr(\"rx\", 10)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"currentColor\")\n    .attr(\"stroke-opacity\", 0.5);\n\n  // Label Ω\n  svg.append(\"text\")\n    .attr(\"x\", pad + 8).attr(\"y\", pad + 16)\n    .text(\"Ω\")\n    .style(\"font-weight\", 400)\n    .style(\"font-size\", \"14px\");\n\n  // Colors (CSS vars with fallbacks)\n  const colA   = \"var(--brand-teal, #14b8a6)\";\n  const colB   = \"var(--brand-red, #ef4444)\";\n  const hilite = \"var(--brand-orange, #F39C65)\";\n  const grid   = \"var(--plot-grid, #94a3b8)\";\n\n  // --- Defs: clips & mask for A^c ---\n  const defs = svg.append(\"defs\");\n\n  defs.append(\"clipPath\").attr(\"id\", \"clipA\")\n    .append(\"circle\").attr(\"cx\", cxA).attr(\"cy\", cyA).attr(\"r\", r);\n\n  defs.append(\"clipPath\").attr(\"id\", \"clipB\")\n    .append(\"circle\").attr(\"cx\", cxB).attr(\"cy\", cyB).attr(\"r\", r);\n\n  // Mask for A^c: white keeps, black removes (subtract circle A from Ω rect)\n  const m = defs.append(\"mask\").attr(\"id\", \"maskAc\");\n  m.append(\"rect\")\n    .attr(\"x\", pad).attr(\"y\", pad)\n    .attr(\"width\", width - 2*pad).attr(\"height\", height - 2*pad)\n    .attr(\"rx\", 10)\n    .attr(\"fill\", \"white\");\n  m.append(\"circle\")\n    .attr(\"cx\", cxA).attr(\"cy\", cyA).attr(\"r\", r)\n    .attr(\"fill\", \"black\");\n\n  // Base sets (always lightly transparent)\n  svg.append(\"circle\")\n    .attr(\"cx\", cxA).attr(\"cy\", cyA).attr(\"r\", r)\n    .attr(\"fill\", colA).attr(\"fill-opacity\", 0.18);\n\n  svg.append(\"circle\")\n    .attr(\"cx\", cxB).attr(\"cy\", cyB).attr(\"r\", r)\n    .attr(\"fill\", colB).attr(\"fill-opacity\", 0.18);\n\n  // Highlight layer\n  if (vennOp === \"A\") {\n    svg.append(\"circle\")\n      .attr(\"cx\", cxA).attr(\"cy\", cyA).attr(\"r\", r)\n      .attr(\"fill\", hilite).attr(\"fill-opacity\", 0.99);\n\n  } else if (vennOp === \"B\") {\n    svg.append(\"circle\")\n      .attr(\"cx\", cxB).attr(\"cy\", cyB).attr(\"r\", r)\n      .attr(\"fill\", hilite).attr(\"fill-opacity\", 0.99);\n\n  } else if (vennOp === \"Aᶜ\") {\n    // Draw Ω in orange, but masked to remove A\n    svg.append(\"rect\")\n      .attr(\"x\", pad).attr(\"y\", pad)\n      .attr(\"width\", width - 2*pad).attr(\"height\", height - 2*pad)\n      .attr(\"rx\", 10)\n      .attr(\"fill\", hilite).attr(\"fill-opacity\", 0.99)\n      .attr(\"mask\", \"url(#maskAc)\");\n\n  } else if (vennOp === \"Union\") {\n    // A ∪ B: overlay both circles solid orange\n    svg.append(\"circle\")\n      .attr(\"cx\", cxA).attr(\"cy\", cyA).attr(\"r\", r)\n      .attr(\"fill\", hilite).attr(\"fill-opacity\", 0.99);\n    svg.append(\"circle\")\n      .attr(\"cx\", cxB).attr(\"cy\", cyB).attr(\"r\", r)\n      .attr(\"fill\", hilite).attr(\"fill-opacity\", 0.99);\n\n  } else if (vennOp === \"Intersection\") {\n    // A ∩ B: clip B by A (equivalently clip A by B)\n    svg.append(\"g\").attr(\"clip-path\", \"url(#clipA)\")\n      .append(\"circle\")\n      .attr(\"cx\", cxB).attr(\"cy\", cyB).attr(\"r\", r)\n      .attr(\"fill\", hilite).attr(\"fill-opacity\", 0.99);\n  }\n\n  // Labels A and B\n  svg.append(\"text\")\n    .attr(\"x\", cxA - 26).attr(\"y\", cyA - 8)\n    .text(\"A\")\n    .style(\"font-size\", \"14px\")\n    .style(\"font-weight\", 600);\n\n  svg.append(\"text\")\n    .attr(\"x\", cxB + 22).attr(\"y\", cyB - 8)\n    .text(\"B\")\n    .style(\"font-size\", \"14px\")\n    .style(\"font-weight\", 600);\n\n  // Subtle baseline\n  svg.append(\"line\")\n    .attr(\"x1\", pad).attr(\"x2\", width - pad)\n    .attr(\"y1\", height - pad - 0.5).attr(\"y2\", height - pad - 0.5)\n    .attr(\"stroke\", grid).attr(\"stroke-opacity\", 0.2);\n\n  return svg.node();\n}\n\n\n\n\n\n\n\n\nFigure 1.1: Venn diagram of two events A and B inside the sample space Ω.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1-prob-theory.html#probability-and-events",
    "href": "lectures/part1/1-prob-theory.html#probability-and-events",
    "title": "1  Probability Theory",
    "section": "",
    "text": "The sample space \\(\\Omega\\). all possible outcomes of an experiment.\nExample: for a coin toss, \\(\\Omega = \\{\\text{Heads}, \\text{Tails}\\}\\).\n\nEvents subsets of the sample space. We use capital letters \\(A, B, \\ldots\\) to denote events.\n\nThe complement of \\(A\\) is \\(A^c\\), meaning \\(A\\) does not occur.\n\nThe intersection \\(A \\cap B\\) means \\(A\\) and \\(B\\) occur.\n\nThe union \\(A \\cup B\\) means \\(A\\) or \\(B\\) (or both) occur.\n\n\nA probability distribution \\(\\mathrm{Pr}\\). Assigns numbers between \\(0\\) and \\(1\\) to events, with \\(\\mathrm{Pr}(\\Omega)=1\\).\n\n\n\n\n\n\n\n\n\nDefinition 1.1 (Complement, Intersection and Union)  \n\nComplement: \\(A^c = \\{x \\in \\Omega \\mid x \\notin A \\}\\).\nIntersection: \\(A\\cap B = \\{x \\in \\Omega \\mid x \\in A \\text{ and } x \\in B\\}\\).\n\nUnion: \\(A\\cup B = \\{x \\in \\Omega \\mid x \\in A \\text{ or } x \\in B\\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Addition rule (inclusion–exclusion)) \\[\n\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Mutually exclusive events) Two events \\(A\\) and \\(B\\) are mutually exclusive if they cannot both occur: \\[\n\\Pr(A \\cap B) = 0.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Exhaustive events) Two events \\(A\\) and \\(B\\) are exhaustive if at least one must occur: \\[\n\\Pr(A \\cup B) = 1.\n\\] (More generally, a collection of events is exhaustive if their union equals the whole sample space.)\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.5 (Independence) Two events \\(A\\) and \\(B\\) are independent if knowing that one occurs does not change the probability of the other: \\[\n\\Pr(A \\cap B) = \\Pr(A)\\Pr(B).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.6 (Conditional probability) If \\(\\Pr(B) &gt; 0\\), the probability of \\(A\\) given \\(B\\) is \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Bayes’ Theorem) If \\(\\Pr(B) &gt; 0\\), then \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}.\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof 1.1. From the definition of conditional probability 1.6,\n\\[\n\\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nBut the intersection \\(A \\cap B\\) can also be written the other way round: \\[\n\\Pr(A \\cap B) = \\Pr(B \\cap A).\n\\]\nUsing the definition of conditional probability again, \\[\n\\Pr(B \\cap A) = \\Pr(B \\mid A)\\Pr(A).\n\\]\nSubstituting this into the first expression gives \\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}.\n\\]\nThis completes the proof.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Law of Total Probability) If \\(B_1,\\dots,B_n\\) form a partition of the sample space (mutually exclusive and exhaustive events), then for any event \\(A\\): \\[\n\\Pr(A) = \\sum_{i=1}^n \\Pr(A \\mid B_i)\\Pr(B_i).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 1.1 (A quick inclusion–exclusion check) Roll a fair six-sided die. Let \\(A=\\{\\text{even}\\}\\) and \\(B=\\{\\text{number} \\ge 4\\}\\). Compute \\(\\Pr(A\\cup B)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.1. \\(\\Pr(A)=3/6=1/2\\) (even outcomes \\(2,4,6\\)).\n\\(\\Pr(B)=3/6=1/2\\) (outcomes \\(4,5,6\\)).\n\\(\\Pr(A\\cap B)=2/6=1/3\\) (outcomes \\(4,6\\)).\nBy inclusion–exclusion,\n\\[\n\\Pr(A\\cup B)=\\tfrac{1}{2}+\\tfrac{1}{2}-\\tfrac{1}{3}=\\tfrac{2}{3}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 1.2 (Student Example) Seventy percent of students are attentive in lectures and thirty percent are not. If a student is attentive the probability of passing the course is \\(0.8\\). If a student is not attentive the probability of passing the course is \\(0.1\\).\n\nA student is selected at random. What is the probability that they pass the course?\nGiven that the student passed the course, what is the probability that they were attentive?\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.2. Let\n\n\\(P\\) denote the event of a student passing the course.\n\\(A\\) denote the event of a student being attentive.\n\\(A^c\\) denote the event of a student not being attentive.\n\n\nAttentive and inattentive form a partition. By the law of total probability,\n\n\\[\n\\Pr(P)=\\Pr(P\\mid A)\\Pr(A)+\\Pr(P\\mid A^c)\\Pr(A^c)=0.8\\times0.7+0.1\\times0.3=0.59.\n\\]\n\nUsing Bayes’ theorem,\n\n\\[\n\\Pr(A\\mid P)=\\frac{\\Pr(P\\mid A)\\Pr(A)}{\\Pr(P)}\n=\\frac{0.8\\times0.7}{0.59}=0.949\\ \\text{(3 s.f.)}.\n\\]",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1-prob-theory.html#random-variables",
    "href": "lectures/part1/1-prob-theory.html#random-variables",
    "title": "1  Probability Theory",
    "section": "1.2 Random Variables",
    "text": "1.2 Random Variables\nWe use upper-case letters for random variables, e.g. \\(X, Y, X_1, X_2,\\dots\\).\n\nCumulative distribution function (cdf). For any random variable \\(X\\),\n\\[\nF(x)=\\Pr(X\\le x).\n\\]\nDiscrete case (pmf). If \\(X\\) is discrete, its probability mass function is\n\\[\np(x)=\\Pr(X=x),\\qquad \\sum_x p(x)=1.\n\\]\nContinuous case (pdf). If \\(X\\) is continuous, its probability density function satisfies\n\\[\nf(x)=\\frac{dF(x)}{dx},\\qquad \\Pr(a&lt;X\\le b)=\\int_a^b f(x)\\,dx,\\qquad \\int_{-\\infty}^{\\infty} f(x)\\,dx=1.\n\\]\n\n\n\n\n\n\n\n\nExample 1.3 (Discrete CDF.) Let \\(X\\) be the outcome of a fair six-sided die. Find \\(p(x)\\) and \\(F(3)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.3. \\(p(x)=\\Pr(X=x)=1/6\\) for \\(x\\in\\{1,2,3,4,5,6\\}\\), and \\(0\\) otherwise.\n\\(F(3)=\\Pr(X\\le 3)=\\Pr(\\{1,2,3\\})=3\\times(1/6)=1/2.\\)\n\n\n\n\n\n\n\n\n\n\n\nExample 1.4 Let \\(X\\sim \\mathrm{Uniform}(0,1)\\). Compute \\(\\Pr(0.2&lt;X&lt;0.5)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 1.4. Here \\(f(x)=1\\) for \\(0&lt;x&lt;1\\) and \\(0\\) otherwise.\n\\[\n\\Pr(0.2&lt;X&lt;0.5)=\\int_{0.2}^{0.5} 1\\,dx = 0.3.\n\\]\n\n\n\n\n\n1.2.1 Joint, Marginal, and Conditional Densities\nSuppose \\(X\\) and \\(Y\\) are random variables with joint PDF/PMF \\(f_{XY}(x,y)\\) (often written \\(f(x,y)\\)). From this joint distribution we can define:\n\nMarginal distributions: the distribution of each variable on its own.\n\nConditional distributions: the distribution of one variable given the other.\n\nIndependence: the case where the joint factorises.\n\n\n\n\n\n\n\n\nDefinition 1.7 (Marginal Distributions) If we only care about \\(X\\), regardless of \\(Y\\), we integrate/sum out \\(Y\\): \\[\nf_X(x) = \\int f(x,y)\\,\\mathrm{d}y.\n\\] Similarly for \\(f_Y(y)\\).\nNote: knowing the marginals does not determine the joint.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.8 (Conditional Densities) For \\(y\\) in the support of \\(Y\\) with \\(f_Y(y) &gt; 0\\), we define: \\[\nf_{X \\mid Y}(x \\mid y) = \\frac{f(x,y)}{f_Y(y)}.\n\\] This describes the distribution of \\(X\\) given \\(Y=y\\).\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.3 (Law of Total Probability) We can recover the marginal by “averaging” the conditional over \\(Y\\): \\[\nf_X(x) = \\int f_{X \\mid Y}(x \\mid y) f_Y(y)\\,\\mathrm{d}y.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.9 (Independence) \\(X\\) and \\(Y\\) are independent if and only if \\[\nf(x,y) = f_X(x) f_Y(y).\n\\] In this case, \\(f_{X \\mid Y}(x \\mid y) = f_X(x)\\): knowing \\(Y\\) tells us nothing about \\(X\\).",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1-prob-theory.html#expectations-variances-and-covariances",
    "href": "lectures/part1/1-prob-theory.html#expectations-variances-and-covariances",
    "title": "1  Probability Theory",
    "section": "1.3 Expectations, Variances and Covariances",
    "text": "1.3 Expectations, Variances and Covariances",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/1-prob-theory.html#footnotes",
    "href": "lectures/part1/1-prob-theory.html#footnotes",
    "title": "1  Probability Theory",
    "section": "",
    "text": "Note on Probability Theory. Formally, a probability space is written as a triple \\((\\Omega,\\Sigma,P)\\), where \\(\\Sigma\\) is the collection of “measurable” events. This level of mathematical detail goes beyond what we need in this course and will not be assessed. (See the Wikipedia article on probability spaces if you are curious.)↩︎",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2-what-is-stats-inference.html",
    "href": "lectures/part1/2-what-is-stats-inference.html",
    "title": "2  What is Statistical Inference?",
    "section": "",
    "text": "2.1 Statistical Inference as Inverse Probability Theory\nStatistical inference underpins experimental science, machine learning, clinical trials, and more. The overall aim is:\nTypical conclusions include estimating a proportion or mean, quantifying uncertainty, comparing groups, or predicting future outcomes.\nExamples: 1. Simple one-dimensional example 2. Slightly more convoluted two-dimensional example 3. Machine learning (images of cats)\nWe assume the population can be represented by a probability distribution \\(f(x\\mid \\theta)\\), where \\(\\theta\\) is an unknown parameter (or vector of parameters). Our data are usually modelled as an independent and identically distributed (iid) sample:\n\\[\nX_1,\\ldots,X_n \\stackrel{\\text{iid}}{\\sim} f(x\\mid \\theta).\n\\]\nInference is the process of learning about \\(\\theta\\) (and hence about the population distribution) from the observed sample. In practice, always ask whether iid sampling is plausible for your study design.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2-what-is-stats-inference.html#statistical-inference-as-inverse-probability-theory",
    "href": "lectures/part1/2-what-is-stats-inference.html#statistical-inference-as-inverse-probability-theory",
    "title": "2  What is Statistical Inference?",
    "section": "",
    "text": "Independent: knowing one \\(X_i\\) tells you nothing about another.\nIdentically distributed: each \\(X_i\\) has the same distribution \\(f\\).\n\n\n\n2.1.1 Statistical Inference by Eye\nAs an informal starting point, suppose \\(X_1,\\ldots,X_n\\) are sampled from a \\(\\mathcal{N}(\\mu,\\sigma^2)\\) distribution, with \\(\\mu\\) and \\(\\sigma^2\\) unknown. Try to “guess” \\(\\mu\\) and \\(\\sigma\\) by eye from the data:\n\nhtml`\n&lt;style&gt;\n.controls-grid {\n  display: grid;\n  grid-template-columns: 1fr 1fr 1fr;   /* 3 columns */\n  gap: 0.8rem;\n  align-items: end;\n}\n@media (max-width: 800px){\n  .controls-grid { grid-template-columns: 1fr; }\n}\n&lt;/style&gt;\n`\n\n\n\n\n\n\n\nviewof n = Inputs.range([50, 1000], { value: 200, step: 50, label: \"n\" })\n\ntrueChoices = new Map([\n  [\"N(0, 1²)\",    {mu: 0,  sigma2: 1}],\n  [\"N(0, 2²)\",    {mu: 0,  sigma2: 4}],\n  [\"N(1, 1²)\",    {mu: 1,  sigma2: 1}],\n  [\"N(-2, 0.5²)\", {mu: -2, sigma2: 0.25}],\n  [\"N(2, 1.5²)\",  {mu: 2,  sigma2: 2.25}]\n])\n\nviewof trueDist = Inputs.select(trueChoices, {\n  label: \"True Distribution\",\n  value: trueChoices.get(\"N(0, 1²)\")\n})\n\nviewof approxVar = Inputs.range([0.1, 9], { value: 1, step: 0.1, label: \"Approx. variance (σ²)\" })\nviewof approxMu  = Inputs.range([-5, 5], { value: 0, step: 0.1, label: \"Approx. mean (μ)\" })\n\nviewof resample = Inputs.button(\"Generate new sample\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  const box = html`&lt;div class=\"controls-grid\"&gt;&lt;/div&gt;`;\n  box.append(\n    viewof n,\n    viewof trueDist,\n    viewof approxMu,\n    viewof approxVar,\n    viewof resample\n  );\n  return box;\n}\n\n\n\n\n\n\n\nbodyFont = getComputedStyle(document.body).fontFamily\n\n\n\n\n\n\n\nmu = trueDist.mu\nsigma2 = trueDist.sigma2\n\n\n\n// data\ndata = {\n  resample;\n  const sigma = Math.sqrt(sigma2);\n  const rnd = d3.randomNormal(mu, sigma);\n  return Array.from({length: n}, () =&gt; rnd());\n}\n\napproxSigma = Math.sqrt(approxVar)\n\n// plot\n{\n  const xMin = d3.min(data), xMax = d3.max(data);\n  const pad = 0.1 * (xMax - xMin || 1);\n  const domain = [xMin - pad, xMax + pad];\n\n  const xs = d3.range(domain[0], domain[1], (domain[1]-domain[0]) / 200);\n  const normalPdf = (x, mu, sigma) =&gt; (1/(sigma*Math.sqrt(2*Math.PI))) * Math.exp(-0.5*((x-mu)/sigma)**2);\n  const curve = xs.map(x =&gt; ({x, y: normalPdf(x, approxMu, approxSigma)}));\n\n  const binCount = 30;\n  const binWidth = (domain[1] - domain[0]) / binCount;\n  const scaledCurve = curve.map(d =&gt; ({x: d.x, y: d.y * n * binWidth}));\n\n  return Plot.plot({\n    marginLeft: 48,\n    marginBottom: 40,\n    style: {\n      fontSize: 14,\n      fontFamily: bodyFont,\n    },\n    width: 700,\n    height: 420,\n    x: {label: \"x\", domain},\n    y: {label: \"Count\"},\n    marks: [\n      Plot.rectY(data, Plot.binX({y: \"count\"}, {thresholds: binCount})),\n      Plot.line(scaledCurve, {x: \"x\", y: \"y\", strokeWidth: 2})\n    ]\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(This intuition will be formalised later when we develop estimators and uncertainty quantification.)",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2-what-is-stats-inference.html#statistical-models",
    "href": "lectures/part1/2-what-is-stats-inference.html#statistical-models",
    "title": "2  What is Statistical Inference?",
    "section": "2.2 Statistical Models",
    "text": "2.2 Statistical Models\nWhenever we perform statistical inference, we must specify a statistical model.\n\n\n\n\n\n\n\nDefinition 2.1 (Statistical Model) A statistical model is a set of candidate population distributions: \\[\n\\mathcal{P}=\\{P_\\theta \\mid \\theta\\in\\Theta\\},\n\\] where \\(P_\\theta\\) is a probability distribution indexed by a parameter \\(\\theta\\), and \\(\\Theta\\) is the parameter space (the set of allowable parameter values).\n\n\n\n\nTerminology checkpoint. A parameter \\(\\theta\\) belongs to the model/population; a statistic (e.g. a sample mean) is computed from the data to learn about \\(\\theta\\). Remember: models are working assumptions to be checked.\n\n\n\n\n\n\n\nExample 2.1 (Exponential model) Suppose we are interested in modelling waiting times until some event occurs\n(e.g. the time between customer arrivals at a service desk).\nA simple model is to assume the data are i.i.d. from an exponential distribution: \\[\nX_i \\mid \\lambda \\sim \\mathrm{Exp}(\\lambda).\n\\] Write this formally as a statistical model and write down the parameter space.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.1. Each data follows the exponential distribution with parameter \\(\\lambda\\). Since \\(\\lambda &gt; 0\\), we have the model: \\[\n\\mathcal{P}=\\big\\{\\mathrm{Exp}(\\lambda)\\ \\big|\\ \\lambda&gt;0\\big\\},\n\\] with parameter \\(\\theta=\\lambda\\) and parameter space1 \\[\n\\Theta = \\{\\lambda \\mid \\lambda &gt; 0\\} = (0,\\infty) = \\mathbb{R}^+.\n\\]\nThe exponential distribution is often used for modelling the time between events in a Poisson process — situations where events occur continuously and independently at a constant average rate.\nFor example:\n- the time between buses arriving at a stop if buses come at random,\n- the time until the next radioactive decay of an atom,\n- the waiting time until the next customer enters a shop.\nIn each case, the exponential model is a natural first assumption.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.2 (Normal model) Write down the statistical model and the corresponding parameter space used in Section 2.1.1.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.2. We assumed the data are i.i.d. Normally distributed with unknown mean and unknown variance: \\[\nX_i \\mid \\mu, \\sigma^2 \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2).\n\\]\nThe statistical model is \\[\n\\mathcal{P} = \\{ \\mathcal{N}(\\mu,\\sigma^2) \\mid \\mu \\in \\mathbb{R},\\ \\sigma^2 &gt; 0 \\}.\n\\]\nHere there are two parameters, \\(\\mu\\) and \\(\\sigma^2\\), which we group into a parameter vector \\(\\underline{\\theta} = (\\mu, \\sigma^2)\\).\nThe parameter space is2\n\\[\n\\Theta = \\{ (\\mu, \\sigma^2) \\mid \\mu \\in \\mathbb{R},\\ \\sigma^2 &gt; 0 \\}\n       = \\mathbb{R} \\times (0,\\infty)\n       = \\mathbb{R} \\times \\mathbb{R}^+.\n\\]\nThe normal model is widely used in practice, for example when data are approximately symmetric and unimodal, such as measurement errors in physics or biological traits like human heights.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.3 (From words to a model (one-dimensional)) A questionnaire records whether each respondent would recommend a service: “yes” or “no”. Write down a simple model and identify the parameter.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.3. Let \\(X_i=1\\) if respondent \\(i\\) says “yes”, and \\(0\\) otherwise.\nModel \\(X_1,\\ldots,X_n \\stackrel{\\text{iid}}{\\sim}\\mathrm{Bernoulli}(p)\\), where \\(p=\\Pr(X_i=1)\\) is the (unknown) long-run recommendation proportion.\nHere \\(\\theta=p\\) and \\(\\Theta=(0,1)\\). The population is all (current/future) potential respondents of interest; the sample is the \\(n\\) surveyed respondents.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.4 (A slightly richer example (multinomial)) A bag contains sweets of three colours \\(\\{\\text{red},\\text{blue},\\text{green}\\}\\). You draw \\(n\\) sweets at random with replacement and record counts \\((R,B,G)\\). Propose a model and name the parameter(s).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.4. Model \\((R,B,G) \\sim \\mathrm{Multinomial}\\!\\left(n;\\,p_R,p_B,p_G\\right)\\) with \\(p_R,p_B,p_G\\ge 0\\) and \\(p_R+p_B+p_G=1\\).\nThe parameter is \\(\\theta=(p_R,p_B,p_G)\\), and the parameter space is the 2-simplex \\(\\Theta=\\{(p_R,p_B,p_G)\\in[0,1]^3:\\sum p_\\cdot=1\\}\\). The population is the long-run distribution of colours produced by the bag; the sample is the observed draws.\n\n\n\n\n\n\n\n\n\n\n\nExample 2.5 (Image Classification from Machine Learning) In image classification (cat vs not-cat), each training example is a pair \\((X_i,Y_i)\\) where \\(X_i\\) is an image and \\(Y_i\\in\\{0,1\\}\\). A generic probabilistic model writes the conditional distribution \\[\n\\mathrm{Pr}_\\theta(Y=1\\mid X=x)=g_\\theta(x),\n\\] for some function family \\(g_\\theta\\) (e.g. logistic regression or a neural network). What are the sample, population, model, and parameter?\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 2.5. \n\nSample: the labelled training set \\(\\{(X_i,Y_i)\\}_{i=1}^n\\).\n\nPopulation: all (future) images and labels of interest.\n\nModel: the family of conditional distributions \\(\\{ \\Pr_\\theta(Y=1\\mid X=x)=g_\\theta(x)\\}\\).\n\nParameter: \\(\\theta\\) (e.g. regression co\n\n\n\n\n\n\n\n\n\n\n\nWarningNote on Statistical Models\n\n\n\nIn this course we focus on how to perform statistical inference once a statistical model has been specified.\nWe will assume the model is given, and our task is to carry out inference by finding good parameter values using techniques such as:\n\nParameter estimation\n\nBayesian inference\n\nInterval estimation (e.g. confidence intervals, HDIs)\n\nHypothesis testing",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2-what-is-stats-inference.html#exercises",
    "href": "lectures/part1/2-what-is-stats-inference.html#exercises",
    "title": "2  What is Statistical Inference?",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\n\nFor a study recording daily counts of emails received by a helpdesk, propose a one-parameter model and state the parameter space.\nIn a satisfaction survey with responses \\(\\{\\text{poor},\\text{OK},\\text{good}\\}\\), suggest a simple probabilistic model and identify the parameter(s).\nState what “iid” means in one sentence, and give one example of how it could fail in practice.\nRevisit {ref}stat-by-eye: if the spread of points increases, which parameter of \\(\\mathcal{N}(\\mu,\\sigma^2)\\) has changed, and how would you describe that change informally?",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/2-what-is-stats-inference.html#footnotes",
    "href": "lectures/part1/2-what-is-stats-inference.html#footnotes",
    "title": "2  What is Statistical Inference?",
    "section": "",
    "text": "Set notation. There are several equivalent ways to write sets:\n\nSet-builder notation: \\(\\{\\lambda \\mid \\lambda &gt; 0\\}\\)\n\nInterval notation: \\((0,\\infty)\\)\n\nNamed set: \\(\\mathbb{R}^+\\)\n\nAll three describe the same set. In this course, we recommend using set-builder notation when writing statistical models and their parameter spaces.↩︎\nSet notation (multiple parameters). When a model has several parameters, it is best to use set-builder notation and collect the parameters into a vector \\[\n    \\underline{\\theta} = (\\theta_1, \\theta_2, \\ldots, \\theta_p).\n  \\] The parameter space can then be written as \\[\n    \\Theta = \\{(\\theta_1,\\ldots,\\theta_p) \\mid \\theta_1 \\in S_1,\\ \\ldots,\\ \\theta_p \\in S_p\\}.\n  \\]\nHere each \\(S_j\\) is the set of allowed values for parameter \\(\\theta_j\\). To construct \\(\\Theta\\), simply read off the constraints on each parameter from the definition of the distribution (see the Formula Sheet 0.0.0.0.1).\nFor example, in the Normal model:\n\n\\(\\mu \\in \\mathbb{R}\\) (any real number),\n\n\\(\\sigma^2 \\in (0,\\infty)\\) (variance must be positive),\n\nso the parameter space is\n\\[\n    \\Theta = \\mathbb{R} \\times (0,\\infty).\n  \\]\nThis is called a Cartesian product: we form the parameter space by taking all possible pairs \\((\\mu, \\sigma^2)\\) where the first component is from \\(\\mathbb{R}\\) and the second from \\((0,\\infty)\\).\nIntuitively, it’s just “all combinations” of the allowed values for each parameter.↩︎",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Statistical Inference?</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-freq-vs-bayes.html",
    "href": "lectures/part1/3-freq-vs-bayes.html",
    "title": "3  Types of Inference",
    "section": "",
    "text": "3.1 Frequentist Inference\nThere are two fundamentally different approaches to statistical inference:\nIn practice, the distinction between frequentist and Bayesian inference comes down to what is treated as random and what is not. The table below summarises this key philosophical difference:\nFrequentist inference",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-freq-vs-bayes.html#bayesian-inference",
    "href": "lectures/part1/3-freq-vs-bayes.html#bayesian-inference",
    "title": "3  Types of Inference",
    "section": "3.2 Bayesian Inference",
    "text": "3.2 Bayesian Inference\nBayesian inference specifies a prior over \\(\\theta\\).",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4-freq-point-estimation.html",
    "href": "lectures/part2/4-freq-point-estimation.html",
    "title": "4  Frequentist Point Estimation",
    "section": "",
    "text": "4.1 Estimators vs. Estimates\nA statistic is any function of the sample\n\\[\nT \\;=\\; T(X_1,\\dots,X_n).\n\\]\nBecause it is built from the random variables \\(X_1,\\dots,X_n\\), the statistic \\(T\\) itself is a random quantity: its value varies from sample to sample. Once the data have been observed \\((x_1,\\dots,x_n)\\), plugging them into the same formula produces a single non-random number, which we denote by \\(t_{\\mathrm{obs}}\\) (or simply \\(t\\)).\nWhen we pick a statistic for the specific purpose of approximating a parameter \\(\\theta\\), we call it an estimator. Its realised value is the estimate.\nAn estimator is a random variable; an estimate is its realised value.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4-freq-point-estimation.html#estimators-vs.-estimates",
    "href": "lectures/part2/4-freq-point-estimation.html#estimators-vs.-estimates",
    "title": "4  Frequentist Point Estimation",
    "section": "",
    "text": "Concept\nNotation\nDescription\n\n\n\n\nEstimator\n\\(\\hat{\\theta} = \\hat{\\theta}(X_1, \\dots, X_n)\\)\nA function of the data. A rule for producing an estimate.\n\n\nEstimate\n\\(\\hat{\\theta}_{\\text{obs}} = \\hat{\\theta}(x_1, \\dots, x_n)\\)\nThe numerical value you get after plugging in your data.\n\n\n\n\n\n4.1.1 Sample Mean and Variance\n\n\n\n\n\n\n\nDefinition 4.1 Sample Mean.\nThe sample mean of a random sample \\(X_1,\\ldots,X_n\\) is\n\\[\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4.2 Sample Variance.\nFor a random sample \\(X_1,\\ldots,X_n\\), the sample variance is\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2.\n\\]\n\n\n\n\n\n\n4.1.2 The Randomness of Estimators\nThe randomness of an estimator \\(\\hat{\\theta}\\) is due to randomness of the sample being plugged into the function.\n\nmutable rngSeed = 1\nmutable meanHistory = []\nmutable resampleClicks = 0\nmutable resetClicks = 0\nmutable lastXbar = null\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  if (reset &gt; resetClicks) {\n    mutable meanHistory = [];\n    mutable resetClicks = reset;\n    mutable lastXbar = null;  // Also reset the stored xbar\n  }\n  return html``; // &lt;- prevents \"undefined\" from being shown\n}\n\n\n\n\n\n\n\n{\n  if (resample &gt; resampleClicks) {\n    // If we have a previous mean stored, add it to history\n    if (lastXbar !== null) {\n      mutable meanHistory = [...meanHistory, lastXbar];\n    }\n    // Store current mean for next time\n    mutable lastXbar = xbar;\n    // Then bump seed for new sample\n    mutable rngSeed = rngSeed + 1;\n    mutable resampleClicks = resample;\n  }\n  return html``; // &lt;- prevents \"undefined\" from being shown\n}\n\n\n\n\n\n\n\n\n\nfunction styledButton(label, cls, options = {}) {\n  const el = Inputs.button(label, options);              // &lt;form&gt;…&lt;button&gt;…&lt;/button&gt;&lt;/form&gt;\n  const btn = el.querySelector('button');\n  if (btn) btn.classList.add(cls);                       // add your custom class\n  return el;\n}\n\n// Define themed buttons\nviewof resample = styledButton(\"Resample\", \"btn-resample\", {\n  reduce: c =&gt; (c ?? 0) + 1\n});\n\nviewof reset = styledButton(\"Reset\", \"btn-reset\", {\n  reduce: c =&gt; (c ?? 0) + 1\n});\n\n\n// viewof resample = Inputs.button(\"Resample\", { reduce: c =&gt; (c ?? 0) + 1 }) \n// viewof reset = Inputs.button(\"Reset\", { reduce: c =&gt; (c ?? 0) + 1 })\n\n// --- Current sample depends on rngSeed + controls ---\nX = {\n  const rand = d3.randomNormal.source(d3.randomLcg(rngSeed))(controls.trueMu, controls.trueSigma);\n  return d3.range(controls.n).map(rand);\n}\nxbar = d3.mean(X)\n\n// ========= Helpers =========\nmakeDensityReducer = (total) =&gt; (values, extent) =&gt;\n  values.length / Math.max(1, total) / Math.max(1e-9, extent.x2 - extent.x1);\n\n// ========= Left panel: current sample (histogram + dots at y=0) =========\nleftDensity = makeDensityReducer(controls.n)\nxExtent = d3.extent(X)\nxPad = 3 * controls.trueSigma\nxDomain = [xExtent[0] - 0.1 * (xPad || 1), xExtent[1] + 0.1 * (xPad || 1)]\n\n// === Compute yMax from actual binned densities ===\nbins = d3.bin().domain(xDomain).thresholds(controls.bins)(X);\ndensities = bins.map(b =&gt;\n  (b.length / Math.max(1, controls.n)) / Math.max(1e-9, (b.x1 - b.x0))\n);\nyMax = d3.max(densities);\nstickHeight = 0.1 * yMax; // 10% of the current y-range used by the histogram\n\nleftPlot = Plot.plot({\n  width: Math.min(600, Math.max(320, width/2 - 20)),\n  height: 320,\n  style: {\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"           // drives 'currentColor'\n  },\n  x: { label: \"Data\", domain: xDomain },\n  y: { label: \"Density\" },\n  marks: [\n    // histogram\n    Plot.rectY(\n      X,\n      Plot.binX(\n        { y: leftDensity },\n        {\n          x: d =&gt; d,\n          thresholds: controls.bins,\n          inset: 0,\n          fillOpacity: 0.4,\n          fill: \"var(--brand-teal)\"\n        }\n      )\n    ),\n    // raw points\n    // Plot.dot(\n    //   X.map(x =&gt; ({ x, y: 0 })),\n    //   {\n    //     x: \"x\",\n    //     y: \"y\",\n    //     r: 2,\n    //     fillOpacity: 0.9,\n    //     fill: \"var(--brand-teal)\",\n    //     symbol: \"cross\"\n    //   }\n    // ),\n    Plot.ruleX(\n    X,\n    { y1: 0, y2: yMax * 0.05, stroke: \"var(--brand-teal)\", strokeWidth: 3, strokeOpacity: 1.}\n    ),\n    // xbar reference\n    Plot.ruleX([xbar], { stroke: \"currentColor\", strokeDasharray: \"4,3\" }),\n    Plot.dot([{ x: xbar, y: 0 }], { x: \"x\", y: \"y\", r: 3, fill: \"currentColor\" }),\n    Plot.ruleY([0])\n  ]\n})\n\n// ========= Right panel: saved sample means (strip + hist + optional true curve) =========\nmeans = meanHistory\nrightDensity = makeDensityReducer(means.length)\n\nmPad = 3 * controls.trueSigma / Math.sqrt(controls.n)\nmeansDomain = means.length ? d3.extent(means) : [controls.trueMu - mPad, controls.trueMu + mPad]\nmDomain = [\n  Math.min(meansDomain[0], controls.trueMu - mPad),\n  Math.max(meansDomain[1], controls.trueMu + mPad)\n]\n\nsampleMeanPdf = (x) =&gt;\n  (1 / Math.sqrt(2 * Math.PI * (controls.trueSigma**2 / controls.n))) *\n  Math.exp(-((x - controls.trueMu)**2) / (2 * (controls.trueSigma**2 / controls.n)))\n\npdfXs = d3.scaleLinear().domain(mDomain).ticks(200)\npdfData = pdfXs.map(x =&gt; ({ x, y: sampleMeanPdf(x) }))\n\nrightPlot = Plot.plot({\n  width: Math.min(600, Math.max(320, width/2 - 20)),\n  height: 320,\n  style: {                     // make the figure match your theme\n    background: \"var(--plot-panel-bg)\",\n    color: \"var(--brand-fg)\"   // this drives 'currentColor'\n  },\n  x: { label: \"x̄\", domain: mDomain },\n  y: { label: \"Density\" },\n  marks: [\n    Plot.rectY(\n      means,\n      Plot.binX(\n        { y: rightDensity },\n        { x: d =&gt; d, thresholds: controls.bins, inset: 0, fillOpacity: 0.3,\n          fill: \"var(--brand-red)\" }\n      )\n    ),\n    Plot.dot(means.map(m =&gt; ({ x: m, y: 0 })), {\n      x: \"x\", y: \"y\", r: 2, fillOpacity: 0.7, fill: \"var(--brand-red)\"\n    }),\n    Plot.dot([{ x: xbar, y: 0 }], {\n      x: \"x\", y: \"y\", r: 4, fill: \"currentColor\"     // uses style.color above\n    }),\n    ...(controls.showTrue\n      ? [Plot.line(pdfData, { x: \"x\", y: \"y\", strokeWidth: 2, stroke: \"var(--brand-red)\" })]\n      : []\n    ),\n    Plot.ruleY([0])\n  ]\n})\n\n// ========= Display =========\n\n\n// html`&lt;div style=\"\n//   display:flex;\n//   align-items:center;\n//   gap:10px;\n//   margin-bottom:6px;\n//   width:100%;\n// \"&gt;\n//   &lt;span style=\"display:inline-flex;\"&gt;${viewof resample}&lt;/span&gt;\n//   &lt;span style=\"flex:1 1 auto; min-width:100px;\"&gt;&lt;/span&gt;  &lt;!-- flexible spacer that can shrink --&gt;\n//   &lt;span style=\"display:inline-flex;\"&gt;${viewof reset}&lt;/span&gt;\n// &lt;/div&gt;`\n{\n  // Make the forms behave nicely in the grid\n  for (const f of [viewof resample, viewof reset]) {\n    f.style.margin = \"0\";\n    f.style.width = \"100%\";      // inline style beats the library rule\n    f.style.maxWidth = \"none\";\n    f.style.display = \"flex\";    // so we can push the button left/right\n  }\n  // Left button to the left, right button to the right\n  viewof resample.style.justifyContent = \"flex-start\";\n  viewof reset.style.justifyContent    = \"flex-end\";\n\n  return html`&lt;div class=\"ojs-toolbar-grid\"&gt;\n    &lt;div class=\"left\"&gt;${viewof resample}&lt;/div&gt;\n    &lt;div class=\"center\"&gt;\n      ${md`${tex`\\bar{x} = ${xbar.toFixed(2)}`}`}\n    &lt;/div&gt;\n    &lt;div class=\"right\"&gt;${viewof reset}&lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;style&gt;\n    .ojs-toolbar-grid{\n      display:grid;\n      grid-template-columns:1fr auto 1fr; /* left, middle, right */\n      align-items:center;\n      width:100%;\n      box-sizing:border-box;\n      margin:0; padding:0;\n    }\n    .ojs-toolbar-grid form{\n      margin:0 !important;\n      width:100% !important;\n      max-width:none !important;\n      display:flex !important;\n    }\n    .ojs-toolbar-grid .left  form{ justify-content:flex-start !important; }\n    .ojs-toolbar-grid .right form{ justify-content:flex-end   !important; }\n\n    .btn.btn-resample{ background: var(--brand-teal); color:#fff; border:none; }\n    .btn.btn-resample:hover{ background: var(--brand-teal-hover); }\n    .btn.btn-reset{ background: var(--brand-red); color:#fff; border:none; }\n    .btn.btn-reset:hover{ background: var(--brand-red-hover); }\n    .btn.btn-resample:focus-visible,\n    .btn.btn-reset:focus-visible{\n      outline: 2px solid currentColor;\n      outline-offset: 2px;\n    }\n\n    .ojs-toolbar-grid .center{\n      text-align:center;\n      font-size:0.9rem;\n      color: var(--brand-fg);\n    }\n  &lt;/style&gt;`;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;div style=\"max-width:100%; display:flex; gap:14px; align-items:flex-start; flex-wrap:nowrap;\"&gt;\n  &lt;div&gt;${leftPlot}&lt;/div&gt;\n  &lt;div&gt;${rightPlot}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\nhtml`&lt;style&gt;\ndetails.controls-root &gt; summary {\n  list-style: none; cursor: pointer; user-select: none;\n  background: var(--surface);\n  border: 1px solid var(--border);\n  border-radius: 10px;\n  padding: 8px 10px;\n  font-weight: 600; font-size: 13px;\n  color: var(--fg-strong);\n  display: flex; align-items: center; gap: 8px;\n}\ndetails.controls-root &gt; summary::-webkit-details-marker { display: none; }\ndetails.controls-root[open] &gt; summary { border-bottom-left-radius: 0; border-bottom-right-radius: 0; }\n\n/* ---------- Compact Inputs (fixed slider alignment) ---------- */\n.controls-col label { font-size: 12px !important; }\n\n.controls-col input[type=\"range\"] {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 100%;\n  height: 18px;                     /* overall control box height */\n  background: transparent;\n  vertical-align: middle;\n  accent-color: var(--brand-teal, #55C3CB);\n}\n\n/* WebKit track */\n.controls-col input[type=\"range\"]::-webkit-slider-runnable-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* WebKit thumb */\n.controls-col input[type=\"range\"]::-webkit-slider-thumb {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n  /* Center the 12px thumb over the 4px track: (4 - 12)/2 = -4px */\n  margin-top: -4px;\n}\n\n/* Firefox track */\n.controls-col input[type=\"range\"]::-moz-range-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* Firefox thumb (auto-centred; no negative margin needed) */\n.controls-col input[type=\"range\"]::-moz-range-thumb {\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n}\n\n/* Optional: hide the default filled \"progress\" colour in Firefox */\n.controls-col input[type=\"range\"]::-moz-range-progress {\n  height: 4px; border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n.controls-grid {\n  display: grid;\n  grid-template-columns: minmax(260px,1fr) minmax(260px,1fr);\n  gap: 12px;\n  border: 1px solid var(--border); border-top: none;\n  border-bottom-left-radius: 10px; border-bottom-right-radius: 10px;\n  padding: 10px;\n  background: var(--surface-weak);\n  overflow-x: auto;\n}\n.controls-col { min-width: 260px; }\n.controls-col h4 {\n  margin: 0 0 6px 0; font-size: 12px; font-weight: 700;\n  color: var(--fg-muted);\n}\n\n/* chevron */\ndetails.controls-root &gt; summary::before {\n  content: \"\"; display: inline-block; margin-right: 8px; width: 0; height: 0;\n  border-style: solid; border-width: 3px 0 3px 6px;\n  border-color: transparent transparent transparent var(--fg-strong);\n  transform: rotate(0deg); transform-origin: 3px 50%; transition: transform 50ms ease;\n}\ndetails.controls-root[open] &gt; summary::before { transform: rotate(90deg); }\n&lt;/style&gt;`\n\n\n\n\n\n\n\nviewof controls = {\n\n  // Left column: data & truth\n  const leftForm = Inputs.form({\n    n: Inputs.range([1, 400], { value: 10, step: 1, label: md`${tex`n`}` }),\n    trueMu: Inputs.range([-2, 2], { value: 0, step: 0.1, label: md`${tex`\\mu_{\\text{true}}`}` }),\n    trueSigma: Inputs.range([0.3, 3], { value: 1, step: 0.1, label: md`${tex`\\sigma_{\\text{true}}`}` })\n  }, { submit: false });\n\n  // Right column: display & actions\n  const rightForm = Inputs.form({\n    bins: Inputs.range([10, 60], { value: 24, step: 1, label: \"Histogram bins\" }),\n    showTrue: Inputs.toggle({ label: md`Show ${tex`\\bar{X}`} distribution`, value: false }),\n    // include the buttons as inputs so their *click counts* become part of form.value\n  }, { submit: false });\n\n  // Collapsible wrapper\n  const root = html`&lt;details class=\"controls-root\" open&gt;\n    &lt;summary&gt;Simulation controls&lt;/summary&gt;\n    &lt;div class=\"controls-grid\"&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Data & truth&lt;/h4&gt;&lt;/div&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Display & actions&lt;/h4&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/details&gt;`;\n  root.querySelector(\".controls-grid\").children[0].append(leftForm);\n  root.querySelector(\".controls-grid\").children[1].append(rightForm);\n\n  // Merge both forms into a single value object\n  const getValue = () =&gt; ({ ...leftForm.value, ...rightForm.value });\n  const update = () =&gt; {\n    root.value = getValue();\n    root.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  };\n\n  leftForm.addEventListener(\"input\", update);\n  rightForm.addEventListener(\"input\", update);\n  // make sure button clicks trigger updates even if no 'input' is fired\n\n  queueMicrotask(update);\n  return root;\n}",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4-freq-point-estimation.html#bias-variance-mse-and-consistency",
    "href": "lectures/part2/4-freq-point-estimation.html#bias-variance-mse-and-consistency",
    "title": "4  Frequentist Point Estimation",
    "section": "4.2 Bias, Variance, MSE and Consistency",
    "text": "4.2 Bias, Variance, MSE and Consistency\n\n\n\n\n\n\n\nDefinition 4.3 Bias.\nThe bias of an estimator \\(\\hat{\\theta}\\) for parameter \\(\\theta\\) is\n\\[\n\\mathrm{Bias}(\\hat{\\theta}) = \\mathrm{E}[\\hat{\\theta}] - \\theta.\n\\] If \\(\\mathrm{Bias}(\\hat{\\theta}) = 0\\), the estimator is unbiased.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4.4 Variance.\nThe variance of an estimator \\(\\hat{\\theta}\\) is simply\n\\[\n\\mathrm{Var}(\\hat{\\theta}) = \\mathrm{E}\\!\\big[(\\hat{\\theta}-\\mathrm{E}[\\hat{\\theta}])^2\\big].\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4.5 Mean Squared Error (MSE).\nThe MSE of \\(\\hat{\\theta}\\) is defined as\n\\[\n\\mathrm{MSE}(\\hat{\\theta})\n= \\mathrm{E}\\!\\big[(\\hat{\\theta}-\\theta)^2\\big]\n= \\mathrm{Var}(\\hat{\\theta}) + \\mathrm{Bias}(\\hat{\\theta})^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4.6 Consistency.\nAn estimator \\(\\hat{\\theta}_n\\) (depending on sample size \\(n\\)) is consistent if\n\\[\n\\hat{\\theta}_n \\;\\xrightarrow{p}\\; \\theta\n\\quad \\text{as } n \\to \\infty,\n\\]\ni.e. it converges in probability to the true parameter value.\n\n\n\n\n\n4.2.1 Examples\n\n\n\n\n\n\n\nExample 4.1 (Bias of the Sample Mean) For \\(X_1,\\dots,X_n \\overset{iid}{\\sim}\\) distribution with mean \\(\\mu\\):\n\\[\n\\mathrm{E}[\\bar{X}] = \\mu\n\\quad\\implies\\quad\n\\mathrm{Bias}(\\bar{X}) = 0.\n\\]\nSo the sample mean is unbiased for \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\nExample 4.2 (Variance of the Sample Mean) If \\(\\mathrm{Var}(X_i) = \\sigma^2\\):\n\\[\n\\mathrm{Var}(\\bar{X})\n= \\frac{1}{n^2} \\sum_{i=1}^n \\mathrm{Var}(X_i)\n= \\frac{\\sigma^2}{n}.\n\\]\nSo as \\(n\\) grows, the estimator becomes more concentrated around \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\nExample 4.3 (Bias of the Sample Variance) If we instead used the “naïve” variance\n\\[\ns^2_{naive} = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2,\n\\]\nthen\n\\[\n\\mathrm{E}[s^2_{naive}] = \\frac{n-1}{n}\\sigma^2,\n\\]\nwhich underestimates \\(\\sigma^2\\).\nThat is why we divide by \\(n-1\\) instead of \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\nExample 4.4 (MSE of Sample Mean) Suppose we estimate \\(\\mu\\) with \\(\\hat{\\mu} = c\\bar{X}\\), where \\(c\\) is a constant.\n\nBias:\n\\[\n\\mathrm{Bias}(\\hat{\\mu}) = (c-1)\\mu.\n\\]\nVariance:\n\\[\n\\mathrm{Var}(\\hat{\\mu}) = c^2 \\frac{\\sigma^2}{n}.\n\\]\nMSE:\n\\[\n\\mathrm{MSE}(\\hat{\\mu})\n= \\frac{c^2\\sigma^2}{n} + (c-1)^2 \\mu^2.\n\\]\n\nThis shows how a biased estimator can still have low MSE if variance is reduced.\n\n\n\n\n\n\n\n\n\n\n\nExample 4.5 (Consistency of \\(\\bar{X}\\)) By the Law of Large Numbers,\n\\[\n\\bar{X} \\;\\xrightarrow{p}\\; \\mu,\n\\]\nso the sample mean is a consistent estimator of \\(\\mu\\).",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/4-freq-point-estimation.html#central-limit-theorem-clt",
    "href": "lectures/part2/4-freq-point-estimation.html#central-limit-theorem-clt",
    "title": "4  Frequentist Point Estimation",
    "section": "4.3 Central Limit Theorem (CLT)",
    "text": "4.3 Central Limit Theorem (CLT)\n\n\n\n\n\n\n\nDefinition 4.7 (Central Limit Theorem) If \\(X_1,\\dots,X_n \\overset{iid}{\\sim}\\) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), then\n\\[\n\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\;\\xrightarrow{d}\\; N(0,1).\n\\]\n\n\n\n\nThe CLT explains why many estimators (like \\(\\bar{X}\\)) are approximately normal in large samples, which is crucial for constructing confidence intervals and hypothesis tests.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequentist Point Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5-likelihood.html",
    "href": "lectures/part2/5-likelihood.html",
    "title": "5  Likelihood",
    "section": "",
    "text": "5.1 From Joint Density to Likelihood\nThe likelihood function is the joint density (or probability mass) function of the data:\n\\[\nf(\\underline{x} \\mid \\theta), \\quad \\underline{x} = (x_1, \\dots, x_n).\n\\]\nSuppose we observe \\(n\\) independent and identically distributed (i.i.d.) samples,\n\\[\nX_1, \\dots, X_n \\overset{\\text{iid}}{\\sim} f_X(x \\mid \\theta).\n\\]\nThen the joint density is given by multiplying the individual densities:\n\\[\nf(\\underline{x} \\mid \\theta)\n= \\prod_{i=1}^n f_X(x_i \\mid \\theta).\n\\]",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5-likelihood.html#from-joint-density-to-likelihood",
    "href": "lectures/part2/5-likelihood.html#from-joint-density-to-likelihood",
    "title": "5  Likelihood",
    "section": "",
    "text": "TipRemark (Why products?)\n\n\n\n\n\n\nRemark 5.1 (Why products?). \n\nIndependence means that joint probabilities factorise into products:\n\\[\nP(A \\cap B) = P(A)\\,P(B).\n\\]\nBy the same rule, the joint density of independent random variables is the product of their individual densities.\nThis is often the first place where students forget the independence assumption! Without it, you cannot just multiply.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5-likelihood.html#likelihood-function",
    "href": "lectures/part2/5-likelihood.html#likelihood-function",
    "title": "5  Likelihood",
    "section": "5.2 Likelihood Function",
    "text": "5.2 Likelihood Function\n\n\n\n\n\n\n\nDefinition 5.1 Likelihood Function. For fixed observed data \\(\\underline{x}\\), the likelihood function is defined as\n\\[\nL(\\theta ; \\underline{x}) \\coloneqq f(\\,\\underline{x} \\mid \\theta).\n\\]\nHere we treat \\(\\theta\\) as the variable, and the data \\(\\underline{x}\\) as fixed.\n\n\n\n\n\n\n\n\n\n\nTipLikelihood vs. Probability:\n\n\n\n\nPDF/PMF: function of \\(\\underline{x}\\) (data random, parameter fixed).\nLikelihood: function of \\(\\theta\\) (data fixed, parameter variable).\n\n\n\n\nVisualisation:\n\nhtml`&lt;style&gt;\n.plot--large-axis .x-axis .label,\n.plot--large-axis .y-axis .label {\nfont-size: 24px; font-weight: 600;\n}\n.plot--large-axis .x-axis text,\n.plot--large-axis .y-axis text {\nfont-size: 12px; /* tick labels (optional) */\n}\n&lt;/style&gt;`\n\n\n\n\n\n\n\nX = {\n  // Trigger on: resim button, n, trueMu, or trueSigma changes\n  resim; n; trueMu; trueSigma;\n  \n  const m = trueMu, s = trueSigma;\n  const out = new Array(n);\n  for (let i = 0; i &lt; n; i += 2) {\n    const u = Math.random(), v = Math.random();\n    const R = Math.sqrt(-2 * Math.log(u)), T = 2 * Math.PI * v;\n    const z0 = R * Math.cos(T), z1 = R * Math.sin(T);\n    out[i] = m + s * z0;\n    if (i + 1 &lt; n) out[i + 1] = m + s * z1;\n  }\n  return out;\n};\n\nxbar = d3.mean(X);\nS = d3.sum(X, x =&gt; (x - xbar) ** 2);\n\nlogLik = (mu, sigma2) =&gt; {\n  const ss = d3.sum(X, x =&gt; (x - mu) ** 2);\n  return -0.5 * n * Math.log(2 * Math.PI * sigma2) - 0.5 * ss / sigma2;\n};\n\nmuhat = xbar;\nsigma2hat = S / n;\n\n// --- Grid for contours (axes ticks) ---\n// These only need to update when display parameters change\nmuGrid = d3.ticks(muhat - muHalfWidth, muhat + muHalfWidth, gridN)\nsig2Grid = d3.ticks(Math.max(0.05, sigma2hat / 20), sig2Max, gridN).filter(s =&gt; s &gt; 0)\n\n// --- Build a row-major grid for Plot.raster / Plot.contour ---\nnx = muGrid.length\nny = sig2Grid.length\n\n// Memoize the expensive log-likelihood computation\nllValues = {\n  const z = new Float64Array(nx * ny);\n  for (let j = 0; j &lt; ny; ++j) {\n    for (let i = 0; i &lt; nx; ++i) {\n      const s2 = Math.max(1e-6, sig2Grid[j]);\n      z[j * nx + i] = logLik(muGrid[i], s2);\n    }\n  }\n  return z;\n}\n\n\n// ---- Geometry / domains (responsive) ----\ngap = 20\ncontainerW = width\nplotW = Math.round(Math.max(240, Math.min(600, (containerW - gap) / 2 - 15)))\nplotH = Math.round(plotW * 0.8)\n\nmarginTop = 20\nmarginRight = 35\nmarginBottom = 20\nmarginLeft = 35\ninnerW = plotW - marginLeft - marginRight\ninnerH = plotH - marginTop - marginBottom\n\nmuDomain = [muGrid[0], muGrid[muGrid.length - 1]]\nsig2Domain = [sig2Grid[0], sig2Grid[sig2Grid.length - 1]]\n\n// ---- Colour scaling (robust & reactive) ----\nllMax = d3.max(llValues)\nllRel = Float64Array.from(llValues, v =&gt; v - llMax)\n\n// Use more efficient quantile computation\nllRelFinite = Array.from(llRel).filter(Number.isFinite)\nllRelSorted = llRelFinite.slice().sort(d3.ascending)\nq05 = d3.quantileSorted(llRelSorted, 0.05) ?? d3.min(llRelFinite)\n\ncolorDomain = [q05, 0]\nllRelClipped = Float64Array.from(llRel, v =&gt; Math.max(q05, v))\ncontourThresholds = d3.ticks(q05, 0, 10)\n\nbodyFont = getComputedStyle(document.body).fontFamily\n\n// ---- Left panel: raster (clipped) + contour (same grid) ----\nviewof leftPanel = {\n  const wrap = html`&lt;div style=\"\n    position: relative;\n    width: ${plotW}px;\n    height: ${plotH}px;\n    flex: 0 0 ${plotW}px;\n    line-height: 0;\n  \"&gt;&lt;/div&gt;`;\n\n  const base = Plot.plot({\n    width: plotW, height: plotH,\n    marginLeft, marginRight, marginTop, marginBottom,\n    style: {\n      background: \"var(--brand-bg)\",\n      color: \"var(--brand-fg)\",\n      fontFamily: bodyFont, \n      fontSize: 14\n    },\n    x: { label: \"μ\", domain: muDomain },\n    y: { label: \"σ²\", domain: sig2Domain },\n    color: {\n      type: \"symlog\",\n      domain: colorDomain,\n      clamp: true,\n      scheme: \"turbo\",\n      legend: false\n    },\n    marks: [\n      Plot.raster(llRelClipped, {\n        width: nx, height: ny,\n        x1: muDomain[0], x2: muDomain[1],\n        y1: sig2Domain[0], y2: sig2Domain[1],\n        interpolate: \"nearest\"\n      }),\n      Plot.dot([{ mu: muhat, sig2: sigma2hat }], {\n        x: \"mu\", y: \"sig2\", r: 5, stroke: \"currentColor\", fill: \"var(--brand-bg)\"\n      })\n    ]\n  });\n\n  base.classList.add(\"plot--large-axis\");\n  base.style.display = \"block\";\n  wrap.append(base);\n\n\n  // Crosshair overlay fills wrapper\n  const svg = d3.create(\"svg\")\n    .attr(\"width\", plotW).attr(\"height\", plotH)\n    .style(\"position\", \"absolute\").style(\"left\", \"0\").style(\"top\", \"0\")\n    .style(\"width\", \"100%\").style(\"height\", \"100%\")\n    .style(\"pointer-events\", \"none\");\n\n\n\n  const xScale = d3.scaleLinear().domain(muDomain).range([marginLeft, marginLeft + innerW]);\n  const yScale = d3.scaleLinear().domain(sig2Domain).range([marginTop + innerH, marginTop]);\n\n  const vline = svg.append(\"line\")\n  .attr(\"x1\", xScale(muhat)).attr(\"x2\", xScale(muhat))\n  .attr(\"y1\", marginTop).attr(\"y2\", marginTop + innerH)\n  .attr(\"stroke\", \"currentColor\");\n\n  const hline = svg.append(\"line\")\n  .attr(\"x1\", marginLeft).attr(\"x2\", marginLeft + innerW)\n  .attr(\"y1\", yScale(sigma2hat)).attr(\"y2\", yScale(sigma2hat))\n  .attr(\"stroke\", \"currentColor\");\n\n  wrap.append(svg.node());\n\n  // Hitbox fills wrapper; subtract margins in code\n  const hit = html`&lt;div style=\"\n    position:absolute; left:0; top:0; width:100%; height:100%;\n    cursor:crosshair; background:transparent; touch-action:none;\n  \"&gt;&lt;/div&gt;`;\n  wrap.append(hit);\n\n  const state = { x: muhat, y: sigma2hat, dragging: false };\n  wrap.value = { ...state };            // &lt;-- expose full state via the view's value\n\n  function emit() {\n    wrap.value = { ...state };          // &lt;-- assign a fresh object so equality changes\n    wrap.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  }\n\n  function setFromEvent(evt) {\n    const r = hit.getBoundingClientRect();\n    let px = evt.clientX - r.left, py = evt.clientY - r.top;\n    px = Math.min(Math.max(0, px - marginLeft), innerW);\n    py = Math.min(Math.max(0, py - marginTop), innerH);\n    state.x = muDomain[0] + (px / innerW) * (muDomain[1] - muDomain[0]);\n    state.y = sig2Domain[1] - (py / innerH) * (sig2Domain[1] - sig2Domain[0]);\n    vline.attr(\"x1\", xScale(state.x)).attr(\"x2\", xScale(state.x));\n    hline.attr(\"y1\", yScale(state.y)).attr(\"y2\", yScale(state.y));\n    emit();                              // &lt;-- notify dependents\n  }\n\n  // RAF throttle for move\n  let raf = 0;\n  function scheduleUpdate(e) {\n    if (raf) return;\n    raf = requestAnimationFrame(() =&gt; { raf = 0; setFromEvent(e); });\n  }\n\n  function setDragging(on) {\n    if (state.dragging === on) return;\n    state.dragging = on;\n    emit();                              // &lt;-- notify on drag state changes too\n  }\n\n  hit.addEventListener(\"pointerdown\", (e) =&gt; {\n    setDragging(true);\n    setFromEvent(e);\n    hit.setPointerCapture(e.pointerId);\n  });\n\n  hit.addEventListener(\"pointermove\", (e) =&gt; {\n    if (state.dragging) scheduleUpdate(e);\n  });\n\n  const endDrag = () =&gt; setDragging(false);\n  hit.addEventListener(\"pointerup\", endDrag);\n  hit.addEventListener(\"pointercancel\", endDrag);\n  hit.addEventListener(\"pointerleave\", endDrag);\n\n  // initial notify\n  queueMicrotask(emit);\n\n  return wrap;                           // &lt;-- the view element\n}\n\n\n// isDragging = leftPanel.dragging ?? false\n// console.log(isDragging)\n// numPts = isDragging ? 100 : 200   // fewer x-samples while dragging\n// pdfStroke = isDragging ? 1 : 2   // thinner line while dragging\n\n\n// // Read probe values reactively (no extra visible block)\n// probeRaw = Generators.input(leftPanel.hit);\n// probeMu = (Array.isArray(probeRaw) ? probeRaw[0] : probeRaw?.x) ?? muhat;\n// probeSig2 = (Array.isArray(probeRaw) ? probeRaw[1] : probeRaw?.y) ?? sigma2hat;\n// probeLL = logLik(probeMu, probeSig2);\n\n\n// leftPanel is now { x, y, dragging }\nisDragging = leftPanel.dragging\nnumPts     = isDragging ? 100 : 200\npdfStroke  = isDragging ? 1   : 2\n\n// if you want the probe to follow the crosshair:\nprobeMu   = leftPanel.x\nprobeSig2 = leftPanel.y\nprobeLL = logLik(probeMu, probeSig2);\n\n\n// --- Right panel ---\nnormalPdf = (x, mu, sig2) =&gt; (1 / Math.sqrt(2 * Math.PI * sig2)) * Math.exp(-((x - mu) ** 2) / (2 * sig2))\n\nxExtent = d3.extent(X)\nxPad = 3 * Math.sqrt(probeSig2)\nxMin = Math.min(xExtent[0], probeMu - xPad)\nxMax = Math.max(xExtent[1], probeMu + xPad)\n\n// PDF curve\nxs = d3.range(numPts).map(i =&gt; xMin + (i / (numPts - 1)) * (xMax - xMin))\npdfYs = xs.map(x =&gt; normalPdf(x, probeMu, probeSig2))\n\n// Density reducer\ndensityReducer = (values, extent) =&gt; values.length / n / (extent.x2 - extent.x1)\n\n// Compute yMax\nbins_ = d3.bin().domain([xMin, xMax]).thresholds(bins)(X);\ndensities = bins_.map(b =&gt;\n  (b.length / Math.max(1, n)) / Math.max(1e-9, (b.x1 - b.x0))\n);\nyMax = d3.max(densities);\n\nrightPanel = Plot.plot({\n  width: plotW, height: plotH,\n  marginLeft, marginBottom,\n  x: { label: \"Data\", domain: [xMin, xMax] },\n  y: { label: \"Density\" },\n  style: {\n      fontFamily: bodyFont, \n      fontSize: 14.\n  }, \n  marks: [\n    Plot.rectY(\n      X,\n      Plot.binX(\n        { y: densityReducer },\n        {\n          x: d =&gt; d,\n          thresholds: bins,\n          inset: 0,\n          fill: \"var(--brand-red)\",\n          fillOpacity: 0.2\n        }\n      )\n    ),\n    Plot.line(xs.map((x, i) =&gt; ({ x, y: pdfYs[i] })), {\n      x: \"x\", y: \"y\",\n      stroke: \"var(--brand-red)\",\n      strokeWidth: pdfStroke\n    }),\n    Plot.ruleX(X, {\n      y1: 0,\n      y2: yMax * 0.05,\n      stroke: \"var(--brand-red)\",\n      strokeWidth: 2,\n      strokeOpacity: 0.8\n    }),\n    Plot.ruleY([0]),\n    Plot.ruleX([probeMu], { stroke: \"currentColor\", strokeDasharray: \"4,3\" })\n  ]\n})\n\nprobeInfo = md`**Probe:** ${tex`\\mu=${probeMu.toFixed(2)},\\ \\sigma^2=${probeSig2.toFixed(3)}\\ \\Rightarrow\\ \\log f(\\underline{x} \\mid \\mu, \\sigma^2)=${probeLL.toFixed(2)}`}`\n\nhtml`&lt;div style=\"max-width:100%;\"&gt;\n&lt;div style=\"display:flex; gap:${gap}px; align-items:flex-start; flex-wrap:nowrap; overflow-x:auto;\"&gt;\n&lt;div style=\"flex:0 0 ${plotW}px;\"&gt;${viewof leftPanel}&lt;/div&gt;\n&lt;div style=\"flex:0 0 ${plotW}px;\"&gt;${rightPanel}&lt;/div&gt;\n&lt;/div&gt;\n&lt;div style=\"margin-top:10px; margin-bottom:-10px\"&gt;${probeInfo}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;style&gt;\n/* ---------- Collapsible container ---------- */\ndetails.controls-root &gt; summary {\n  list-style: none; cursor: pointer; user-select: none;\n  background: var(--surface, #f1f5f9);\n  border: 1px solid var(--border, #e5e7eb);\n  border-radius: 10px;\n  padding: 8px 10px;\n  font-weight: 600; font-size: 13px;\n  color: var(--fg-strong, #334155);\n  display: flex; align-items: center; gap: 8px;\n}\ndetails.controls-root &gt; summary::-webkit-details-marker { display: none; }\ndetails.controls-root[open] &gt; summary { border-bottom-left-radius: 0; border-bottom-right-radius: 0; }\n\n/* ---------- Two-column grid ---------- */\n.controls-grid {\n  display: grid;\n  grid-template-columns: minmax(260px, 1fr) minmax(260px, 1fr);\n  gap: 12px;\n  border: 1px solid var(--border, #e5e7eb); border-top: none;\n  border-bottom-left-radius: 10px; border-bottom-right-radius: 10px;\n  padding: 10px;\n  background: var(--surface-weak, #fafafa);\n  overflow-x: auto; /* prefer scroll over wrapping on narrow screens */\n}\n\n.controls-col { min-width: 260px; }\n.controls-col h4 {\n  margin: 0 0 6px 0;\n  font-size: 12px; font-weight: 700;\n  color: var(--fg-muted, #475569);\n}\n\n/* ---------- Compact Inputs (fixed slider alignment) ---------- */\n.controls-col label { font-size: 12px !important; }\n\n.controls-col input[type=\"range\"] {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 100%;\n  height: 18px;                     /* overall control box height */\n  background: transparent;\n  vertical-align: middle;\n  accent-color: var(--brand-teal, #55C3CB);\n}\n\n/* WebKit track */\n.controls-col input[type=\"range\"]::-webkit-slider-runnable-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* WebKit thumb */\n.controls-col input[type=\"range\"]::-webkit-slider-thumb {\n  -webkit-appearance: none;\n  appearance: none;\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n  /* Center the 12px thumb over the 4px track: (4 - 12)/2 = -4px */\n  margin-top: -4px;\n}\n\n/* Firefox track */\n.controls-col input[type=\"range\"]::-moz-range-track {\n  height: 4px;\n  border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n/* Firefox thumb (auto-centred; no negative margin needed) */\n.controls-col input[type=\"range\"]::-moz-range-thumb {\n  width: 12px; height: 12px; border-radius: 50%;\n  background: var(--brand-teal, #55C3CB);\n  border: 0;\n}\n\n/* Optional: hide the default filled \"progress\" colour in Firefox */\n.controls-col input[type=\"range\"]::-moz-range-progress {\n  height: 4px; border-radius: 999px;\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 35%, transparent);\n}\n\n\n/* ---------- Button ---------- */\n.btn-resim button {\n  background: var(--brand-teal, #55C3CB);\n  color: #fff;\n  border: none;\n  border-radius: 6px;\n  padding: 6px 12px;\n  font-weight: 600;\n  font-size: 12px;\n  cursor: pointer;\n  transition: filter 120ms ease, background-color 120ms ease;\n}\n\n/* hover effect */\n.btn-resim button:hover {\n  background: color-mix(in srgb, var(--brand-teal, #55C3CB) 88%, black);\n}\n\n/* chevron */\ndetails.controls-root &gt; summary::before {\n  content: \"\"; display: inline-block; margin-right: 8px; width: 0; height: 0;\n  border-style: solid; border-width: 3px 0 3px 6px;\n  border-color: transparent transparent transparent var(--fg-strong);\n  transform: rotate(0deg); transform-origin: 3px 50%; transition: transform 50ms ease;\n}\ndetails.controls-root[open] &gt; summary::before { transform: rotate(90deg); }\n&lt;/style&gt;`\n\n\n\n\n\n\n\nviewof controls = {\n  const resimBtn = Inputs.button(\"Resimulate data\");\n  resimBtn.classList.add(\"btn-resim\");\n\n  const leftForm = Inputs.form({\n    n: Inputs.range([1, 400], { value: 10, step: 1, label: md`${tex`n`}` }),\n    trueMu: Inputs.range([-3, 3], { value: 0.5, step: 0.05, label: md`${tex`\\mu_{\\text{true}}`}` }),\n    trueSigma: Inputs.range([0.3, 3], { value: 1.2, step: 0.05, label: md`${tex`\\sigma_{\\text{true}}`}` })\n  }, { submit: false });\n\n  const rightForm = Inputs.form({\n    muHalfWidth: Inputs.range([1, 5], { value: 2.5, step: 0.05, label: md`${tex`\\mu\\text{-range}`}` }),\n    sig2Max: Inputs.range([0.2, 6], { value: 3, step: 0.05, label: md`${tex`\\sigma^2\\text{ max}`}` }),\n    gridN: Inputs.range([25, 121], { value: 81, step: 4, label: \"Grid size\" }),\n    bins: Inputs.range([10, 60], { value: 24, step: 1, label: \"Histogram bins\" })\n  }, { submit: false });\n\n  const root = html`&lt;details class=\"controls-root\" open&gt;\n    &lt;summary&gt;Simulation controls&lt;/summary&gt;\n    &lt;div class=\"controls-grid\"&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Data & truth&lt;/h4&gt;&lt;/div&gt;\n      &lt;div class=\"controls-col\"&gt;&lt;h4&gt;Grid & display&lt;/h4&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/details&gt;`;\n\n  const grid = root.querySelector(\".controls-grid\");\n  const leftCol = grid.children[0];\n  const rightCol = grid.children[1];\n\n  leftCol.append(leftForm);\n\n  rightCol.append(rightForm);\n  const btnWrap = html`&lt;div style=\"display:flex; justify-content:flex-start; margin-top:6px;\"&gt;&lt;/div&gt;`;\n  btnWrap.append(resimBtn);\n  rightCol.append(btnWrap);\n\n  // --- Merge values\n  const getValue = () =&gt; ({ ...leftForm.value, ...rightForm.value, resim: resimBtn.value });\n  const update = () =&gt; {\n    root.value = getValue();\n    root.dispatchEvent(new CustomEvent(\"input\", { bubbles: true }));\n  };\n\n  // ONE listener for all children (sliders + button)\n  grid.addEventListener(\"input\", update);\n\n  queueMicrotask(update);\n  return root;\n}\n\n// --- Expose reactive vars\nn = controls.n\ntrueMu = controls.trueMu\ntrueSigma = controls.trueSigma\nmuHalfWidth = controls.muHalfWidth\nsig2Max = controls.sig2Max\ngridN = controls.gridN\nbins = controls.bins\nresim = controls.resim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.1 Computing Likelihood Functions\nThis is a very important part of the course.\n\n\n\n\n\n\n\nExample 5.1 (Bernoulli Likelihood) Suppose we have \\(n\\) i.i.d. data distributed according to the Bernoulli distribution 1.1:\n\\[\n  Y_i \\mid p \\sim \\textrm{Bernoulli}(p).\n\\] Derive the likelihood function.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2 (Bernoulli Likelihood) Suppose \\(n=5\\) trials give data \\(\\underline{x} = (1,0,1,1,0)\\). Then \\(\\sum\\_i x\\_i = 3\\). \\[\nL(p ; \\underline{x}) = p^3 (1-p)^2.\n\\]\n\nIf \\(p=0.5\\), \\(L(0.5) = 0.5^3 \\cdot 0.5^2 = 0.03125\\).\nIf \\(p=0.7\\), \\(L(0.7) = 0.7^3 \\cdot 0.3^2 \\approx 0.0309\\).\nIf \\(p=0.8\\), \\(L(0.8) = 0.8^3 \\cdot 0.2^2 = 0.0205\\).\n\nSo the data are slightly more likely under \\(p=0.5\\) than \\(p=0.8\\).",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5-likelihood.html#maximum-likelihood-estimation",
    "href": "lectures/part2/5-likelihood.html#maximum-likelihood-estimation",
    "title": "5  Likelihood",
    "section": "5.3 Maximum Likelihood Estimation",
    "text": "5.3 Maximum Likelihood Estimation\n\n\n\n\n\n\n\nDefinition 5.2 (Maximum Likelihood Estimator (MLE)) The MLE is the parameter value \\(\\hat{\\theta}\\) that maximises the likelihood function: \\[\n  \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_{\\theta \\in \\Theta} L(\\theta \\mid \\underline{x}).\n  \\]\n\n\n\n\nBecause logs are monotone, it is usually easier to maximise the log-likelihood:\n\\[\n\\ell(\\theta ; \\underline{x}) = \\log L(\\theta ; \\underline{x}).\n\\]\n\n5.3.1 Computing MLEs\n\n\n\n\n\n\n\nExample 5.3 (Bernoulli MLE) From Example 5.1, we have\n\\[\nL(\\theta ; x)\n\\]\nCompute the MLE of \\(p\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.1. From the provided likelihood, we first compute the log-likelihood:\n\\[\n\\ell(p \\mid \\underline{x})\n= \\Big(\\sum_i x_i \\Big)\\log p + \\Big(n-\\sum_i x_i\\Big)\\log(1-p).\n\\] Differentiate and solve: \\[\n\\frac{\\partial}{\\partial p}\\ell(p \\mid \\underline{x})\n= \\frac{\\sum_i x_i}{p} - \\frac{n - \\sum_i x_i}{1-p} = 0.\n\\] This gives \\[\n\\hat{p}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nthe sample proportion of successes.\n\n\n\n\n\n\n\n\n\n\n\nExample 5.4 (Binomial MLE) Suppose we have a single Binomial 1.2 observation \\[\nX \\mid \\theta \\sim \\textrm{Bin}(n, \\theta).\n\\] Derive its likelihood function.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.2. Since we only have one observation, the likelihood function is just the distribution of the observation: \\[\nL(\\theta ; x) = f(x \\mid \\theta) = \\binom{n}{x} \\theta^x(1-\\theta)^{n-x}.\n\\] The log-likelihood is thus \\[\n\\ell(\\theta; x) = \\log f(x \\mid \\theta) = \\log\\left(\\binom{n}{x}\\right) + x\\log\\theta + (n-x)\\log(1-\\theta).\n\\] Differentiating1, we obtain \\[\n\\frac{\\partial}{\\partial \\theta}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 5.5 (Binomial MLE) Suppose we have \\(n\\) independent Binomial data: \\[\nX_i \\mid \\theta \\sim \\textrm{Bin}(n_i, \\theta).\n\\] Note here that each Binomial data has a different number of Bernoulli trials \\(n_i\\), which are known.\nDerive its likelihood function.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 5.3. The likelihood function is the joint distribution of the \\(n\\) observations \\[\nf(\\underline{x} \\mid \\theta) = \\prod_{i=1}^n \\binom{n_i}{x_i} \\theta^{x_i}(1-\\theta)^{n_i - x_i}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nImportant✨ Big picture:\n\n\n\n\nLikelihood tells us how plausible each parameter value is, given the data.\nThe MLE chooses the parameter value that makes the observed data most likely.\nLater, in Bayesian inference, we will combine the likelihood with a prior distribution on \\(\\theta\\) to obtain a posterior distribution.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5-likelihood.html#footnotes",
    "href": "lectures/part2/5-likelihood.html#footnotes",
    "title": "5  Likelihood",
    "section": "",
    "text": "Differentiating the log.↩︎",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part2/6-bayes-inference.html",
    "href": "lectures/part2/6-bayes-inference.html",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "6.1 Introduction to the Bayesian approach\nAs discussed in the course introduction, Bayesian inference takes a different perspective from the frequentist approach:\nThe randomness here does not mean that the parameter is physically fluctuating — instead, it reflects our uncertainty about its value.",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/6-bayes-inference.html#introduction-to-the-bayesian-approach",
    "href": "lectures/part2/6-bayes-inference.html#introduction-to-the-bayesian-approach",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "In the frequentist view, the parameter \\(\\theta\\) is fixed but unknown.\nIn the Bayesian view, the parameter \\(\\theta\\) is treated as a random variable.\n\n\n\n6.1.1 Why is this useful?\nA key advantage of Bayesian inference is that it allows us to incorporate prior information1 about the parameter.\n\n\n\n\n\n\n\nDefinition 6.1 (Prior Distribution.) Our uncertainty about \\(\\theta\\) is represented by a probability distribution, called the prior distribution:\n\\[\n\\theta \\sim \\pi(\\theta).\n\\]\nHere \\(\\pi(\\theta)\\) denotes the prior density (PDF or PMF).\n\n\n\n\n\n\n\n\n\n\n\nExample 6.1 Suppose \\(\\theta\\) is the average exam score of students in a module. From past experience, the average score tends to be around 60, with most cohorts lying within about 10 marks of this.\nWe could encode this information as a Normal prior:\n\\[\n\\theta \\sim \\mathcal{N}(\\mu=60,\\;\\sigma^2=10^2).\n\\]\nThis prior expresses our belief before seeing the new data.\n\n\n\n\n\nPythonR\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ndef normal_pdf(theta, mean, sd):\n    const = 1 / (sd * (2 * math.pi) ** 0.5)\n    exp_term = np.exp(-((theta - mean) / sd) ** 2 / 2)\n    return const * exp_term\n\nmean = 100.0\nsd   = 10.0\n\ntheta_grid = np.linspace(mean - 3*sd, mean + 3*sd, 200)\npdf_values = normal_pdf(theta_grid, mean=mean, sd=sd)\n\nplt.figure(figsize=(7,4))\nplt.plot(theta_grid, pdf_values)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"Density\")\nplt.title(\"Normal PDF: mean = 100, sd = 10\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\nmean &lt;- 100\nsd   &lt;- 10\n\ntheta_grid &lt;- seq(mean - 3*sd, mean + 3*sd, length.out = 200)\npdf_values &lt;- dnorm(theta_grid, mean = mean, sd = sd)\n\nplot(theta_grid, pdf_values, type = \"l\",\n     xlab = expression(theta),\n     ylab = \"Density\",\n     main = \"Normal PDF: mean = 100, sd = 10\")",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/6-bayes-inference.html#bayes-theorem",
    "href": "lectures/part2/6-bayes-inference.html#bayes-theorem",
    "title": "6  Bayesian Inference",
    "section": "6.2 Bayes’ Theorem",
    "text": "6.2 Bayes’ Theorem\nBayesian inference takes its name from Bayes’ theorem, which tells us how to update our beliefs about \\(\\theta\\) after seeing data.\nRecall from probability that for events \\(A\\) and \\(B\\):\n\\[\n\\mathrm{Pr}(A \\mid B) = \\frac{\\mathrm{Pr}(B \\mid A)\\,\\mathrm{Pr}(A)}{\\mathrm{Pr}(B)}.\n\\]\nTo generalise this idea to random variables and densities, we use conditional densities:\n\\[\nf_{X \\mid Y}(x \\mid y) = \\frac{f_{XY}(x,y)}{f_Y(y)}.\n\\]\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark 6.1. Notation. We will usually write \\(f(x \\mid y)\\) rather than \\(f_{X \\mid Y}(x \\mid y)\\) to keep things simple. Both mean the same thing.\n\n\n\n\nUsing this, we obtain Bayes’ theorem for densities.\n\n\n\n\n\n\n\nTheorem 6.1 (Bayes’ Theorem for Densities.) For random variables \\(X\\) and \\(Y\\):\n\\[\nf(x \\mid y) = \\frac{f(y \\mid x)\\,f(x)}{f(y)}.\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof 6.1. By the definition of conditional density:\n\\[\nf(x \\mid y) = \\frac{f(x,y)}{f(y)},\n\\qquad\nf(y \\mid x) = \\frac{f(x,y)}{f(x)}.\n\\]\nRearranging the second expression gives \\(f(x,y) = f(y \\mid x) f(x)\\). Substituting this into the first gives\n\\[\nf(x \\mid y) = \\frac{f(y \\mid x) f(x)}{f(y)}.\n\\]",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/6-bayes-inference.html#bayes-theorem-in-statistics",
    "href": "lectures/part2/6-bayes-inference.html#bayes-theorem-in-statistics",
    "title": "6  Bayesian Inference",
    "section": "6.3 Bayes’ Theorem in Statistics",
    "text": "6.3 Bayes’ Theorem in Statistics\nIn Bayesian inference, we apply Bayes’ theorem with:\n\n\\(x = \\theta\\) (the parameter, treated as random),\n\\(y = \\underline{x}\\) (the observed data).\n\nThis gives:\n\\[\n\\pi(\\theta \\mid \\underline{x})\n= \\frac{f(\\underline{x} \\mid \\theta)\\,\\pi(\\theta)}{f(\\underline{x})}.\n\\]\n\n\n\n\n\n\n\nDefinition 6.2 (Posterior Distribution.) \\[\n\\pi(\\theta \\mid \\underline{x})\n= \\frac{f(\\underline{x} \\mid \\theta)\\,\\pi(\\theta)}{f(\\underline{x})}.\n\\]\nThe posterior combines two ingredients:\n\nThe likelihood \\(f(\\underline{x} \\mid \\theta)\\) (information from the data).\nThe prior \\(\\pi(\\theta)\\) (information before seeing the data).\n\nThe denominator \\(f(\\underline{x})\\) is a normalising constant ensuring the posterior integrates to 1.\n\n\n\n\n\n\n\n\n\n\nImportant✨ Big picture:\n\n\n\nBayesian inference updates prior beliefs about parameters using the likelihood from the data, producing the posterior distribution which represents updated uncertainty.\n\n\nLet’s start with a simple example to see how this works:\n\n\n\n\n\n\n\nExample 6.2 (Biased Coin) We consider a possibly biased coin with parameter \\(\\theta = \\Pr(\\text{Head})\\). Before seeing data, we express no preference over \\(\\theta\\in(0,1)\\) via a uniform prior: \\[\n\\theta \\sim \\mathrm{Uniform}(0,1) \\equiv \\mathrm{Beta}(1,1), \\qquad \\pi(\\theta)=1 \\ \\text{for } 0&lt;\\theta&lt;1.\n\\] Hence \\(\\mathrm{E}[\\theta]=0.5\\).\nWe toss the coin \\(n=5\\) times and observe \\(x=1\\) head. Find the posterior \\(\\pi(\\theta\\mid x)\\).\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nSolution 6.1. Let \\(X\\) be the number of heads in \\(5\\) tosses. Assuming that each coin throw is independent and identically distributed, \\(X\\) is Binomial2: \\[\n    X \\mid \\theta \\sim \\textrm{Bin}(5,\\theta).\n\\] Note that there is only a single observation here: one experiment is “tossing the coin \\(5\\) times and seeing how many come up heads”. The probability of observing \\(X=1\\) is \\[\n    f(x = 1 \\mid \\theta) = 5\\theta(1-\\theta)^4\n\\] If we plot this:\n\n\nShow code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ndef binom_lik(theta):\n    return 5 * theta * (1 - theta) ** 4\n\n\ntheta_grid = np.linspace(0, 1, 200)\nbinom_lik_vals = binom_lik(theta_grid)\n\nplt.figure(figsize=(5,3))\nplt.plot(theta_grid, binom_lik_vals)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"Likelihood\")\nplt.title(\"Binomial Likelihood (with $n=5$ and $X=1$)\")\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n\n\n\n\nwe see that this favours \\(\\theta\\) around \\(0.2\\). In fact, in Example 5.4, we saw that the MLE is \\(\\hat{\\theta} = 0.2\\).\nOur prior for \\(\\theta\\) was \\(\\pi(\\theta) = 1\\), for \\(0 &lt; \\theta &lt; 1\\). To update our beliefs, by conditioning on the single observation \\(x=2\\), we use Bayes Theorem 6.2: \\[\n    \\pi(\\theta|x=1)=\\frac{\\pi(\\theta)f(x=1|\\theta)}{f(x=1)} = \\frac{1 \\times 5\\theta(1-\\theta)^4}{f(x=1)}, \\quad 0 &lt; \\theta &lt; 1.\n\\] To compute the denominator, \\(f(x=1)\\), we use the law of total probability 1.3: \\[\n\\begin{align*}\nf(x=1) &=\\int_\\Theta\\pi(\\theta)f(x=1\\, |\\,\\theta)\\,d\\theta = \\int_0^1 1\\times 5\\theta(1-\\theta)^4\\,d\\theta \\\\\n&=\\int_0^1 \\theta\\times 5(1-\\theta)^4\\,d\\theta =\\left[-(1-\\theta)^5\\,\\theta\\right]^1_0\n+\\int_0^1 (1-\\theta)^5\\,d\\theta \\\\\n&=0 + \\left[-\\frac{(1-\\theta)^6}{6}\\right]^1_0 =\\frac{1}{6}.\n\\end{align*}\n\\]\nTherefore, the posterior density is (for \\(0&lt;\\theta&lt;1\\)): \\[\n\\begin{align*}\n\\pi(\\theta|x=1)&=\\frac{\\pi(\\theta)f(x=1|\\theta)}{f(x=1)} =\\frac{5\\theta(1-\\theta)^4}{1/6}\\\\\n&=30\\,\\theta(1-\\theta)^4 =\\frac{\\theta(1-\\theta)^4}{\\mathrm{B}(2,5)},\\quad\\quad 0&lt;\\theta&lt;1,\n\\end{align*}\n\\] and so the posterior distribution is \\(\\theta|x=1\\sim \\textrm{Beta}(2,5)\\). This distribution has its mode at \\(\\theta=0.2\\), and mean at \\(\\mathrm{E}[\\theta|x=1]=2/7=0.286\\).\n\n\n\n\nThe main difficulty in calculating the posterior distribution was in obtaining the \\(f(x)\\) term. However, in many cases we can recognise the posterior distribution without the need to calculate this constant term (constant with respect to \\(\\theta\\)). In this example, we can calculate the posterior distribution as \\[\n\\begin{align*}\n\\pi(\\theta|\\underline{x})&\\propto\\pi(\\theta)f(x=1|\\theta) \\\\\n&\\propto 1\\times 5\\theta(1-\\theta)^4,\\quad\\quad 0&lt;\\theta&lt;1  \\\\\n&=k\\theta(1-\\theta)^4,\\quad\\quad 0&lt;\\theta&lt;1.\n\\end{align*}\n\\]\nAs \\(\\theta\\) is a continuous quantity, what we would like to know is what continuous distribution defined on \\((0,1)\\) has a probability density function which takes the form \\(k\\theta^{g-1}(1-\\theta)^{h-1}\\). The answer is the \\(\\textrm{Beta}(g,h)\\) distribution. Therefore, choosing \\(g\\) and \\(h\\) appropriately, we can see that the posterior distribution is \\(\\theta|x=1\\sim \\textrm{Beta}(2,5)\\).\nSummary:\nIt is possible that we have a biased coin. If we suppose that all values of \\(\\theta=\\text{Pr(Head)}\\) are equally likely and then observe 1 head out of 5, then the most likely value of \\(\\theta\\) is 0.2 — the same as the most likely value from the data alone (not surprising!). However, on average, we would expect \\(\\theta\\) to be around 0.286. Uncertainty about \\(\\theta\\) has changed from a (prior) standard deviation of 0.289 to a (posterior) standard deviation of 0.160. The changes in our beliefs about \\(\\theta\\) are more fully described by the prior and posterior distributions shown in Figure 6.1.\n\n\nmDomain = [0, 1]\npdfXs = d3.scaleLinear().domain(mDomain).ticks(200)  // 201 points including 0 and 1\n\n// PDFs (closed-form)\nfunction priorPDF(x)     { return (x &gt; 0 && x &lt; 1) ? 1 : 0; }\nfunction posteriorPDF(x) { return (x &gt; 0 && x &lt; 1) ? 30 * x * (1 - x) ** 4 : 0; }\n\n// Data (use the scalar x from map!)\nprior = pdfXs.map(x =&gt; ({ x, y: priorPDF(x), which: \"Prior  Beta(1,1)\" }))\npost  = pdfXs.map(x =&gt; ({ x, y: posteriorPDF(x), which: \"Posterior  Beta(2,5)\" }))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbodyFont = getComputedStyle(document.body).fontFamily\n// Figure\nviewof fig_beta = Plot.plot({\n  width: 640,\n  height: 320,\n  marginLeft: 48,\n  marginBottom: 40,\n  x: { label: \"θ\", domain: [0, 1] },\n  y: { label: \"Density\" },\n  style: { background: \"var(--plot-panel-bg)\", \n           fontSize: 14,\n           fontFamily: bodyFont,  },\n  marks: [\n    Plot.ruleY([0], { stroke: \"var(--border)\", strokeOpacity: 0.6 }),\n    Plot.lineY(prior, { x: \"x\", y: \"y\", stroke: \"var(--brand-red)\",  strokeWidth: 2, strokeOpacity: 0.85 }),\n    Plot.areaY(prior, { x: \"x\", y: \"y\", fill: \"var(--brand-red)\", fillOpacity: 0.05 }),\n    Plot.lineY(post,  { x: \"x\", y: \"y\", stroke: \"var(--brand-teal)\", strokeWidth: 2 }),\n    Plot.areaY(post,  { x: \"x\", y: \"y\", fill: \"var(--brand-teal)\", fillOpacity: 0.2 }),\n    Plot.text(\n      [\n        { x: 0.8, y: priorPDF(0.8),     label: \"Prior\" },\n        { x: 0.35, y: posteriorPDF(0.3), label: \"Posterior\" }\n      ],\n      { x: \"x\", y: \"y\", text: \"label\", dy: -8 }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 6.1: Prior and posterior distributions for the coin bias parameter \\(\\theta=\\Pr(\\text{Head})\\) after observing 1 head in 5 tosses. The posterior is more concentrated, reflecting reduced uncertainty about \\(\\theta\\) compared to the uniform prior\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.3 Consider an experiment to determine how good a music expert is at distinguishing between pages from Haydn and Mozart scores. Let \\(\\theta=\\mathrm{Pr}(\\text{correct choice})\\). Suppose that, before conducting the experiment, we have been told that the expert is very competent. In fact, it is suggested that we should have a prior distribution which has a mode around \\(\\theta=0.95\\) and for which \\(\\mathrm{Pr}(\\theta&lt;0.8)\\) is very small. We choose \\(\\theta\\sim \\textrm{Beta}(77,5)\\), with probability density function \\[\n\\pi(\\theta)=128107980\\,\\theta^{76}(1-\\theta)^4,\\quad\\quad 0&lt;\\theta&lt;1.\n\\] A graph of this prior density is given as follows:\n\nPythonR\n\n\n\n\nShow code\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Beta(α, β) with α=77, β=5  →  π(θ) ∝ θ^(76) (1-θ)^(4)\nalpha, beta = 77, 5\n\n# B(α,β) = Γ(α)Γ(β)/Γ(α+β)  ⇒  1/B = exp(lgamma(α+β) - lgamma(α) - lgamma(β))\n# Note the lgamma function computes the logarithm of the Gamma function.\nbeta_const_inv = math.exp(math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta))\n\ndef beta_pdf(theta):\n    return beta_const_inv * np.power(theta, alpha - 1) * np.power(1 - theta, beta - 1)\n\ntheta_grid = np.linspace(0.0, 1.0, 500)\npdf_values = beta_pdf(theta_grid)\n\nplt.figure(figsize=(7,4))\nplt.plot(theta_grid, pdf_values)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"Density\")\nplt.title(rf\"Beta PDF: $\\alpha={alpha}$, $\\beta={beta}$\")\n# plt.xlim(0, 1)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\nalpha &lt;- 77\nbeta  &lt;- 5\n\n# Normalising constant via log-gamma (works like Python's lgamma)\nbeta_const_inv &lt;- exp(lgamma(alpha + beta) - lgamma(alpha) - lgamma(beta))\n\nbeta_pdf &lt;- function(theta) {\n  beta_const_inv * theta^(alpha - 1) * (1 - theta)^(beta - 1)\n}\n\ntheta_grid &lt;- seq(0, 1, length.out = 500)\npdf_values &lt;- beta_pdf(theta_grid)\n\nplot(theta_grid, pdf_values, type = \"l\",\n     xlab = expression(theta), ylab = \"Density\",\n     main = bquote(\"Beta PDF: \" ~ alpha == .(alpha) ~ \",\" ~ beta == .(beta)))",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/6-bayes-inference.html#footnotes",
    "href": "lectures/part2/6-bayes-inference.html#footnotes",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "Prior distribution. In Bayesian inference, the prior represents our beliefs about the parameter \\(\\theta\\) before seeing any data. It is written as a probability distribution \\(\\pi(\\theta)\\), which can be based on previous studies, expert knowledge, or simply chosen to reflect uncertainty (e.g. a flat prior).↩︎\nBinomial distribution. If we assume each coin throw \\(C_i\\) is an i.i.d. \\(\\textrm{Bernoulli}(\\theta)\\) trial with a \\(1\\) representing a heads landing, and a \\(0\\) representing a tails, the total number of heads out of \\(5\\) is \\(X = C_1 + C_2 + C_3 + C_4 + C_5\\). Therefore: \\[\n    X \\mid \\theta \\sim \\textrm{Bin}(5,\\theta).\n\\]↩︎",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-types-of-inference.html",
    "href": "lectures/part1/3-types-of-inference.html",
    "title": "3  Types of Inference",
    "section": "",
    "text": "3.1 Philosophies of Inference\nStatistical inference is about using a sample to learn about a population. There are two fundamentally different philosophies of inference, and several types of inferential tasks that appear in both.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-types-of-inference.html#bayesian-inference",
    "href": "lectures/part1/3-types-of-inference.html#bayesian-inference",
    "title": "3  Types of Inference",
    "section": "3.2 Bayesian Inference",
    "text": "3.2 Bayesian Inference\nBayesian inference specifies a prior over \\(\\theta\\).",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-types-of-inference.html#philosophies-of-inference",
    "href": "lectures/part1/3-types-of-inference.html#philosophies-of-inference",
    "title": "3  Types of Inference",
    "section": "",
    "text": "3.1.1 Frequentist Inference\nFrequentist inference is based on the idea of long-run frequency.\n\nThe data are considered random, as they represent one possible outcome of repeated sampling from the population.\nThe parameter \\(\\theta\\) is fixed but unknown.\nInference is about constructing procedures (estimators, tests, intervals) that perform well under repeated sampling.\n\nPreview examples (we will return to these later): the sample mean \\(\\hat{\\mu}\\) as an estimator of \\(\\mu\\), a \\(t\\)-test for comparing means, or a \\(95\\%\\) confidence interval.\n\n\n3.1.2 Bayesian Inference\nBayesian inference is based on subjective probability as degree of belief.\n\nThe data are fixed once observed.\nThe parameter \\(\\theta\\) is treated as a random variable, with uncertainty described by a prior distribution \\(\\pi(\\theta)\\).\nInference updates this prior in light of the data, giving a posterior distribution \\(\\pi(\\theta \\mid \\text{data})\\).\n\nPreview examples (we will return to these later): a posterior mean as an estimator of \\(\\mu\\), a 95% credible interval, or comparing hypotheses via posterior probabilities.\n\n\n3.1.3 Key Differences\n\n\n\n\n\n\n\n\nPerspective\nFrequentist Inference\nBayesian Inference\n\n\n\n\nParameter\nFixed but unknown\nRandom variable with a prior\n\n\nSample\nRandom (from repeated sampling)\nFixed (once observed)\n\n\nProbability\nLong-run frequency\nDegree of belief\n\n\nGoal\nConstruct estimators, tests, CIs\nUpdate beliefs via posterior\n\n\nPrior Knowledge\nIgnored or minimal (e.g. non-informative)\nExplicitly modelled via prior distribution\n\n\nOutput\nEstimators, confidence intervals, \\(p\\)-values\nPosterior distribution, credible intervals",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-types-of-inference.html#types-of-inference-output",
    "href": "lectures/part1/3-types-of-inference.html#types-of-inference-output",
    "title": "3  Types of Inference",
    "section": "3.2 Types of Inference Output",
    "text": "3.2 Types of Inference Output\nRegardless of philosophy, the core tasks of inference are similar. Suppose we want to infer a parameter \\(\\theta\\) from data:\n\nPoint Estimation Give a single “best guess” \\(\\hat{\\theta}\\) for the parameter.\n\nFrequentist: choose an estimator with good properties (e.g. unbiasedness).\nBayesian: often the posterior mean, median, or mode.\n\nInterval Estimation Quantify uncertainty in \\(\\theta\\) by reporting a range.\n\nFrequentist: confidence interval\n\\[\nL \\leq \\theta \\leq U\n\\]\nwith a given long-run coverage probability.\nBayesian: credible interval, the posterior probability that \\(\\theta\\) lies in \\(\\[L,U]\\) is, say, 95%.\n\nHypothesis Testing {#sec-hypothesis-testing-intro} Formalise a claim about \\(\\theta\\), e.g. \\(H\\_0: \\theta \\in \\Theta\\_0\\), and use data to decide whether it is compatible.\n\nFrequentist: \\(p\\)-values, test statistics, significance levels.\nBayesian: posterior probabilities for competing hypotheses, or Bayes factors.\n\nPrediction Use data to predict future or unobserved outcomes.\n\nFrequentist: predictive distributions based on sampling models.\nBayesian: posterior predictive distribution, integrating over parameter uncertainty.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/5-likelihood.html#summary",
    "href": "lectures/part2/5-likelihood.html#summary",
    "title": "5  Likelihood",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nWe can compute likelihood functions",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Likelihood</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-types-of-inference.html#inference-tasks",
    "href": "lectures/part1/3-types-of-inference.html#inference-tasks",
    "title": "3  Types of Inference",
    "section": "3.2 Inference Tasks",
    "text": "3.2 Inference Tasks\nRegardless of philosophy, the core tasks of inference are similar. Suppose we want to infer a parameter \\(\\theta\\) from data:\n\nPoint Estimation Give a single “best guess” \\(\\hat{\\theta}\\) for the parameter.\n\nFrequentist: choose an estimator with good properties (e.g. unbiasedness).\nBayesian: often the posterior mean, median, or mode.\n\nInterval Estimation Quantify uncertainty in \\(\\theta\\) by reporting a range.\n\nFrequentist: confidence interval\n\\[\nL \\leq \\theta \\leq U\n\\]\nwith a given long-run coverage probability.\nBayesian: credible interval, the posterior probability that \\(\\theta\\) lies in \\(\\[L,U]\\) is, say, 95%.\n\nHypothesis Testing {#sec-hypothesis-testing-intro} Formalise a claim about \\(\\theta\\), e.g. \\(H\\_0: \\theta \\in \\Theta\\_0\\), and use data to decide whether it is compatible.\n\nFrequentist: \\(p\\)-values, test statistics, significance levels.\nBayesian: posterior probabilities for competing hypotheses, or Bayes factors.\n\nPrediction Use data to predict future or unobserved outcomes.\n\nFrequentist: predictive distributions based on sampling models.\nBayesian: posterior predictive distribution, integrating over parameter uncertainty.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part3/8-bayesian-hdis.html",
    "href": "lectures/part3/8-bayesian-hdis.html",
    "title": "8  Bayesian Intervals",
    "section": "",
    "text": "8.1 Bayesian Credible Intervals",
    "crumbs": [
      "Part III - Interval Estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Intervals</span>"
    ]
  },
  {
    "objectID": "appendices/question-bank.html",
    "href": "appendices/question-bank.html",
    "title": "Question Bank",
    "section": "",
    "text": "Under Construction…",
    "crumbs": [
      "Additional Material",
      "Question Bank"
    ]
  },
  {
    "objectID": "lectures/part3/7-confidence-intervals.html",
    "href": "lectures/part3/7-confidence-intervals.html",
    "title": "7  Frequentist Interval Estimation",
    "section": "",
    "text": "7.1 The Frequentist Perspective\nRecalling the main inference tasks 3.2, we now move from point estimation to interval estimation.\nThe key idea is that instead of giving a single “best guess”1 of a parameter, we want to provide a range of plausible values. In other words, an interval.\nRecall that, for frequentists, the parameter \\(\\theta\\) is fixed but unknown.\nThe randomness comes from the data: if we repeated the experiment many times, we would get different samples, different estimates, and hence different intervals.",
    "crumbs": [
      "Part III - Interval Estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequentist Interval Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part3/7-confidence-intervals.html#footnotes",
    "href": "lectures/part3/7-confidence-intervals.html#footnotes",
    "title": "7  Frequentist Interval Estimation",
    "section": "",
    "text": "Recall from Chapter 6 that Bayesians do not return a single “best guess,” but a full posterior distribution over parameter values. From this distribution we can compute summaries such as plausible ranges of \\(\\theta\\) (see Chapter 8), which play a role similar to confidence intervals but with a different interpretation.↩︎",
    "crumbs": [
      "Part III - Interval Estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequentist Interval Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part3/7-confidence-intervals.html#the-frequentist-perspective",
    "href": "lectures/part3/7-confidence-intervals.html#the-frequentist-perspective",
    "title": "7  Frequentist Interval Estimation",
    "section": "",
    "text": "Point Estimate: a single statistic \\(\\hat{\\theta}(\\underline{X})\\).\nInterval estimate: two statistics \\(L(\\underline{X})\\) and \\(U(\\underline{X})\\) that form a random interval.\n\n\n\n\n\n\n\n\nDefinition 7.1 (Confidence Interval) A \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\theta\\) is a random interval \\[\n\\big[L(\\underline{X}),\\; U(\\underline{X})\\big]\n\\] satisfying \\[\n\\mathrm{Pr}\\!\\big(L(\\underline{X}) &lt; \\theta &lt; U(\\underline{X})\\big) = 1 - \\alpha.\n\\]\nThis property is called coverage: in the long run, a proportion \\(1-\\alpha\\) of such intervals (constructed from repeated experiments) will contain the true \\(\\theta\\).",
    "crumbs": [
      "Part III - Interval Estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequentist Interval Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part3/7-confidence-intervals.html#one-observed-sample",
    "href": "lectures/part3/7-confidence-intervals.html#one-observed-sample",
    "title": "7  Frequentist Interval Estimation",
    "section": "7.2 One Observed Sample",
    "text": "7.2 One Observed Sample\nIn practice, we only ever observe one dataset \\(\\underline{x}\\). So we calculate the observed interval: \\[\n[L_{\\text{obs}}, U_{\\text{obs}}]\n= \\big[L(\\underline{x}), U(\\underline{x})\\big].\n\\]\n\n\n\n\n\n\nImportantConfidence Interval Interpretation\n\n\n\nOnce the data are observed, this interval is fixed, not random.\nTherefore, a frequentist would not say \\[\n\\mathrm{Pr}(L_{\\text{obs}} &lt; \\theta &lt; U_{\\text{obs}}) = 1 - \\alpha.\n\\] That statement is meaningless, because \\(\\theta\\) is not random in the frequentist view.\nInstead, the correct interpretation is: our method has long-run coverage \\(1-\\alpha\\).",
    "crumbs": [
      "Part III - Interval Estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequentist Interval Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part3/7-confidence-intervals.html#deriving-confidence-intervals",
    "href": "lectures/part3/7-confidence-intervals.html#deriving-confidence-intervals",
    "title": "7  Frequentist Interval Estimation",
    "section": "7.3 Deriving Confidence Intervals",
    "text": "7.3 Deriving Confidence Intervals\nExact confidence intervals are not always available. When possible, we aim for well-calibrated (exact coverage) intervals; otherwise, we use approximate intervals (e.g. via the Central Limit Theorem).\nHowever, there are a few cases where we can compute closed-form well-calibrated confidence intervals:\n\n7.3.1 The Normal case (Known Variance)\nSuppose we observe \\(n\\) i.i.d. samples from a normal distribution with known variance:\n\\[\nX_i \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad i=1,\\dots,n.\n\\]\n\nThe sample mean \\(\\bar{X}\\) has distribution:\n\\[\n\\bar{X} \\sim \\mathcal{N}\\!\\left(\\mu, \\tfrac{\\sigma^2}{n}\\right).\n\\]\nStandardising:\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1).\n\\]\nFrom quantiles of the standard normal, we obtain the \\((1-\\alpha)\\) confidence interval:\n\\[\n\\bar{X} \\;\\pm\\; z_{1-\\alpha/2}\\,\\frac{\\sigma}{\\sqrt{n}}.\n\\]\n\n\n\n7.3.2 Normal Case (Unknown Variance)\nIf \\(\\sigma^2\\) is unknown, replace it by the sample variance \\(S^2\\). Then\n\\[\nT = \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}} \\sim t_{n-1}.\n\\]\nSo the \\((1-\\alpha)\\) confidence interval becomes:\n\\[\n\\bar{X} \\;\\pm\\; t_{n-1,\\,1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}}.\n\\]\n\n\n7.3.3 Approximate Intervals via the CLT\nFor many distributions (not just normal), the Central Limit Theorem (CLT) tells us that\n\\[\n\\frac{\\hat{\\theta} - \\theta}{\\text{SE}(\\hat{\\theta})} \\approx \\mathcal{N}(0,1)\n\\]\nfor large \\(n\\), where \\(\\text{SE}(\\hat{\\theta})\\) is the standard error.\nThus, a general approximate confidence interval is:\n\\[\n\\hat{\\theta} \\;\\pm\\; z_{1-\\alpha/2}\\,\\text{SE}(\\hat{\\theta}).\n\\]",
    "crumbs": [
      "Part III - Interval Estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Frequentist Interval Estimation</span>"
    ]
  },
  {
    "objectID": "lectures/part1/3-types-of-inference.html#sec-inference-tasks",
    "href": "lectures/part1/3-types-of-inference.html#sec-inference-tasks",
    "title": "3  Types of Inference",
    "section": "3.2 Inference Tasks",
    "text": "3.2 Inference Tasks\nRegardless of philosophy, the core tasks of inference are similar.\nSuppose we want to infer a parameter \\(\\theta\\) from data:\n\nPoint Estimation Give a single “best guess” \\(\\hat{\\theta}\\) for the parameter.\n\nFrequentist: choose an estimator with good properties (e.g. unbiasedness).\nBayesian: often the posterior mean, median, or mode.\n\nInterval Estimation Quantify uncertainty in \\(\\theta\\) by reporting a range.\n\nFrequentist: confidence interval\n\\[\nL \\leq \\theta \\leq U\n\\]\nwith a given long-run coverage probability.\nBayesian: credible interval, the posterior probability that \\(\\theta\\) lies in \\([L,U]\\) is, say, 95%.\n\nHypothesis Testing {#sec-hypothesis-testing-intro} Ask a yes/no question about the parameter \\(\\theta\\). This is done by formalising a claim (the null hypothesis) such as \\[\n     H_0: \\theta \\in \\Theta_0,\n\\] and then using the data to judge whether this claim is compatible with the evidence.\n\nFrequentist approach: base the decision on a test statistic and its distribution under \\(H_0\\), leading to a \\(p\\)-value or a comparison with a significance level.\nBayesian approach: compare hypotheses using posterior probabilities or Bayes factors, quantifying how much the data shift our beliefs.\n\nPrediction Use data to predict future or unobserved outcomes.\n\nFrequentist: predictive distributions based on sampling models.\nBayesian: posterior predictive distribution, integrating over parameter uncertainty.",
    "crumbs": [
      "Part I — Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part2/6-bayes-inference.html#bayesian-vs.-frequentist-again",
    "href": "lectures/part2/6-bayes-inference.html#bayesian-vs.-frequentist-again",
    "title": "6  Bayesian Inference",
    "section": "6.4 Bayesian vs. Frequentist (again)",
    "text": "6.4 Bayesian vs. Frequentist (again)",
    "crumbs": [
      "Part II — Parameter Estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "lectures/part4/9-freq-hypothesis-testing.html",
    "href": "lectures/part4/9-freq-hypothesis-testing.html",
    "title": "9  Frequentist Hypothesis Testing",
    "section": "",
    "text": "9.1 Formal Setup\nRecalling the main inference tasks 3.2, we now move from interval estimation to hypothesis testing.\nThe name hypothesis testing comes from the scientific method:\nIf the data are consistent, this provides support for the hypothesis (though never proof). If the data look very unlikely under the hypothesis, this is evidence against it.\nUnlike estimation (which gives a range of plausible values), hypothesis testing asks a yes/no question about the parameter \\(\\theta\\).\nWe formalise this by specifying a null hypothesis \\(H_0\\), a concrete mathematical statement about \\(\\theta\\):\n\\[\nH_0 : \\theta \\in \\Theta_0,\n\\]\nwhere \\(\\Theta_0 \\subseteq \\Theta\\) is part of the parameter space.\nWe then use the observed data \\(\\underline{x}\\) to evaluate whether \\(H_0\\) is plausible. The frequentist reasoning is:\nVisualisation\nSuppose our model is\n\\[\nX_i \\mid \\gamma \\sim \\mathcal{N}(200, \\gamma),\n\\]\nwhere \\(\\gamma\\) is the variance.\nWe want to test whether \\(\\gamma = 10\\). That is:\n\\[\nH_0: \\gamma = 10.\n\\]\nThe frequentist approach is: assume \\(H_0\\) is true and then check whether the observed data are typical of a normal distribution with variance \\(10\\).\nn = 30\nmu0 = 200       // null mean\nsigma = 10      // std deviation under null\nalpha = 0.05\n\n// Slider for observed sample mean\nviewof xbar_obs = Inputs.range([mu0 - 3*sigma, mu0 + 3*sigma], \n  {step: 0.5, value: mu0, label: md`Observed sample mean ${tex`\\bar{x}_{obs}`}`} )\n\n// Null sampling distribution\nxgrid = d3.range(mu0 - 4*sigma, mu0 + 4*sigma, 0.1)\npdf = xgrid.map(x =&gt; ({x, y: d3.randomNormal(mu0, sigma/Math.sqrt(n))() })) // or use normal pdf\n\n// Compute rejection region bounds\nzcrit = d3.quantile(d3.range(-5,5,0.01), alpha/2) // could just hardcode 1.96\nlower = mu0 - 1.96 * sigma/Math.sqrt(n)\nupper = mu0 + 1.96 * sigma/Math.sqrt(n)\n\n// Plot\nPlot.plot({\n  width: 600, height: 300,\n  x: {label: \"Sample mean\"},\n  y: {label: \"Density\"},\n  marks: [\n    // Null distribution\n    Plot.line(xgrid.map(x =&gt; ({x, y: 1/(sigma/Math.sqrt(n)*Math.sqrt(2*Math.PI)) \n                                    * Math.exp(-0.5*((x-mu0)/(sigma/Math.sqrt(n)))**2)})), \n              {x:\"x\", y:\"y\", stroke: \"var(--brand-red)\" }),\n\n    // Shaded rejection regions\n    Plot.areaY(xgrid.map(x =&gt; ({x, y: (x&lt;lower||x&gt;upper) ? \n       1/(sigma/Math.sqrt(n)*Math.sqrt(2*Math.PI)) \n         * Math.exp(-0.5*((x-mu0)/(sigma/Math.sqrt(n)))**2) : 0})), \n       {x:\"x\", y:\"y\", fill: \"var(--brand-red)\", fillOpacity:0.3}),\n    \n    // Observed value\n    Plot.ruleX([xbar_obs], {stroke:\"black\", strokeWidth:2}),\n  ]\n})",
    "crumbs": [
      "Part IV — Hypothesis Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Frequentist Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lectures/part4/9-freq-hypothesis-testing.html#formal-setup",
    "href": "lectures/part4/9-freq-hypothesis-testing.html#formal-setup",
    "title": "9  Frequentist Hypothesis Testing",
    "section": "",
    "text": "Assume \\(H_0\\) is true.\nUse the statistical model to work out what kinds of data would be typical under \\(H_0\\).\nCompare the actual data with this prediction. If the data look very unusual under \\(H_0\\), this counts against the hypothesis.",
    "crumbs": [
      "Part IV — Hypothesis Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Frequentist Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lectures/part4/9-freq-hypothesis-testing.html#test-statistics-and-the-testing-procedure",
    "href": "lectures/part4/9-freq-hypothesis-testing.html#test-statistics-and-the-testing-procedure",
    "title": "9  Frequentist Hypothesis Testing",
    "section": "9.2 Test Statistics and the Testing Procedure",
    "text": "9.2 Test Statistics and the Testing Procedure\nWorking directly with the full data distribution can be complicated. Instead, we reduce the data to a test statistic \\(T(\\underline{X})\\) that captures the evidence relevant to \\(H_0\\).\nThe general frequentist procedure is:\n\nChoose a test statistic \\(T(\\underline{X})\\) that is sensitive to departures from \\(H_0\\).\nDerive its sampling distribution under \\(H_0\\).\nCompute the observed test statistic \\(T_{\\text{obs}}\\) from the data.\nCalculate a \\(p\\)-value: the probability of observing \\(T(\\underline{X})\\) at least as extreme as \\(T_{\\text{obs}}\\), assuming \\(H_0\\) is true.\nDecision:\n\nIf the \\(p\\)-value is below a chosen threshold \\(\\alpha\\) (the significance level), we reject \\(H_0\\).\nOtherwise, we fail to reject \\(H_0\\)1.",
    "crumbs": [
      "Part IV — Hypothesis Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Frequentist Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lectures/part4/9-freq-hypothesis-testing.html#types-of-errors",
    "href": "lectures/part4/9-freq-hypothesis-testing.html#types-of-errors",
    "title": "9  Frequentist Hypothesis Testing",
    "section": "9.3 Types of Errors",
    "text": "9.3 Types of Errors\nBecause decisions are based on data (which vary by chance), mistakes are inevitable:\n\nType I Error: rejecting \\(H_0\\) when it is actually true.\nType II Error: failing to reject \\(H_0\\) when the alternative is true.\n\nWe will return to these concepts later when discussing test performance and power.",
    "crumbs": [
      "Part IV — Hypothesis Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Frequentist Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lectures/part4/9-freq-hypothesis-testing.html#footnotes",
    "href": "lectures/part4/9-freq-hypothesis-testing.html#footnotes",
    "title": "9  Frequentist Hypothesis Testing",
    "section": "",
    "text": "It is common to say “reject \\(H_0\\)” when the \\(p\\)-value is small, and “fail to reject \\(H_0\\)” otherwise. Both phrases should be interpreted with care: rejecting \\(H_0\\) does not prove it is false, and failing to reject \\(H_0\\) does not prove it is true. Instead, hypothesis testing provides a decision rule about whether the data are sufficiently inconsistent with \\(H_0\\) at a chosen significance level.↩︎",
    "crumbs": [
      "Part IV — Hypothesis Testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Frequentist Hypothesis Testing</span>"
    ]
  }
]